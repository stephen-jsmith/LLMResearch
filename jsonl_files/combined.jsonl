{"filename": "actor.md", "prompt": "Actors", "completion": "------------------------------------------------------------------------"}
{"filename": "actor.md", "prompt": "Introduction to Abaco", "completion": ""}
{"filename": "actor.md", "prompt": "What is Abaco", "completion": "**Abaco** is an NSF-funded web service and distributed computing\nplatform providing functions-as-a-service (FaaS) to the research\ncomputing community. Abaco implements functions using the Actor Model of\nconcurrent computation. In Abaco, each actor is associated with a Docker\nimage, and actor containers are executed in response to messages posted\nto their inbox which itself is given by a URI exposed over HTTP.\n\nAbaco will ultimately offer three primary higher-level capabilities on\ntop of the underlying Actor model:\n\n> -   *Reactors* for event-driven programming\n> -   *Asynchronous Executors* for scaling out function calls within\n>     running applications, and\n> -   *Data Adapters* for creating rationalized microservices from\n>     disparate and heterogeneous sources of data.\n\nReactors and Asynchronous Executors are available today while Data\nAdapters are still under active development."}
{"filename": "actor.md", "prompt": "Using Abaco", "completion": "Abaco is in production and has been adopted by several projects. Abaco\nis available to researchers and students. To learn more about the the\nsystem, including getting access, follow the instructions in\n`/getting-started/index`{.interpreted-text role=\"doc\"}
.\n\n------------------------------------------------------------------------"
{"filename": "actor.md", "prompt": "Getting Started", "completion": "This Getting Started guide will walk you through the initial steps of\nsetting up the necessary accounts and installing the required software\nbefore moving to the Abaco Quickstart, where you will create and execute\nyour first Abaco actor. If you are already using Docker Hub and the TACC\nCloud APIs, feel free to jump right to the [Abaco\nQuickstart](#abaco-quickstart) or check out the Abaco Live Docs\n[site](https://tacc.github.io/abaco-live-docs/)."}
{"filename": "actor.md", "prompt": "Account Creation and Software Installation", "completion": ""}
{"filename": "actor.md", "prompt": "Create a TACC account", "completion": "The main instance of the Abaco platform is hosted at the Texas Advanced\nComputing Center ([TACC](https://tacc.utexas.edu)). TACC designs and\ndeploys some of the world\\'s most powerful advanced computing\ntechnologies and innovative software solutions to enable researchers to\nanswer complex questions. To use the TACC-hosted Abaco service, please\ncreate a [TACC account](https://portal.tacc.utexas.edu/account-request)\n."}
{"filename": "actor.md", "prompt": "Create a Docker account", "completion": "[Docker](https://www.docker.com/) is an open-source container runtime\nproviding operating-system-level virtualization. Abaco pulls images for\nits actors from the public Docker Hub. To register actors you will need\nto publish images on Docker Hub, which requires a [Docker\naccount](https://hub.docker.com/) ."}
{"filename": "actor.md", "prompt": "Install the Tapis Python SDK", "completion": "To interact with the TACC-hosted Abaco platform in Python, we will\nleverage the Tapis Python SDK, tapipy. To install it, simply run:\n\n\n\n::: attention\n::: title\nAttention\n:::\n\n`tapipy` works with Python 3.\n:::"}
{"filename": "actor.md", "prompt": "Working with TACC OAuth", "completion": "Authentication and authorization to the Tapis APIs uses\n[OAuth2](https://oauth.net/2/), a widely-adopted web standard. Our\nimplementation of OAuth2 is designed to give you the flexibility you\nneed to script and automate use of Tapis while keeping your access\ncredentials and digital assets secure. This is covered in great detail\nin our Tenancy and Authentication section, but some key concepts will be\nhighlighted here, interleaved with Python code."}
{"filename": "actor.md", "prompt": "Create an Tapis Client Object", "completion": "The first step in using the Tapis Python SDK, tapipy, is to create a\nTapis Client object. First, import the `Tapis` class and create python\nobject called `t` that points to the Tapis server using your TACC\nusername and password. Do so by typing the following in a Python shell:\n\n``` python"}
{"filename": "actor.md", "prompt": "Import the Tapis object", "completion": "from tapipy.tapis import Tapis"}
{"filename": "actor.md", "prompt": "Log into you the Tapis service by providing user/pass and url.", "completion": "t = Tapis(base_url='https://tacc.tapis.io',\n          username='your username',\n          password='your password')\n```"}
{"filename": "actor.md", "prompt": "Generate a Token", "completion": "With the `t` object instantiated, we can exchange our credentials for an\naccess token. In Tapis, you never send your username and password\ndirectly to the services; instead, you pass an access token which is\ncryptographically signed by the OAuth server and includes information\nabout your identity. The Tapis services use this token to determine who\nyou are and what you can do.\n\n> \n\nNote that the tapipy `t` object will store and pass your access token\nfor you, so you don\\'t have to manually provide the token when using the\ntapipy operations. You are now ready to check your access to the Tapis\nAPIs. It will expire though, after 4 hours, at which time you will need\nto generate a new token. If you are interested, you can create an OAuth\nclient (a one-time setup step, like creating a TACC account) that can be\nused to generate access and refresh tokens. For simplicity, we are\nskipping that but if you are interested, check out the Tenancy and\nAuthentication section."}
{"filename": "actor.md", "prompt": "Check Access to the Tapis APIs", "completion": "The tapipy `t` object should now be configured to talk to all Tapis APIs\non your behalf. We can check that the client is configured properly by\nmaking any API call. For example, we can use the authenticator service\nto retrieve the full TACC profile of our user. To do so, use the\n`get_profile()` function associated with the `authenticator` object on\nthe `t` object, passing the username of the profile to retrieve, as\nfollows.\n\n\n\n------------------------------------------------------------------------"}
{"filename": "actor.md", "prompt": "Abaco Quickstart", "completion": "In this Quickstart, we will create an Abaco actor from a basic Python\nfunction. Then we will execute our actor on the Abaco cloud and get the\nexecution results."}
{"filename": "actor.md", "prompt": "A Basic Python Function", "completion": "Suppose we want to write a Python function that counts words in a\nstring. We might write something like this:\n\n\n\nIn order to process a message sent to an actor, we use the `raw_message`\nattribute of the `context` dictionary. We can access it by using the\n`get_context` method from the `actors` module in `tapipy`.\n\nFor this example, create a new local directory to hold your work. Then,\ncreate a new file in this directory called `example.py`. Add the\nfollowing to this file:\n\n``` python"}
{"filename": "actor.md", "prompt": "example.py", "completion": "from tapipy.actors import get_context\n\ndef string_count(message):\n    words = message.split(' ')\n    word_count = len(words)\n    print('Number of words is: ' + str(word_count))\n\ncontext = get_context()\nmessage = context['raw_message']\nstring_count(message)\n```"}
{"filename": "actor.md", "prompt": "Building Images From a Dockerfile", "completion": "To register this function as an Abaco actor, we create a docker image\nthat contains the Python function and execute it as part of the default\ncommand.\n\nWe can build a Docker image from a text file called a Dockerfile. You\ncan think of a Dockerfile as a recipe for creating images. The\ninstructions within a Dockerfile either add files/folders to the image,\nadd metadata to the image, or both."}
{"filename": "actor.md", "prompt": "The FROM Instruction", "completion": "Create a new file called `Dockerfile` in the same directory as your\n`example.py` file.\n\nWe can use the `FROM` instruction to start our new image from a known\nimage. This should be the first line of our Dockerfile. We will start an\nofficial Python image:"}
{"filename": "actor.md", "prompt": "The RUN, ADD and CMD Instructions", "completion": "We can run arbitrary Linux commands to add files to our image. We\\'ll\nrun the `pip` command to install the `tapipy` library in our image:\n\n\n\n(Note: there is a `abacosample` image that contains Python and the\ntapipy library; see the Samples section for more details, coming soon.)\n\nWe can also add local files to our image using the `ADD` instruction. To\nadd the `example.py` file from our local directory, we use the following\ninstruction:\n\n\n\nThe last step is to write the command from running the application,\nwhich is simply `python /example.py`. We use the `CMD` instruction to do\nthat:\n\n\n\nWith that, our `Dockerfile` is now ready. This is what is looks like:\n\n\n\nNow that we have our `Dockerfile`, we can build our image and push it to\nDocker Hub. To do so, we use the `docker build` and `docker push`\ncommands \\[note: user is your user on Docker, you must also \\$ docker\nlogin\\] :"}
{"filename": "actor.md", "prompt": "Registering an Actor", "completion": "Now we are going to register the Docker image we just built as an Abaco\nactor. To do this, we will use the `Tapis` client object we created\nabove (see [Working with TACC OAuth](#working-with-tacc-oauth)).\n\nTo register an actor using the tapipy library, we use the `actors.add()`\nmethod and pass the arguments describing the actor we want to register\nthrough the `body` parameter. For example:\n\n\n\nYou should see a response like this:\n\n\n\nNotes:\n\n-   Abaco assigned an id to the actor (in this case `JWpkNmBwKewYo`) and\n    associated it with the image (in this case, `abacosamples/wc`) which\n    it began pulling from the public Docker Hub.\n-   Abaco returned a status of `SUBMITTED` for the actor; behind the\n    scenes, Abaco is starting a worker container to handle messages\n    passed to this actor. The worker must initialize itself (download\n    the image, etc) before the actor is ready.\n-   When the actor\\'s worker is initialized, the status will change to\n    `READY`.\n\nAt any point we can check the details of our actor, including its\nstatus, with the following:\n\n\n\nThe response format is identical to that returned from the `.add()`\nmethod."}
{"filename": "actor.md", "prompt": "Executing an Actor", "completion": "We are now ready to execute our actor by sending it a message. We built\nour actor to process a raw message string, so that is what we will send,\nbut there other options, including JSON and binary data. For more\ndetails, see the `Messages <target messages>`{.interpreted-text\nrole=\"ref\"}
 section.\n\nWe send our actor a message using the `send_message()` method:\n\n\n\nAbaco queues up an execution for our actor and then responds with JSON,\nincluding an id for the execution contained in the `execution_id`:\n\n\n\nIn general, an execution does not start immediately but is instead\nqueued until a future time when a worker for the actor can take the\nmessage and start an actor container with the message. We can retrieve\nthe details about an execution, including its status, using the\n`get_execution()` method:\n\n\n\nThe response will be similar to the following:\n\n\n\nNote that a status of `COMPLETE` indicates that the execution has\nfinished and we are ready to retrieve our results."
{"filename": "actor.md", "prompt": "Retrieving the Logs", "completion": "The Abaco system collects all standard out from an actor execution and\nmakes it available via the `logs` endpoint. Let\\'s retrieve the logs\nfrom the execution we just made. We use the `get_execution_logs()`\nmethod, passing out `actor_id` and our `execution_id`:\n\n\n\nThe response should be similar to the following:\n\n\n\nWe see our actor output [Number of words is: 5]{.title-ref}
, which is\nthe expected result!"
{"filename": "actor.md", "prompt": "Conclusion", "completion": "Congratulations! At this point you have created, registered and executed\nyour first actor, but there is a lot more you can do with the Abaco\nsystem. To learn more about the additional capabilities, please continue\non to the Technical Guide.\n\n------------------------------------------------------------------------"}
{"filename": "actor.md", "prompt": "Actor Registration {#target registration}
", "completion": "When registering an actor, the only required field is a reference to an\nimage on the public Docker Hub. However, there are several other\nproperties that can be set. The following table provides a list of the\nconfigurable properties available to all users and their descriptions.\n\n+--------------+-------------------------------------------------------+\n| Property     | Description                                           |\n| Name         |                                                       |\n+==============+=======================================================+\n| image        | The Docker image to associate with the actor. This    |\n|              | should be a fully qualified image available on the    |\n|              | public Docker Hub. We encourage users to use to image |\n|              | tags to version control their actors.                 |\n+--------------+-------------------------------------------------------+\n| name         | A user defined name for the actor.                    |\n+--------------+-------------------------------------------------------+\n| description  | A user defined description for the actor.             |\n+--------------+-------------------------------------------------------+\n| default      | The default environment is a set of key/value pairs   |\n| _environment | to be injected into every execution of the actor. The |\n|              | values can also be overridden when passing a message  |\n|              | to the reactor in the query parameters (see           |\n|              | `Messages <target messages>`{.interpreted-text        |\n|              | role=\"ref\"
).                                         |\n+--------------+-------------------------------------------------------+\n| hints        | A list of strings representing user-defined \\\"tags\\\"  |\n|              | or metadata about the actor. \\\"Official\\\" Abaco hints |\n|              | can be applied to control configurable aspects of the |\n|              | actor runtime, such as the algorithm used (see        |\n|              | `Autoscaling <target autoscaling>`{.interpreted-text  |\n|              | role=\"ref\"
).                                         |\n+--------------+-------------------------------------------------------+\n| link         | Actor identifier (id or alias) of an actor to         |\n|              | \\\"link\\\" this actor\\'s events to. Requires execute    |\n|              | permissions on the linked actor, and cycles are not   |\n|              | permitted. (see                                       |\n|              | `Actor Links, Events,                                 |\n|              |  and Webhooks <target actor links>`{.interpreted-text |\n|              | role=\"ref\"
).                                         |\n+--------------+-------------------------------------------------------+\n| privileged   | (True/False) - Whether the actor runs in privileged   |\n|              | mode and has access to the Docker daemon. *Note*:     |\n|              | Setting this parameter to True requires elevated      |\n|              | permissions.                                          |\n+--------------+-------------------------------------------------------+\n| stateless    | (True/False) - Whether the actor stores private state |\n|              | as part of its execution. If True, the state API will |\n|              | not be available, but in a future release, the Abaco  |\n|              | service will be able to automatically scale reactor   |\n|              | processes to execute messages in parallel. The        |\n|              | default value is False.                               |\n+--------------+-------------------------------------------------------+\n| token        | (True/False) - Whether to generate an OAuth access    |\n|              | token for every execution of this actor. Generating   |\n|              | an OAuth token add about 500 ms of time to the        |\n|              | execution start up time.                              |\n|              |                                                       |\n|              | *Note: the default value for the \\`\\`token\\`\\`        |\n|              | attribute varies from tenant to tenant. Always        |\n|              | explicitly set the token attribute when registering   |\n|              | new actors to ensure the proper behavior.*            |\n+--------------+-------------------------------------------------------+\n| use_c        | Run the actor using the UID/GID set in the Docker     |\n| ontainer_uid | image. *Note*: Setting this parameter to True         |\n|              | requires elevated permissions.                        |\n+--------------+-------------------------------------------------------+\n| run          | Run the actor using the UID/GID of the executor       |\n| _as_executor | rather than the owner *Note*: this parameter is only  |\n|              | available to certain tenants *Note*: that this cannot |\n|              | be on while the use_container_uid is also on          |\n+--------------+-------------------------------------------------------+\n| webhook      | URL to publish this actor\\'s events to. (see          |\n|              | `Actor Links, Events,                                 |\n|              |  and Webhooks <target actor links>`{.interpreted-text |\n|              | role=\"ref\"
)                                          |\n+--------------+-------------------------------------------------------+"
{"filename": "actor.md", "prompt": "Notes", "completion": "-   The `default_environment` can be used to provide sensitive\n    information to the actor that cannot be put in the image.\n-   In order to execute privileged actors or to override the UID/GID\n    used when executing an actor container, talk to the Abaco\n    development team about your use case.\n-   Abaco supports running specific actors within a given tenant on\n    dedicated and/or specialized hardware for performance reasons. It\n    accomplishes this through the use of actor `queues`. If you need to\n    run actors on dedicated resources, talk to the Abaco development\n    team about your use case."}
{"filename": "actor.md", "prompt": "Examples", "completion": ""}
{"filename": "actor.md", "prompt": "curl", "completion": "Here is an example using curl; note that to set the default environment,\nwe *must* pass content type `application/json` and be sure to pass\nproperly formatted JSON in the payload."}
{"filename": "actor.md", "prompt": "Python", "completion": "To register the same actor using the tapipy library, we use the\n`actors.create_actor()` method and pass the same arguments through the\n[request_body]{.title-ref}
 parameter. In this case, the\n`default_environment` is just a standard Python dictionary where the\nkeys and values are `str` type. For example,\n\n\n\n------------------------------------------------------------------------"
{"filename": "actor.md", "prompt": "Abaco Context & Container Runtime", "completion": "In this section we describe the environment that Abaco actor containers\ncan utilize during their execution."}
{"filename": "actor.md", "prompt": "Context {#target context}
", "completion": "When an actor container is launched, Abaco injects information about the\nexecution into a number of environment variables. This information is\ncollectively referred to as the `context`. The following table provides\na complete list of variable names and their description:\n\n+----------------+-----------------------------------------------------+\n| Variable Name  | Description                                         |\n+================+=====================================================+\n| \\_             | The id of the actor.                                |\n| abaco_actor_id |                                                     |\n+----------------+-----------------------------------------------------+\n| \\_ab           | The Abaco internal id of the actor.                 |\n| aco_actor_dbid |                                                     |\n+----------------+-----------------------------------------------------+\n| \\_abaco_       | The Docker image used to launch this actor          |\n| container_repo | container.                                          |\n+----------------+-----------------------------------------------------+\n| \\_a            | The id of the worker for the actor overseeing this  |\n| baco_worker_id | execution.                                          |\n+----------------+-----------------------------------------------------+\n| \\_abac         | The id of the current execution.                    |\n| o_execution_id |                                                     |\n+----------------+-----------------------------------------------------+\n| \\_abac         | An OAuth2 access token representing the user who    |\n| o_access_token | registered the actor.                               |\n+----------------+-----------------------------------------------------+\n| \\_ab           | The OAuth2 API server associated with the actor.    |\n| aco_api_server |                                                     |\n+----------------+-----------------------------------------------------+\n| \\_aba          | The value of the actor\\'s state at the start of the |\n| co_actor_state | execution.                                          |\n+----------------+-----------------------------------------------------+\n| \\_abac         | The data type of the message (either \\'str\\' or     |\n| o_Content-Type | \\'application/json\\').                              |\n+----------------+-----------------------------------------------------+\n| \\_             | The username of the \\\"executor\\\", i.e., the user    |\n| abaco_username | who sent the message.                               |\n+----------------+-----------------------------------------------------+\n| \\_ab           | The base URL for the Abaco API service.             |\n| aco_api_server |                                                     |\n+----------------+-----------------------------------------------------+\n| > MSG          | The message sent to the actor, as a raw string.     |\n+----------------+-----------------------------------------------------+"
{"filename": "actor.md", "prompt": "Notes", "completion": "-   The `_abaco_actor_dbid` is unique to each actor. Using this id, an\n    actor can distinguish itself from other actors registered with the\n    same function providing for SPMD techniques.\n-   The `_abaco_access_token` is a valid OAuth token that actors can use\n    to make authenticated requests to other TACC Cloud APIs during their\n    execution.\n-   The actor can update its state during the course of its execution;\n    see the section `Actor State <target actor state>`{.interpreted-text\n    role=\"ref\"}
 for more details.\n-   The \\\"executor\\\" of the actor may be different from the owner; see\n    `Sharing <target actor sharing>`{.interpreted-text role=\"ref\"
 for\n    more details."
{"filename": "actor.md", "prompt": "Access from Python", "completion": "The `tapipy.actors` module provides access to the above data in native\nPython objects. Currently, the actors module provides the following\nutilities:\n\n-   \n\n    `get_context()` - returns a Python dictionary with the following fields:\n\n    :   -   `raw_message` - the original message, either string or JSON\n            depending on the Contetnt-Type.\n        -   `content_type` - derived from the original message request.\n        -   `message_dict` - A Python dictionary representing the\n            message (for Content-Type: application/json)\n        -   `execution_id` - the ID of this execution.\n        -   `username` - the username of the user that requested the\n            execution.\n        -   `state` - (for stateful actors) state value at the start of\n            the execution.\n        -   `actor_id` - the actor\\'s id.\n\n-   `get_client()` - returns a pre-authenticated `tapipy.Tapis` object.\n\n-   `update_state(val)` - Atomically, update the actor\\'s state to the\n    value `val`."}
{"filename": "actor.md", "prompt": "Runtime Environment", "completion": "The environment in which an Abaco actor container runs has been built to\naccommodate a number of typical use cases encountered in research\ncomputing in a secure manner."}
{"filename": "actor.md", "prompt": "Container UID and GID", "completion": "When Abaco launches an actor container, it instructs Docker to execute\nthe process using the UID and GID associated with the TACC account of\nthe owner of the actor. This practice guarantees that an Abaco actor\nwill have exactly the same accesses as the original author of the actor\n(for instance, access to files or directories on shared storage) and\nthat files created or updated by the actor process will be owned by the\nunderlying API user. Abaco API users that have elevated privilleges\nwithin the platform can override the UID and GID used to run the actor\nwhen registering the actor (see\n`Registration <target registration>`{.interpreted-text role=\"ref\"}
)."
{"filename": "actor.md", "prompt": "POSIX Interface to the TACC WORK File System", "completion": "When Abaco launches an actor container, it mounts the actor owner\\'s\nTACC WORK file system into the running container. The owner\\'s work file\nsystem is made available at `/work` with the container. This gives the\nactor a POSIX interface to the work file system.\n\n------------------------------------------------------------------------"}
{"filename": "actor.md", "prompt": "Messages, Executions, and Logs", "completion": "Once you have an Abaco actor created the next logical step is to send\nthis actor some type of job or message detailing what the actor should\ndo. The act of sending an actor information to execute a job is called\nsending a message. This sent message can be raw string data, JSON data,\nor a binary message.\n\nOnce a message is sent to an Abaco actor, the actor will create an\nexecution with a unique `execution_id` tied to it that will show\nresults, time running, and other stats which will be listed below.\nExecutions also have logs, and when the log are called for, you\\'ll\nreceive the command line logs of your running execution. Akin to what\nyou\\'d see if you and outputted a script to the command line. Details on\nmessages, executions, and logs are below.\n\n**Note:** Due to each message being tied to a specific execution, each\nexecution will have exactly one message that can be processed."}
{"filename": "actor.md", "prompt": "Messages {#target messages}
", "completion": "A message is simply the message given to an actor with data that can be\nused to run the actor. This data can be in the form of a raw message\nstring, JSON, or binary. Once this message is sent, the messaged Abaco\nactor will queue an execution of the actor\\'s specified image.\n\nOnce off the queue, if your specified image has inputs for the messaged\ndata, then that messaged data will be visible to your program. Allowing\nyou to set custom parameters or inputs for your executions."
{"filename": "actor.md", "prompt": "Sending a message", "completion": ""}
{"filename": "actor.md", "prompt": "cURL", "completion": "To send a message to the `messages` endpoint with cURL, you would do the\nfollowing:"}
{"filename": "actor.md", "prompt": "Python", "completion": "To send a message to the `messages` endpoint with `tapipy` and Python,\nyou would do the following:"}
{"filename": "actor.md", "prompt": "Results", "completion": "These calls result in a list similar to the following:"}
{"filename": "actor.md", "prompt": "Get message count", "completion": "It is possible to retrieve the current number of messages an actor has\nwith the `messages` end point."}
{"filename": "actor.md", "prompt": "cURL", "completion": "The following retrieves the current number of messages an actor has:"}
{"filename": "actor.md", "prompt": "Python", "completion": "To retrieve the current number of messages with `tapipy` the following\nis done:"}
{"filename": "actor.md", "prompt": "Results", "completion": "The result of getting the `messages` endpoint should be similar to:"}
{"filename": "actor.md", "prompt": "Binary Messages", "completion": "An additional feature of the Abaco message system is the ability to post\nbinary data. This data, unlike raw string data, is sent through a Unix\nNamed Pipe (FIFO), stored at /\\_abaco_binary_data, and can be retrieved\nfrom within the execution using a FIFO message reading function. The\nability to read binary data like this allows our end users to do\nnumerous tasks such as reading in photos, reading in code to be ran, and\nmuch more.\n\nThe following is an example of sending a JPEG as a binary message in\norder to be read in by a TensorFlow image classifier and being returned\npredicted image labels. For example, sending a photo of a golden\nretriever might yield, 80% golden retriever, 12% labrador, and 8% clock.\n\nThis example uses Python and `tapipy` in order to keep code in one\nscript."}
{"filename": "actor.md", "prompt": "Python with Tapipy", "completion": "Setting up an `Tapis` object with token and API address information:\n\n\n\nCreating actor with the TensorFlow image classifier docker image:\n\n\n\nThe following creates a binary message from a JPEG image file:\n\n\n\nSending binary JPEG file to actor as message with the sendBinaryMessage\nfunction (You can also just set the headers with\n`Content-Type: application/octet-stream`):\n\n\n\nThe following returns information pertaining to the execution:\n\n\n\nOnce the execution has complete, the logs can be called with the\nfollowing:"}
{"filename": "actor.md", "prompt": "Sending binary from execution", "completion": "Another useful feature of Abaco is the ability to write to a socket\nconnected to an Abaco endpoint from within an execution. This Unix\nDomain (Datagram) socker is mounted in the actor container at\n/\\_abaco_results.sock.\n\nIn order to write binary data this socket you can use `tapipy`\nfunctions, in particular the `send_bytes_result()` function that sends\nbytes as single result to the socket. Another useful function is the\n`send_python_result()` function that allows you to send any Python\nobject that can be pickled with `cloudpickle`.\n\nIn order to retrieve these results from Abaco you can get the\n`/actors/<actor_id>/executions/<execution_id>/results` endpoint. Each\nget of the endpoint will result in exactly one result being popped and\nretrieved. An empty result with be returned if the results queue is\nempty.\n\nAs a socket, the maximum size of a result is 131072 bytes. An execution\ncan send multiple results to the socket and said results will be added\nto a queue. It is recommended to to return a reference to a file or\nobject store.\n\nAs well, results are sent to the socket and available immediately, an\nexecution does not have to complete to pop a result. Results are given\nan expiry time of 60 minutes from creation."}
{"filename": "actor.md", "prompt": "cURL", "completion": "To retrieve a result with cURL you would do the following:"}
{"filename": "apps.md", "prompt": "Overview", "completion": "A Tapis application represents all the information required to run a\nTapis job on a Tapis system and produce useful results. Each application\nis versioned and is associated with a specific tenant and owned by a\nspecific user who has special privileges for the application. In order\nto support this purpose an application definition includes information\nwhich allows the *Jobs* service to:\n\n-   Stage input prior to launching the application\n-   Launch the application\n-   Monitor the application during execution\n-   Archive output after application execution"}
{"filename": "apps.md", "prompt": "Model", "completion": "At a high level an application contains some information that is\nindependent of the version and some information that varies by version."}
{"filename": "apps.md", "prompt": "Non-Versioned Attributes", "completion": "*id*\n\n:   A short descriptive name for the application that is unique within\n    the tenant.\n\n*owner*\n\n:   A specific user set at application creation. Default is\n    `${apiUserId}
`, the user making the request to create the\n    application.\n\n*enabled*\n\n:   Indicates if application is currently considered active and\n    available for use. Default is *true*.\n\n*deleted*\n\n:   Indicates if application has been soft deleted.\n\n*created*\n\n:   When the application was created. Maintained by service.\n\n*updated*\n\n:   When the application was last updated. Maintained by service."
{"filename": "apps.md", "prompt": "Versioned Attributes", "completion": "*version*\n\n:   Applications are expected to evolve over time. `Id` + `version` must\n    be unique within a tenant.\n\n*description*\n\n:   An optional more verbose description for the application.\n\n*runtime*\n\n:   Runtime to be used when executing the application. DOCKER,\n    SINGULARITY, ZIP. Default is DOCKER.\n\n*runtimeVersion*\n\n:   Runtime version to be used when executing the application.\n\n*runtimeOptions*\n\n:   Options that apply to the runtime. Currently only applicable for\n    SINGULARITY runtime.\n\n*containerImage*\n\n:   Reference to be used when running the container image.\n\n*jobType*\n\n:   FORK or BATCH. Jobs submitted will be of this type by default. May\n    be overridden in the job submit request. This allows an application\n    designer to test an application run as a FORK job, for example, and\n    then move on to running as a BATCH job which typically involves\n    further design work. Default is FORK.\n\n*maxJobs*\n\n:   Maximum total number of jobs that can be queued or running for this\n    application on a given execution system at a given time. Note that\n    the execution system may also limit the number of jobs on the system\n    which may further restrict the total number of jobs. Set to -1 for\n    unlimited. Default is unlimited.\n\n*maxJobsPerUser*\n\n:   Maximum total number of jobs associated with a specific job owner\n    that can be queued or running for this application on a given\n    execution system at a given time. Note that the execution system may\n    also limit the number of jobs on the system which may further\n    restrict the total number of jobs. Set to -1 for unlimited. Default\n    is unlimited.\n\n*strictFileInputs*\n\n:   Indicates if a job request is allowed to have unnamed file inputs.\n    If set to true then a job request may only use the named file inputs\n    defined in the application. See attribute *fileInputs* in the\n    JobAttributes table. Default is *false*.\n\n*Job related attributes*\n\n:   Various attributes related to job execution such as *execSystemId*,\n    *execSystemExecDir*, *execSystemInputDir*, *execSystemLogicalQueue*\n    *archiveSystemId*, *fileInputs*, etc.\n\n::: note\n::: title\nNote\n:::\n\nCurrently dynamic selection of an execution system is not supported. For\nthis reason the job related attribute *dynamicExecSystem* should be set\nto *false* (the default) and *execSystemConstraints* should not be set.\n:::"}
{"filename": "apps.md", "prompt": "Required Attributes", "completion": "When creating a application the required attributes are: `id`, `version`\nand `containerImage`. Depending on the type of application and specific\nvalues for certain attributes there are other requirements.\n\nThe restrictions are:\n\n-   If `archiveSystemId` is specified then `archiveSystemDir` must be\n    specified."}
{"filename": "apps.md", "prompt": "Attributes and the Job Execution Environment", "completion": "The runtime environment of a Tapis job is determined by values in the\njob submit request, the application definition and the execution system\ndefinition. Generally speaking, for values that can be assigned in\nmultiple places, the values in the job submit request override those in\nthe application definition, which in turn override those in the system\ndefinition. There are special cases, however, where the values from\ndifferent definitions are merged.\n\nSee the jobs/apps/systems parameter\n[matrix](https://drive.google.com/file/d/1BrY6tHzOegwsgDMrhcKE7RHH7HRAA0Do/view?usp=sharing)\nfor a detailed description of how each attribute is handled."}
{"filename": "apps.md", "prompt": "Getting Started", "completion": "Before going into further details about applications, here we give some\nexamples of how to create and view applications. In the examples below\nwe assume you are using the TACC tenant with a base URL of\n`tacc.tapis.io` and that you have authenticated using PySDK or obtained\nan authorization token and stored it in the environment variable JWT, or\nperhaps both."}
{"filename": "apps.md", "prompt": "Creating an application", "completion": "Create a local file named `app_sample.json` with json similar to the\nfollowing:\n\n    {\n      \"id\":\"tacc-sample-app-<userid>\",\n      \"version\":\"0.1\",\n      \"description\":\"My sample application\",\n      \"runtime\":\"DOCKER\",\n      \"containerImage\":\"docker.io/hello-world:latest\",\n      \"jobType\":\"FORK\",\n      \"jobAttributes\": {\n        \"description\": \"default job description\",\n        \"execSystemId\": \"execsystem1\"\n      }
\n    
\n\nwhere \\<userid\\> is replaced with your user name.\n\n::: note\n::: title\nNote\n:::\n\nIf specified, `execSystemId` must reference a system that exists and has\n`canExec` set to true. If `execSystemId` is not specified, then it must\nbe provided as part of the job submit request.\n:::\n\nUsing PySDK:\n\n\n\nUsing CURL:\n\n    $ curl -X POST -H \"content-type: application/json\" -H \"X-Tapis-Token: $JWT\" https://tacc.tapis.io/v3/apps -d @app_sample.json"
{"filename": "apps.md", "prompt": "Viewing Applications", "completion": ""}
{"filename": "apps.md", "prompt": "Retrieving details for an application", "completion": "To retrieve details for a specific application, such as the one above:\n\n::: note\n::: title\nNote\n:::\n\nSee the section below on [Selecting](#selecting) to find out how to\ncontrol the amount of information returned.\n:::\n\nUsing PySDK:\n\n\n\nUsing CURL:\n\n    $ curl -H \"X-Tapis-Token: $JWT\" https://tacc.tapis.io/v3/apps/tacc-sample-app-<userid>\n\nThe response should look similar to the following:\n\n    {\n       \"result\": {\n           \"tenant\": \"tacc\",\n           \"id\": \"tacc-sample-app-<userid>\",\n           \"version\": \"0.1\",\n           \"description\": \"My sample application\",\n           \"owner\": \"<userid>\",\n           \"enabled\": true,\n           \"runtime\": \"DOCKER\",\n           \"runtimeVersion\": null,\n           \"runtimeOptions\": [],\n           \"containerImage\": \"docker.io/hello-world:latest\",\n           \"jobType\": \"FORK\",\n           \"maxJobs\": 0,\n           \"maxJobsPerUser\": 0,\n           \"strictFileInputs\": false,\n           \"jobAttributes\": {\n               \"description\": \"default job description\",\n               \"dynamicExecSystem\": false,\n               \"execSystemConstraints\": [],\n               \"execSystemId\": \"execsystem1\",\n               \"execSystemExecDir\": null,\n               \"execSystemInputDir\": null,\n               \"execSystemOutputDir\": null,\n               \"execSystemLogicalQueue\": null,\n               \"archiveSystemId\": null,\n               \"archiveSystemDir\": null,\n               \"archiveOnAppError\": false,\n               \"isMpi\": false,\n               \"mpiCmd\": null,\n               \"cmdPrefix\": null,\n               \"parameterSet\": {\n                   \"appArgs\": [\n                     {\n                       \"arg\": \"arg1\",\n                       \"name\": \"appArg1Name\",\n                       \"description\": \"description for app arg 1\",\n                       \"inputMode\": \"FIXED\",\n                       \"notes\": {\n                         \"arg1note\": \"my first arg\"\n                       }
\n                     
\n                   ],\n                   \"containerArgs\": [],\n                   \"schedulerOptions\": [],\n                   \"envVariables\": [\n                     {\n                       \"key\": \"ENV_1\",\n                       \"value\": \"env_1_value\",\n                       \"description\": \"my env var #1\",\n                       \"inputMode\": \"INCLUDE_BY_DEFAULT\",\n                       \"notes\": {
\n                     
,\n                     {\n                       \"key\": \"APP_ONLY_FIXED_SET\",\n                       \"value\": \"app_only_fixed_set\",\n                       \"description\": \"FIXED env var only in app definition\",\n                       \"inputMode\": \"FIXED\",\n                       \"notes\": {\n                         \"app_only_fixed_notes\": \"testing_it\"\n                       
\n                     
,\n                     {\n                       \"key\": \"APP_REQUIRED_SET\",\n                       \"value\": \"app_required_set\",\n                       \"description\": \"\",\n                       \"inputMode\": \"REQUIRED\",\n                       \"notes\": {
\n                     
\n                   ],\n                   \"archiveFilter\": {\n                       \"includes\": [],\n                       \"excludes\": [],\n                       \"includeLaunchFiles\": true\n                   
\n               
,\n               \"fileInputs\": [\n                 {\n                   \"name\": \"empty\",\n                   \"description\": \"An empty file\",\n                   \"inputMode\": \"OPTIONAL\",\n                   \"autoMountLocal\": true,\n                   \"notes\": {
,\n                   \"sourceUrl\": \"tapis://test-storage-linux/data_input/empty.txt\",\n                   \"targetPath\": \"empty.txt\"\n                 
,\n                 {\n                   \"name\": \"file1\",\n                   \"description\": \"A random text file\",\n                   \"inputMode\": \"REQUIRED\",\n                   \"autoMountLocal\": true,\n                   \"notes\": {
,\n                   \"sourceUrl\": \"tapis://test-storage-linux/data_input/file1.txt\",\n                   \"targetPath\": \"file1.txt\"\n                 
,\n                 {\n                   \"name\": \"s3_ceph_file\",\n                   \"description\": \"A file from an s3 ceph storage system.\",\n                   \"inputMode\": \"REQUIRED\",\n                   \"autoMountLocal\": true,\n                   \"notes\": {
,\n                   \"sourceUrl\": \"tapis://test-storage-s3-ceph/object1\",\n                   \"targetPath\": \"s3_ceph_file.dat\"\n                 
,\n                 {\n                   \"name\": \"s3_aws_test1\",\n                   \"description\": \"File from an s3 aws storage system.\",\n                   \"inputMode\": \"REQUIRED\",\n                   \"autoMountLocal\": true,\n                   \"notes\": {
,\n                   \"sourceUrl\": \"tapis://test-s3-storage/object2\",\n                   \"targetPath\": \"s3_aws/test1.dat\"\n                 
\n               ],\n               \"fileInputArrays\": [\n                 {\n                   \"name\": \"fileInputArray1\",\n                         \"description\": \"A list of files in a single directory\",\n                   \"sourceUrls\": [\n                      \"tapis://test-storage-linux/data_input/file1a.txt\",\n                      \"tapis://test-storage-linux/data_input/file2a.txt\",\n                      \"tapis://test-storage-linux/data_input/file3a.txt\"\n                   ],\n                   \"targetDir\": \"myFileInputArrayDir/subdir1\"\n                 
\n               ],\n               \"nodeCount\": 1,\n               \"coresPerNode\": 1,\n               \"memoryMB\": 100,\n               \"maxMinutes\": 10,\n               \"subscriptions\": [\n                 {\n                   \"description\": \"Email on job new status\",\n                   \"enabled\": true,\n                   \"jobEventCategoryFilter\": \"JOB_NEW_STATUS\",\n                   \"deliveryTargets\": [\n                     {\n                       \"deliveryMethod\": \"EMAIL\",\n                       \"deliveryAddress\": \"me@example.com\"\n                     
\n                   ],\n                   \"ttlMinutes\": 10080\n                 
\n               ],\n               \"tags\": []\n           
,\n           \"tags\": [],\n           \"notes\": {
,\n           \"uuid\": \"40a60a11-41fe-45ea-8674-d2cfe04992f6\",\n           \"deleted\": false,\n           \"created\": \"2021-04-22T21:30:10.590999Z\",\n           \"updated\": \"2021-04-22T21:30:10.590999Z\"\n       
,\n       \"status\": \"success\",\n       \"message\": \"TAPIS_FOUND App found: tacc-sample-app-<userid>\",\n       \"version\": \"0.0.1-SNAPSHOT\",\n       \"metadata\": null\n    
"
{"filename": "apps.md", "prompt": "Retrieving details for all applications", "completion": "To see the list of applications that you own:\n\nUsing PySDK:\n\n\n\nUsing CURL:\n\n    $ curl -H \"X-Tapis-Token: $JWT\" https://tacc.tapis.io/v3/apps?select=allAttributes\n\nThe response should contain a list of items similar to the single\nlisting shown above.\n\n::: note\n::: title\nNote\n:::\n\nSee the sections below on [Searching](#searching),\n[Selecting](#selecting), [Sorting](#sorting) and [Limiting](#limiting)\nto find out how to control the amount of information returned.\n:::"}
{"filename": "apps.md", "prompt": "Minimal Definition and Restrictions", "completion": "When creating an application the required attributes are: *id*,\n*version* and *containerImage* Depending on the type of application and\nspecific values for certain attributes there are other requirements. The\nrestrictions are:\n\n-   If *archiveSystemId* is specified then *archiveSystemDir* is\n    required."}
{"filename": "apps.md", "prompt": "Version", "completion": "The versioning scheme is at the discretion of the application author.\nThe combination of `tenant+id+version` uniquely identifies an\napplication in the Tapis environment. It is recommended that a two or\nthree level form of semantic versioning be used. The fully qualified\napplication reference within a tenant is constructed by appending a\nhyphen to the name followed by the version string. For example, the\nfirst two versions of an application might be *myapp-0.0.1* and\n*myapp-0.0.2*. If a version is not specified when retrieving an\napplication then by default the most recently created version of the\napplication will be returned."}
{"filename": "apps.md", "prompt": "Containerized Application", "completion": "An application that has been containerized is one that can be executed\nusing a single container image. Tapis will use the appropriate container\nruntime command and provide support for making the input and output\ndirectories available to the container when running the container image.\n\n::: note\n::: title\nNote\n:::\n\nCurrently only containerized applications are supported\n:::"}
{"filename": "apps.md", "prompt": "Directory Semantics and Macros", "completion": "At job submission time the Jobs service supports the use of macros based\non template variables. These variables may be referenced when specifying\ndirectories in an application definition. For a full list of supported\nvariables and more information please see the Jobs Service\ndocumentation. Here are some examples of variables that may be used when\nspecifying directories for an application:\n\n-   *JobUUID* - The Id of the job determined at job submission.\n-   *JobOwner* - The owner of the job determined at job submission.\n-   *JobWorkingDir* - Default parent directory from which a job is run.\n    This will be relative to the effective root directory *rootDir* on\n    the execution system. *rootDir* and *jobWorkingDir* are attributes\n    of the execution system.\n-   *HOST_EVAL(\\$\\<ENV_VARIABLE\\>)* - The value of the environment\n    variable *ENV_VARIABLE* when evaluated on the execution system host\n    when logging in under the job\\'s effective user ID. This is a\n    dynamic value determined at job submission time. The function\n    *HOST_EVAL()* extracts specific environment variable values for use\n    during job setup. In particular, the TACC specific values of\n    *\\$HOME*, *\\$WORK*, *\\$SCRATCH* and *\\$FLASH* can be referenced. The\n    specified environment variable name is used **as-is**. It is **not**\n    subject to macro substitution. However, the function call can have a\n    path string appended to it, such as in\n    *HOST_EVAL(\\$SCRATCH)/tmp/\\${JobUUID}
*, and macro substitution will\n    be applied to the path string."
{"filename": "apps.md", "prompt": "Permissions", "completion": "The permissions model allows for fine grained access control of Tapis\napplications.\n\nAt application creation time the owner is given full access to the\napplication. Permissions for other users may be granted and revoked\nthrough the applications API. Please note that grants and revokes\nthrough this service only impact the default role for the user. A user\nmay still have access through permissions in another role. So even after\nrevoking permissions through this service when permissions are retrieved\nthe access may still be listed. This indicates access has been granted\nvia another role.\n\nPermissions are specified as either `*` for all permissions or some\ncombination of the following specific permissions:\n`(\"READ\",\"MODIFY\",\"EXECUTE\")`. Specifying permissions in all lower case\nis also allowed. Having `MODIFY` implies `READ`."}
{"filename": "apps.md", "prompt": "Sharing", "completion": "In addition to fine grained permissions support, Tapis also supports a\nhigher level approach to granting access. This approach is known simply\nas *sharing*. The sharing API allows you to share an application with a\nset of users as well as share publicly with all users in a tenant.\nSharing grants `READ+EXECUTE` access and, more importantly, facilitates\nallowing others to run a job using the application.\n\nSharing an application gives a user certain implicit access to resources\nin the context of running a job. When a properly designed application is\nshared it may be used by many users to run jobs without the need to\nexplicitly grant permissions to associated resources such as systems and\nfile paths.\n\nFor more information on sharing please see `sharing`{.interpreted-text\nrole=\"doc\"}
"
{"filename": "apps.md", "prompt": "Deletion", "completion": "An application may be deleted and undeleted. Deletion means the\napplication is marked as deleted and is no longer available for use.\nNote that although this is a soft delete the operation is intended for\nuse when an application is to be permanently made unavailable for use.\nTo temporarily make an application unavailable for use please use\nsupport for enabling and disabling an application.\n\nBy default deleted applications will not be included in searches and\noperations on deleted applications will not be allowed. When listing\napplications the query parameter *showDeleted* may be used in order to\ninclude deleted applications in the results. Note that deletion applies\nto all version of an application. It is not possible to delete a\nspecific version."}
{"filename": "apps.md", "prompt": "Application Attributes Table", "completion": "+---------+-------+---------+----------------------------------------+\n| At      | Type  | Example | Notes                                  |\n| tribute |       |         |                                        |\n+=========+=======+=========+========================================+\n| tenant  | S     | des     | -   Name of the tenant for which the   |\n|         | tring | ignsafe |     application is defined.            |\n|         |       |         | -   *tenant* + \\$version\\* + *id* must |\n|         |       |         |     be unique.                         |\n+---------+-------+---------+----------------------------------------+\n| id      | S     | my      | -   Name of the application. URI safe, |\n|         | tring | -ds-app |     see RFC 3986.                      |\n|         |       |         | -   *tenant* + \\$version\\* + *id* must |\n|         |       |         |     be unique.                         |\n|         |       |         | -   Allowed characters: Alphanumeric   |\n|         |       |         |     \\[0-9a-zA-Z\\] and special          |\n|         |       |         |     characters \\[-.\\_\\~\\].             |\n|         |       |         | -   **Required** at creation time.     |\n+---------+-------+---------+----------------------------------------+\n| version | S     | 0.0.1   | -   Version of the application. URI    |\n|         | tring |         |     safe, see RFC 3986.                |\n|         |       |         | -   *tenant* + \\$version\\* + *id* must |\n|         |       |         |     be unique.                         |\n|         |       |         | -   Allowed characters: Alphanumeric   |\n|         |       |         |     \\[0-9a-zA-Z\\] and special          |\n|         |       |         |     characters \\[-.\\_\\~\\].             |\n|         |       |         | -   **Required** at creation time.     |\n+---------+-------+---------+----------------------------------------+\n| desc    | S     | A       | -   Optional description               |\n| ription | tring | sample  |                                        |\n|         |       | appl    |                                        |\n|         |       | ication |                                        |\n+---------+-------+---------+----------------------------------------+\n| owner   | S     | jdoe    | -   User name of *owner*.              |\n|         | tring |         | -   Variable references:               |\n|         |       |         |     *\\${apiUserId}
*                    |\n|         |       |         | -   Default is *\\${apiUserId
*         |\n+---------+-------+---------+----------------------------------------+\n| enabled | bo    | FALSE   | -   Indicates if application currently |\n|         | olean |         |     enabled for use. Default is TRUE.  |\n+---------+-------+---------+----------------------------------------+\n| runtime | enum  | SING    | -   Runtime to be used when executing  |\n|         |       | ULARITY |     the application.                   |\n|         |       |         | -   Runtimes: DOCKER, SINGULARITY, ZIP |\n|         |       |         | -   Default is DOCKER                  |\n+---------+-------+---------+----------------------------------------+\n| runtime | S     | 2.5.2   | -   Optional version or range of       |\n| Version | tring |         |     versions required.                 |\n+---------+-------+---------+----------------------------------------+\n| runtime | \\[e   |         | -   Options that apply to specific     |\n| Options | num\\] |         |     runtimes.                          |\n|         |       |         | -   Options: NONE, SINGULARITY_START,  |\n|         |       |         |     SINGULARITY_RUN                    |\n|         |       |         | -   If runtime is SINGULARITY then     |\n|         |       |         |     must have one of                   |\n|         |       |         |     SINGULARITY_START, SINGULARITY_RUN |\n|         |       |         | -   Default is NONE.                   |\n+---------+-------+---------+----------------------------------------+\n| contain | S     | docker. | -   Reference for the container image. |\n| erImage | tring | io/hell |     Other examples:                    |\n|         |       | o-world | -   Singularity:                       |\n|         |       |         |     shub://GodloveD/lolcow             |\n|         |       |         | -   Docker: tapis/hello-tapis:0.0.1    |\n|         |       |         | -   **Required** at creation time.     |\n+---------+-------+---------+----------------------------------------+\n| jobType | enum  | BATCH   | -   Default job type.                  |\n|         |       |         | -   Types: BATCH, FORK                 |\n|         |       |         | -   Jobs will be of this type by       |\n|         |       |         |     default. May be overridden in the  |\n|         |       |         |     job submit request.                |\n|         |       |         | -   Default is FORK.                   |\n+---------+-------+---------+----------------------------------------+\n| maxJobs | int   | 10      | -   Max number of jobs that can be     |\n|         |       |         |     running for this app on a system.  |\n|         |       |         | -   System may also limit the number   |\n|         |       |         |     of jobs.                           |\n|         |       |         | -   Set to -1 for unlimited. Default   |\n|         |       |         |     is unlimited.                      |\n+---------+-------+---------+----------------------------------------+\n| maxJobs | int   | 2       | -   Max number of jobs per job owner.  |\n| PerUser |       |         | -   System may also limit the number   |\n|         |       |         |     of jobs.                           |\n|         |       |         | -   Set to -1 for unlimited. Default   |\n|         |       |         |     is unlimited.                      |\n+---------+-------+---------+----------------------------------------+\n| st      | bo    | FALSE   | -   Indicates if a job request is      |\n| rictFil | olean |         |     allowed to have unnamed file       |\n| eInputs |       |         |     inputs.                            |\n|         |       |         | -   If TRUE then a job request may     |\n|         |       |         |     only use named file inputs defined |\n|         |       |         |     in the app.                        |\n|         |       |         | -   Default is FALSE.                  |\n+---------+-------+---------+----------------------------------------+\n| jobAtt  | Job   |         | -   Various attributes related to job  |\n| ributes | Attri |         |     execution.                         |\n|         | butes |         | -   See table below.                   |\n+---------+-------+---------+----------------------------------------+\n| tags    | \\[Str |         | -   List of tags as simple strings.    |\n|         | ing\\] |         |                                        |\n+---------+-------+---------+----------------------------------------+\n| notes   | S     | {\\\"pro  | -   Simple metadata in the form of a   |\n|         | tring | ject\\\": |     Json object.                       |\n|         |       | \\\"my    | -   Not used by Tapis.                 |\n|         |       | proj\\\"
 |                                        |\n+---------+-------+---------+----------------------------------------+\n| uuid    | UUID  | 20281   | -   Auto-generated by service.         |\n+---------+-------+---------+----------------------------------------+\n| created | Time  | 2020-0  | -   When the app was created.          |\n|         | stamp | 6-19T15 |     Maintained by service.             |\n|         |       | :10:43Z |                                        |\n+---------+-------+---------+----------------------------------------+\n| updated | Time  | 2020-0  | -   When the app was last updated.     |\n|         | stamp | 7-04T23 |     Maintained by service.             |\n|         |       | :21:22Z |                                        |\n+---------+-------+---------+----------------------------------------+"
{"filename": "apps.md", "prompt": "JobAttributes Table", "completion": "+---------+--------+---------+----------------------------------------+\n| At      | Type   | Example | Notes                                  |\n| tribute |        |         |                                        |\n+=========+========+=========+========================================+\n| desc    | String |         | -   Description to be filled in when   |\n| ription |        |         |     this application is used to run a  |\n|         |        |         |     job.                               |\n|         |        |         | -   Macros allow this to act as a      |\n|         |        |         |     template to be filled in at job    |\n|         |        |         |     runtime.                           |\n+---------+--------+---------+----------------------------------------+\n| execS   | String |         | -   Specific system on which the       |\n| ystemId |        |         |     application is to be run.          |\n+---------+--------+---------+----------------------------------------+\n| exe     | String |         | -   Directory where application assets |\n| cSystem |        |         |     are staged.                        |\n| ExecDir |        |         | -   Current working directory at       |\n|         |        |         |     application launch time.           |\n|         |        |         | -   Macro template variables such as   |\n|         |        |         |     \\${JobWorkingDir}
 may be used.     |\n|         |        |         | -   Default is                         |\n|         |        |         |     \\${JobWorkingDir
/jobs/\\${JobUUID
 |\n+---------+--------+---------+----------------------------------------+\n| exec    | String |         | -   Directory where Tapis is to stage  |\n| SystemI |        |         |     the inputs required by the         |\n| nputDir |        |         |     application.                       |\n|         |        |         | -   Macro template variables such as   |\n|         |        |         |     \\${JobWorkingDir
 may be used.     |\n|         |        |         | -   Default is                         |\n|         |        |         |     \\${JobWorkingDir
/jobs/\\${JobUUID
 |\n+---------+--------+---------+----------------------------------------+\n| execS   | String |         | -   Directory where Tapis expects the  |\n| ystemOu |        |         |     application to store its final     |\n| tputDir |        |         |     output results.                    |\n|         |        |         | -   Files here are candidates for      |\n|         |        |         |     archiving.                         |\n|         |        |         | -   Macro template variables such as   |\n|         |        |         |     \\${JobWorkingDir
 may be used.     |\n|         |        |         | -   Default is                         |\n|         |        |         |     \\${                                |\n|         |        |         | JobWorkingDir
/jobs/\\${JobUUID
/output |\n+---------+--------+---------+----------------------------------------+\n| exe     | String | normal  | -   LogicalQueue to use when running   |\n| cSystem |        |         |     the job.                           |\n| Logic   |        |         |                                        |\n| alQueue |        |         |                                        |\n+---------+--------+---------+----------------------------------------+\n| a       | String |         | -   System to use when archiving       |\n| rchiveS |        |         |     outputs.                           |\n| ystemId |        |         |                                        |\n+---------+--------+---------+----------------------------------------+\n| ar      | String |         | -   Directory on *archiveSystemId*     |\n| chiveSy |        |         |     where outputs will be placed.      |\n| stemDir |        |         | -   This will be relative to the       |\n|         |        |         |     effective root directory defined   |\n|         |        |         |     for archiveSystemId.               |\n|         |        |         | -   Default is                         |\n|         |        |         |     \\${JobWorkingDir
/jobs/\\${JobUUID
 |\n+---------+--------+---------+----------------------------------------+\n| arc     | b      |         | -   Indicates if outputs should be     |\n| hiveOnA | oolean |         |     archived if there is an error      |\n| ppError |        |         |     while running job.                 |\n|         |        |         | -   The default is TRUE.               |\n+---------+--------+---------+----------------------------------------+\n| isMpi   | b      |         | -   Indicates that application is to   |\n|         | oolean |         |     be executed as an MPI job.         |\n|         |        |         | -   The default is FALSE.              |\n+---------+--------+---------+----------------------------------------+\n| mpiCmd  | String | > \\\"m   | -   Command used to launch MPI jobs.   |\n|         |        | pirun\\\" | -   Prepended to the command used to   |\n|         |        | >       |     execute the application.           |\n|         |        | \\\"ibrun | -   Conflicts with cmdPrefix if isMpi  |\n|         |        | > -n    |     is set.                            |\n|         |        | > 4\\\"   |                                        |\n+---------+--------+---------+----------------------------------------+\n| cm      | String |         | -   String prepended to the            |\n| dPrefix |        |         |     application invocation command.    |\n|         |        |         | -   Conflicts with mpiCmd if isMpi is  |\n|         |        |         |     set.                               |\n+---------+--------+---------+----------------------------------------+\n| param   | Parame |         | -   Various collections used during    |\n| eterSet | terSet |         |     job execution.                     |\n|         |        |         | -   App arguments, container           |\n|         |        |         |     arguments, scheduler options,      |\n|         |        |         |     environment variables, etc.        |\n|         |        |         | -   See table below.                   |\n+---------+--------+---------+----------------------------------------+\n| fil     | \\      |         | -   Collection of file inputs that     |\n| eInputs | [FileI |         |     must be staged for the             |\n|         | nput\\] |         |     application.                       |\n|         |        |         | -   Each input must have a name.       |\n|         |        |         | -   *strictFileInputs* =TRUE means     |\n|         |        |         |     only inputs defined here may be    |\n|         |        |         |     specified for job.                 |\n|         |        |         | -   See table below.                   |\n+---------+--------+---------+----------------------------------------+\n| f       | \\[File |         | -   Collection of arrays of inputs     |\n| ileInpu | InputA |         |     that must be staged for the        |\n| tArrays | rray\\] |         |     application.                       |\n|         |        |         | -   Each input must have a name. All   |\n|         |        |         |     inputs in an array have the same   |\n|         |        |         |     target directory.                  |\n|         |        |         | -   *strictFileInputs* =TRUE means     |\n|         |        |         |     only inputs defined here may be    |\n|         |        |         |     specified for job.                 |\n|         |        |         | -   See table below.                   |\n+---------+--------+---------+----------------------------------------+\n| no      | int    |         | -   Number of nodes to request during  |\n| deCount |        |         |     job submission.                    |\n+---------+--------+---------+----------------------------------------+\n| cores   | int    |         | -   Number of cores per node to        |\n| PerNode |        |         |     request during job submission.     |\n+---------+--------+---------+----------------------------------------+\n| m       | int    |         | -   Memory in megabytes to request     |\n| emoryMB |        |         |     during job submission.             |\n+---------+--------+---------+----------------------------------------+\n| max     | int    |         | -   Run time to request during job     |\n| Minutes |        |         |     submission.                        |\n+---------+--------+---------+----------------------------------------+\n| subscr  |        |         | -   Notification subscriptions.        |\n| iptions |        |         | -   See table below.                   |\n+---------+--------+---------+----------------------------------------+\n| tags    | \\[St   |         | -   List of tags as simple strings.    |\n|         | ring\\] |         |                                        |\n+---------+--------+---------+----------------------------------------+"
{"filename": "apps.md", "prompt": "ParameterSet Attributes Table", "completion": "+---------+-------+---------+----------------------------------------+\n| At      | Type  | Example | Notes                                  |\n| tribute |       |         |                                        |\n+=========+=======+=========+========================================+\n| appArgs | \\[    |         | -   Command line arguments passed to   |\n|         | Arg\\] |         |     the application.                   |\n|         |       |         | -   See table below.                   |\n+---------+-------+---------+----------------------------------------+\n| contai  | \\[    |         | -   Command line arguments passed to   |\n| nerArgs | Arg\\] |         |     the container runtime.             |\n|         |       |         | -   See table below.                   |\n+---------+-------+---------+----------------------------------------+\n| sc      | \\[    |         | -   Scheduler options passed to the    |\n| heduler | Arg\\] |         |     HPC batch scheduler.               |\n| Options |       |         | -   See table below.                   |\n+---------+-------+---------+----------------------------------------+\n| envVa   | \\     |         | -   Environment variables placed into  |\n| riables | [KeyV |         |     the runtime environment.           |\n|         | alueP |         | -   Each entry has *key* (required)    |\n|         | air\\] |         |     and *value* (optional) as well as  |\n|         |       |         |     other attributes.                  |\n|         |       |         | -   See table *KeyValuePair            |\n|         |       |         |     Attributes* below for more         |\n|         |       |         |     information.                       |\n+---------+-------+---------+----------------------------------------+\n| archiv  | Arc   |         | -   Sets of files to include or        |\n| eFilter | hiveF |         |     exclude when archiving.            |\n|         | ilter |         | -   Default is to include all files in |\n|         |       |         |     *execSystemOutputDir*.             |\n|         |       |         | -   See table below.                   |\n+---------+-------+---------+----------------------------------------+"}
{"filename": "apps.md", "prompt": "ArchiveFilter Attributes Table", "completion": "+---------+-------+---------+----------------------------------------+\n| At      | Type  | Example | Notes                                  |\n| tribute |       |         |                                        |\n+=========+=======+=========+========================================+\n| i       | \\[Str |         | -   Files to include when archiving    |\n| ncludes | ing\\] |         |     after execution of the             |\n|         |       |         |     application.                       |\n|         |       |         | -   excludes list has precedence.      |\n+---------+-------+---------+----------------------------------------+\n| e       | \\[Str |         | -   Files to skip when archiving after |\n| xcludes | ing\\] |         |     execution of the application.      |\n|         |       |         | -   excludes list has precedence.      |\n+---------+-------+---------+----------------------------------------+\n| incl    | bo    |         | -   Indicates if Tapis generated       |\n| udeLaun | olean |         |     launch scripts are to be included  |\n| chFiles |       |         |     when archiving.                    |\n|         |       |         | -   The default is TRUE.               |\n+---------+-------+---------+----------------------------------------+"}
{"filename": "apps.md", "prompt": "Arg Attributes Table", "completion": "For more information on how the *inputMode* attribute is used during job\nprocessing, please see\n[JobsArgProcessing](https://tapis.readthedocs.io/en/latest/technical/jobs.html#argument-processing).\n\n+---------+-----+----------+-----------------------------------------+\n| At      | T   | Example  | Notes                                   |\n| tribute | ype |          |                                         |\n+=========+=====+==========+=========================================+\n| name    | Str |          | -   Identifying label associated with   |\n|         | ing |          |     the argument.                       |\n|         |     |          | -   **Required** at creation time.      |\n+---------+-----+----------+-----------------------------------------+\n| desc    | Str |          | -   Optional description of the         |\n| ription | ing |          |     argument which may include usage,   |\n|         |     |          |     purpose, etc.                       |\n+---------+-----+----------+-----------------------------------------+\n| in      | e   |          | -   Indicates how argument is to be     |\n| putMode | num |          |     treated when processing individual  |\n|         |     |          |     job requests.                       |\n|         |     |          | -   Modes: REQUIRED, FIXED,             |\n|         |     |          |     INCLUDE_ON_DEMAND,                  |\n|         |     |          |     INCLUDE_BY_DEFAULT                  |\n|         |     |          | -   Default is INCLUDE_ON_DEMAND.       |\n|         |     |          | -   REQUIRED: Must be provided in a job |\n|         |     |          |     request.                            |\n|         |     |          | -   FIXED: Completely defined in the    |\n|         |     |          |     application and not overridable in  |\n|         |     |          |     a job request.                      |\n|         |     |          | -   INCLUDE_ON_DEMAND: Included if      |\n|         |     |          |     referenced in a job request.        |\n|         |     |          | -   INCLUDE_BY_DEFAULT: Included unless |\n|         |     |          |     *include=false* in a job request.   |\n+---------+-----+----------+-----------------------------------------+\n| arg     | Str |          | -   Value for the argument              |\n|         | ing |          | -   **Required** at creation time.      |\n+---------+-----+----------+-----------------------------------------+\n| notes   | Str | {\\\"fiel  | -   Metadata in the form of a Json      |\n|         | ing | dType\\\": |     object, such as type, allowed       |\n|         |     | \\\"int\\\"}
 |     values, etc.                        |\n|         |     |          | -   Not used by Tapis.                  |\n+---------+-----+----------+-----------------------------------------+"
{"filename": "apps.md", "prompt": "KeyValuePair Attributes Table", "completion": "+----------+---+----------+------------------------------------------+\n| A        | T | Example  | Notes                                    |\n| ttribute | y |          |                                          |\n|          | p |          |                                          |\n|          | e |          |                                          |\n+==========+===+==========+==========================================+\n| key      | S | > \\\"INPU | -   Environment variable name. Required. |\n|          | t | T_FILE\\\" |                                          |\n|          | r |          |                                          |\n|          | i |          |                                          |\n|          | n |          |                                          |\n|          | g |          |                                          |\n+----------+---+----------+------------------------------------------+\n| value    | S | > \\\"/    | -   Environment variable value           |\n|          | t | tmp/file |                                          |\n|          | r | .input\\\" |                                          |\n|          | i |          |                                          |\n|          | n |          |                                          |\n|          | g |          |                                          |\n+----------+---+----------+------------------------------------------+\n| des      | S |          | -   Description                          |\n| cription | t |          |                                          |\n|          | r |          |                                          |\n|          | i |          |                                          |\n|          | n |          |                                          |\n|          | g |          |                                          |\n+----------+---+----------+------------------------------------------+\n| i        | e | >        | -   Indicates how argument is to be      |\n| nputMode | n | REQUIRED |     treated when processing individual   |\n|          | u |          |     job requests.                        |\n|          | m |          | -   Modes: REQUIRED, FIXED,              |\n|          |   |          |     INCLUDE_ON_DEMAND,                   |\n|          |   |          |     INCLUDE_BY_DEFAULT                   |\n|          |   |          | -   Default is INCLUDE_BY_DEFAULT.       |\n|          |   |          | -   REQUIRED: Must be provided in a job  |\n|          |   |          |     request or application definition.   |\n|          |   |          | -   FIXED: Not overridable in            |\n|          |   |          |     application or job request.          |\n|          |   |          | -   INCLUDE_ON_DEMAND: Included if       |\n|          |   |          |     referenced in a job request.         |\n|          |   |          | -   INCLUDE_BY_DEFAULT: Included unless  |\n|          |   |          |     *include=false* in a job request.    |\n+----------+---+----------+------------------------------------------+\n| notes    | S | > \\\"{}
\\\" | -   Simple metadata in the form of a     |\n|          | t |          |     Json object.                         |\n|          | r |          | -   Not used by Tapis.                   |\n|          | i |          |                                          |\n|          | n |          |                                          |\n|          | g |          |                                          |\n+----------+---+----------+------------------------------------------+"
{"filename": "apps.md", "prompt": "FileInput Attributes Table", "completion": "+--------+-----+----------+-------------------------------------------+\n| Att    | T   | Example  | Notes                                     |\n| ribute | ype |          |                                           |\n+========+=====+==========+===========================================+\n| name   | Str |          | -   Identifying label associated with the |\n|        | ing |          |     input. Typically used during a job    |\n|        |     |          |     request.                              |\n|        |     |          | -   **Required** at creation time.        |\n+--------+-----+----------+-------------------------------------------+\n| descr  | Str |          | -   Optional description.                 |\n| iption | ing |          |                                           |\n+--------+-----+----------+-------------------------------------------+\n| inp    | e   |          | -   Indicates how input is to be treated  |\n| utMode | num |          |     when processing individual job        |\n|        |     |          |     requests.                             |\n|        |     |          | -   REQUIRED, OPTIONAL, FIXED             |\n|        |     |          | -   Default is OPTIONAL.                  |\n+--------+-----+----------+-------------------------------------------+\n| au     | b   |          | -   Indicates if Jobs service should      |\n| toMoun | ool |          |     automatically mount file paths into   |\n| tLocal | ean |          |     containers.                           |\n|        |     |          | -   Note that not all container runtimes  |\n|        |     |          |     require this.                         |\n|        |     |          | -   Setting to FALSE allows user complete |\n|        |     |          |     control using *containerArg*          |\n|        |     |          |     parameters.                           |\n|        |     |          | -   Default is TRUE.                      |\n+--------+-----+----------+-------------------------------------------+\n| sou    | Str |          | -   Source used by Jobs service when      |\n| rceUrl | ing |          |     staging file inputs.                  |\n+--------+-----+----------+-------------------------------------------+\n| targ   | Str |          | -   Target path used by Jobs service when |\n| etPath | ing |          |     staging file inputs.                  |\n+--------+-----+----------+-------------------------------------------+"}
{"filename": "apps.md", "prompt": "FileInputArray Attributes Table", "completion": "+--------+-----+----------+-------------------------------------------+\n| Att    | T   | Example  | Notes                                     |\n| ribute | ype |          |                                           |\n+========+=====+==========+===========================================+\n| name   | Str |          | -   Identifying label associated with the |\n|        | ing |          |     input. Typically used during a job    |\n|        |     |          |     request.                              |\n|        |     |          | -   **Required** at creation time.        |\n+--------+-----+----------+-------------------------------------------+\n| descr  | Str |          | -   Optional description.                 |\n| iption | ing |          |                                           |\n+--------+-----+----------+-------------------------------------------+\n| inp    | e   |          | -   REQUIRED, OPTIONAL, FIXED             |\n| utMode | num |          | -   Default is OPTIONAL.                  |\n+--------+-----+----------+-------------------------------------------+\n| sour   | \\   |          | -   Array of sources used by Jobs service |\n| ceUrls | [St |          |     when staging file inputs.             |\n|        | rin |          |                                           |\n|        | g\\] |          |                                           |\n+--------+-----+----------+-------------------------------------------+\n| tar    | Str |          | -   Target directory used by Jobs service |\n| getDir | ing |          |     when staging file inputs.             |\n+--------+-----+----------+-------------------------------------------+"}
{"filename": "apps.md", "prompt": "Searching", "completion": "The service provides a way for users to search for applications based on\na list of search conditions provided either as query parameters for a\nGET call or a list of conditions in a request body for a POST call to a\ndedicated search endpoint."}
{"filename": "apps.md", "prompt": "Search using GET", "completion": "To search when using a GET request to the `apps` endpoint a list of\nsearch conditions may be specified using a query parameter named\n`search`. Each search condition must be:\n\n> -   surrounded with parentheses\n> -   have three parts separated by the character `.`\n> -   be joined using the character `~`.\n\nAll conditions are combined using logical AND. The general form for\nspecifying the query parameter is as follows:\n\n    ?search=(<attribute_1>.<op_1>.<value_1>)~(<attribute_2>.<op_2>.<value_2>)~ ... ~(<attribute_N>.<op_N>.<value_N>)\n\nAttribute names are given in the table above and may be specified using\nCamel Case or Snake Case.\n\nSupported operators: `eq` `neq` `gt` `gte` `lt` `lte` `in` `nin` `like`\n`nlike` `between` `nbetween`\n\nExample CURL command to search for applications that have `Test` in the\nid, are of type FORK and allow for *maxJobs* greater than `5`:\n\n    $ curl -H \"X-Tapis-Token: $JWT\" https://tacc.tapis.io/v3/apps?search=\"(id.like.*Test*)~(job_type.eq.FORK)~(max_jobs.gt.5)\"\n\nNotes:\n\n-   For the `like` and `nlike` operators the wildcard character `*`\n    matches zero or more characters and `!` matches exactly one\n    character.\n-   For the `between` and `nbetween` operators the value must be a two\n    item comma separated list of unquoted values.\n-   If there is only one condition the surrounding parentheses are\n    optional.\n-   In a shell environment the character `&` separating query parameters\n    must be escaped with a backslash.\n-   In a shell environment the query value must be surrounded by double\n    quotes and the following characters must be escaped with a backslash\n    in order to be properly interpreted by the shell:\n    -   `\"` `\\` ``\\`\n-   Attribute names may be specified using Camel Case or Snake Case.\n-   Following complex attributes not supported when searching:\n    -   `jobAttributes` `notes`"}
{"filename": "apps.md", "prompt": "Dedicated Search Endpoint", "completion": "The service provides the dedicated search endpoint `apps/search/apps`\nfor specifying complex queries. Using a GET request to this endpoint\nprovides functionality similar to above but with a different syntax. For\nmore complex queries a POST request may be used with a request body\nspecifying the search conditions using an SQL-like syntax."}
{"filename": "apps.md", "prompt": "Search using GET on Dedicated Endpoint", "completion": "Sending a GET request to the search endpoint provides functionality very\nsimilar to that provided for the endpoint `apps` described above. A list\nof search conditions may be specified using a series of query\nparameters, one for each attribute. All conditions are combined using\nlogical AND. The general form for specifying the query parameters is as\nfollows:\n\n    ?<attribute_1>.<op_1>=<value_1>&<attribute_2>.<op_2>=<value_2>)& ... &<attribute_N>.<op_N>=<value_N>\n\nAttribute names are given in the table above and may be specified using\nCamel Case or Snake Case.\n\nSupported operators: `eq` `neq` `gt` `gte` `lt` `lte` `in` `nin` `like`\n`nlike` `between` `nbetween`\n\nExample CURL command to search for applications that have `Test` in the\nid, are of type FORK and allow for *maxJobs* greater than `5`:\n\n    $ curl -H \"X-Tapis-Token: $JWT\" https://tacc.tapis.io/v3/apps/search/apps?id.like=*Test*\\&job_type.eq=FORK\\&max_jobs.gt=5\n\nNotes:\n\n-   For the `like` and `nlike` operators the wildcard character `*`\n    matches zero or more characters and `!` matches exactly one\n    character.\n-   For the `between` and `nbetween` operators the value must be a two\n    item comma separated list of unquoted values.\n-   In a shell environment the character `&` separating query parameters\n    must be escaped with a backslash.\n-   Attribute names may be specified using Camel Case or Snake Case.\n-   Following complex attributes not supported when searching:\n    -   `jobAttributes` `tags` `notes`"}
{"filename": "apps.md", "prompt": "Search using POST on Dedicated Endpoint", "completion": "More complex search queries are supported when sending a POST request to\nthe endpoint `apps/search/apps`. For these requests the request body\nmust contain json with a top level property name of `search`. The\n`search` property must contain an array of strings specifying the search\ncriteria in an SQL-like syntax. The array of strings are concatenated to\nform the full search query. The full query must be in the form of an\nSQL-like `WHERE` clause. Note that not all SQL features are supported.\n\nFor example, to search for apps that are owned by `jdoe` and of type\n`FORK` or owned by `jsmith` and allow for *maxJobs* less than `5` create\na local file named `app_search.json` with following json:\n\n    {\n      \"search\":\n        [\n          \"(owner = 'jdoe' AND job_type = 'FORK') OR\",\n          \"(owner = 'jsmith' AND max_jobs < 5)\"\n        ]\n    }
\n\nTo execute the search use a CURL command similar to the following:\n\n    $ curl -X POST -H \"content-type: application/json\" -H \"X-Tapis-Token: $JWT\" https://tacc.tapis.io/v3/apps/search/apps -d @app_search.json\n\nNotes:\n\n-   String values must be surrounded by single quotes.\n-   Values for BETWEEN must be surrounded by single quotes.\n-   Search query parameters as described above may not be used in\n    conjunction with a POST request.\n-   SQL features not supported include:\n    -   `IS NULL` and `IS NOT NULL`\n    -   Arithmetic operations\n    -   Unary operators\n    -   Specifying escape character for `LIKE` operator"
{"filename": "apps.md", "prompt": "Map of SQL operators to Tapis operators", "completion": "---------------------------------\n  Sql Operator     Tapis Operator\n  ---------------- ----------------\n  =                eq\n\n  \\<\\>             neq\n\n  \\<               lt\n\n  \\<=              lte\n\n  \\>               gt\n\n  \\>=              gte\n\n  LIKE             like\n\n  NOT LIKE         nlike\n\n  BETWEEN          between\n\n  NOT BETWEEN      nbetween\n\n  IN               in\n\n  NOT IN           nin\n  ---------------------------------"}
{"filename": "apps.md", "prompt": "Sort, Limit, Select and ListType", "completion": "When a list of Applications is retrieved the service provides for\nsorting, filtering and limiting the results. By default, only resources\nowned by you will be included. The service provides a way for you to\nrequest that all resources accessible to you be included. This is\ndetermined by the query parameter *listType*.\n\nWhen retrieving either a list of resources or a single resource the\nservice also provides a way to *select* which fields (i.e. attributes)\nare included in the results. Sorting, limiting and attribute selection\nare supported using query parameters."}
{"filename": "apps.md", "prompt": "Selecting", "completion": "When retrieving applications the fields (i.e. attributes) to be returned\nmay be specified as a comma separated list using a query parameter named\n`select`. Attribute names may be given using Camel Case or Snake Case.\n\nNotes:\n\n> -   Special select keywords are supported: `allAttributes` and\n>     `summaryAttributes`\n> -   Summary attributes include:\n>     -   `id`, `version`, `owner`\n> -   By default all attributes are returned when retrieving a single\n>     resource via the endpoint apps/\\<app_id\\>.\n> -   By default summary attributes are returned when retrieving a list\n>     of applications.\n> -   Specifying nested attributes is not supported.\n> -   The attribute `id` is always returned.\n\nFor example, to return only the attributes `version` and\n`containerImage` the CURL command would look like this:\n\n    $ curl -H \"X-Tapis-Token: $JWT\" https://tacc.tapis.io/v3/apps?select=version,containerImage\n\nThe response should look similar to the following:\n\n    {\n       \"result\": [\n           {\n               \"id\": \"TestApp1\",\n               \"version\": \"0.0.1\",\n               \"containerImage\": \"containterimage1\"\n           }
,\n           {\n               \"id\": \"JobApp1\",\n               \"version\": \"0.0.1\",\n               \"containerImage\": \"containterimage1\"\n           
,\n           {\n               \"id\": \"JobAppWithInput\",\n               \"version\": \"0.0.1\",\n               \"containerImage\": \"containterimage1\"\n           
,\n           {\n               \"id\": \"SleepSeconds\",\n               \"version\": \"0.0.1\",\n               \"containerImage\": \"tapis/testapps:main\"\n           
\n       ],\n       \"status\": \"success\",\n       \"message\": \"TAPIS_FOUND Apps found: 11 applications\",\n       \"version\": \"0.0.1-SNAPSHOT\",\n       \"metadata\": {\n           \"recordCount\": 4,\n           \"recordLimit\": 100,\n           \"recordsSkipped\": 0,\n           \"orderBy\": null,\n           \"startAfter\": null,\n           \"totalCount\": -1\n       
\n    
"
{"filename": "apps.md", "prompt": "Sorting", "completion": "The query parameter for sorting is named `orderBy` and the value is the\nattribute name to sort on with an optional sort direction. The general\nformat is `<attribute_name>(<dir>)`. The direction may be `asc` for\nascending or `desc` for descending. The default direction is ascending.\n\nExamples:\n\n> -   orderBy=id\n> -   orderBy=id(asc)\n> -   orderBy=name(desc),created\n> -   orderBy=id(asc),created(desc)"}
{"filename": "apps.md", "prompt": "Limiting", "completion": "Additional query parameters may be used in order to limit the number and\nstarting point for results. This is useful for implementing paging. The\nquery parameters are:\n\n> -   `limit` - Limit number of items returned. For example limit=10.\n>     -   Use 0 or less for unlimited.\n>     -   Default is 100.\n> -   `skip` - Number of items to skip. For example skip=10.\n>     -   May not be used with startAfter.\n>     -   Default is 0.\n> -   `startAfter` - Where to start when sorting. For example\n>     limit=10&orderBy=id(asc),created(desc)&startAfter=101\n>     -   May not be used with `skip`.\n>     -   Must also specify `orderBy`.\n>     -   The value of `startAfter` applies to the major `orderBy`\n>         field.\n>     -   Condition is context dependent. For ascending the condition is\n>         value \\> `startAfter` and for descending the condition is\n>         value \\< `startAfter`.\n\nWhen implementing paging it is recommend to always use `orderBy` and\nwhen possible use `limit+startAfter` rather than `limit+skip`. Sorting\nshould always be included since returned results are not guaranteed to\nbe in the same order for each call. The combination of\n`limit+startAfter` is preferred because `limit+skip` is more likely to\nresult in inconsistent results as records are added and removed. Using\n`limit+startAfter` works best when the attribute has a natural\nsequential ordering such as when an attribute represents a timestamp or\na sequential ID."}
{"filename": "apps.md", "prompt": "ListType", "completion": "By default, you will only see the resources that you own. The query\nparameter *listType* allows you to see additional resources that are\navailable to you.\n\nOptions:\n\n*OWNED*\n\n:   Include only items owned by you (Default)\n\n*SHARED_PUBLIC*\n\n:   Include only items shared publicly\n\n*ALL*\n\n:   Include all items you are authorized to view."}
{"filename": "apps.md", "prompt": "Tapis Responses", "completion": "For requests that return a list of resources the response result object\nwill contain the list of resource records that match the user\\'s query\nand the response metadata object will contain information related to\nsorting and limiting.\n\nThe metadata object will contain the following information:\n\n> -   `recordCount` - Actual number of records returned.\n> -   `recordLimit` - The limit query parameter specified in the\n>     request. -1 if query parameter was not specified.\n> -   `recordsSkipped` - The skip query parameter specified in the\n>     request. -1 if query parameter was not specified.\n> -   `orderBy` - The orderBy query parameter specified in the request.\n>     Empty string if query parameter was not specified.\n> -   `startAfter` - The startAfter query parameter specified in the\n>     request. Empty string if query parameter was not specified.\n> -   `totalCount` - Total number of records that would have been\n>     returned without a limit query parameter being imposed. -1 if\n>     total count was not computed.\n\nFor performance reasons computation of `totalCount` is only determined\non demand. This is controlled by the boolean query parameter\n`computeTotal`. By default `computeTotal` is *false*.\n\nExample query and response:\n\nQuery:\n\n    $ curl -H \"X-Tapis-Token: $JWT\" https://tacc.tapis.io/v3/apps?limit=2&orderBy=id(desc)\n\nResponse:\n\n    {\n       \"result\": [\n           {\n               \"id\": \"TestApp1\",\n               \"version\": \"0.0.1\",\n               \"owner\": \"testuser2\"\n           }
,\n           {\n               \"id\": \"tacc-sample-app\",\n               \"version\": \"0.1\",\n               \"owner\": \"testuser2\"\n           
\n       ],\n       \"status\": \"success\",\n       \"message\": \"TAPIS_FOUND Apps found: 2 applications\",\n       \"version\": \"0.0.1-SNAPSHOT\",\n       \"metadata\": {\n           \"recordCount\": 2,\n           \"recordLimit\": 2,\n           \"recordsSkipped\": 0,\n           \"orderBy\": \"id(desc)\",\n           \"startAfter\": null,\n           \"totalCount\": -1\n       
\n     
"
{"filename": "authentication.md", "prompt": "Sites", "completion": "Tapis supports geographically distributed deployments where different\ncomponents are running in different data centers and managed by\ndifferent institutions. These physically isolated installations of Tapis\nsoftware are referred to as *sites*. There is a single *primary site*\nand zero or more *associate sites* within a Tapis installation."}
{"filename": "authentication.md", "prompt": "Primary Site", "completion": "The primary site in a Tapis installation runs a complete set of Tapis\nAPI services and all associated 3rd-party services, such as databases\nand message queues. The creation of new sites is coordinated through the\nprimary site, and the primary site runs the unique instance of the Sites\nand Tenants API (see `Tenants`{.interpreted-text role=\"ref\"}
 below)\nwhich maintain the complete registry of all sites and tenants in the\ninstallation.\n\nThe primary site of the main Tapis installation is hosted at the Texas\nAdvanced Computing Center at the tapis.io domain."
{"filename": "authentication.md", "prompt": "Associate Sites", "completion": "Associate sites are required to run the Tapis Security Kernel, a\ncompliant Token Generator API, and one or more additional Tapis\nservices. Each associate site is managed and operated by a separate,\npartner institution. For Tapis services not run at the associate site,\nthe corresponding service at the primary site is used for requests. In\nthis way, partner institutions can choose which Tapis services to run\nwithin their institution and leverage the primary site deployment for\nthe rest."}
{"filename": "authentication.md", "prompt": "Deployment", "completion": "The official Tapis deployment tooling targets the Kubernetes container\norchestration platform. The project maintains a set of deployment\ntemplates which can be used in conjunction with configuration files to\ndeploy Tapis services. If your institution is interested in becomming a\nTapis associate site please contact us.\n\nDetails about the current list of sites is available from the tenants\nAPI. For example, one can retrieve the full list of sites as follows:\n\nWith PySDK:\n\n\n\nWith CURL:\n\n\n\nThe response will look similar to the following (the response below is\ntruncated for brevity):\n\n\n\nEach site has a `site_id` as well as a list of Tapis services it\nprovides and the tenant ID of the administrative tenant (`admin_tenant`)\nassociated with it."}
{"filename": "authentication.md", "prompt": "Tenants {#Tenants}
", "completion": "Tapis is a *multi-tenant* platform, meaning that different projects (or\n*tenants*) can have logically isolated views of the Tapis objects (i.e.,\nthe systems, files, actors, etc.) they create for their project.\n\nEach tenant is made up of the following:\n\n1.  A base URL with which to access the tenant; by default, the base URL\n    takes the form `https://<tenant_id>.tapis.io` where `tenant_id` is a\n    short, unique identifier for the tenant in the Tapis system. For\n    example, `https://tacc.tapis.io` is the base URL for the `tacc`\n    tenant.\n2.  An *authenticator* providing the rules for who can authenticate in\n    the tenant.\n\nAdditionally, each tenant is \\\"managed\\\" by a site.\n\nTo see the current list of tenants registered with Tapis, we can use the\ntenants API.\n\nWith PySDK:\n\n\n\nWith CURL:\n\n\n\nThe response will look similar to the following (the response below is\ntruncated for brevity):\n\n\n\nHere we see the first two tenants registered in the Tapis framework, the\n`tacc` and `dev` tenants.\n\nIn general, the rules for authentication vary from tenant to tenant\nwithin Tapis. For example, in the `tacc` tenant, any user with a valid\nTACC account can authenticate and use the APIs. The `dev` tenant is a\ndevelopment sandbox with test accounts used by the core Tapis team.\n\nThis documentation focuses on the `tacc` tenant; however, much of what\nfollows in the subsequent sections will be the same regardless of the\ntenant you are using."
{"filename": "authentication.md", "prompt": "Authentication", "completion": "The default authenticator provided by the Tapis project is based on\nOAuth2, and this is the authentication mechanism in place for the `tacc`\ntenant. The OAuth2-based authentication services are available via the\n`/v3/oauth2` endpoints.\n\nOAuth uses different *grant type flows* for generating tokens in\ndifferent situations. We do not provide a comprehensive guide to OAuth2;\nfor that, we refer the reader to the [OAuth2\ndocs](https://oauth.net/2/). Instead, we provide a guide to the two most\ncommon use cases for users: generating tokens for themselves using the\n*password* grant flow, and generating tokens on behalf of others in a\nweb application using the *authorization code* grant flow.\n\nIn the PySDK examples that follow, we will tacitly assume the\n`tapipy.tapis.Tapis` object has been instantiated as the Python object\n`t`. There are several options in the `Tapis` constructor, but the basic\noptions include `base_url` and `username`, for example:"}
{"filename": "authentication.md", "prompt": "Password Grant - Generating a Token For Yourself", "completion": "The simplest case is that you want to generate a Tapis OAuth token for\nyourself; to do this you can use the *password* grant flow, providing\nyour username and password.\n\nTapis v3 tries to make this process as easy as possible by providing a\nsimplified version of the password grant flow that does not require an\nOAuth client (see the `oauth-clients-label`{.interpreted-text\nrole=\"ref\"}
 section).\n\nWith PySDK:\n\n\n\nWith CURL:\n\n\n\nIn the PySDK, the access token is a first-class Python object stored on\nthe Tapis object (the `t` in the examples above). We can inspect it\n\n\n\nWhat we see is that the `access_token.access_token` is a Python string\nrepresenting a JSON Web Token ([JWT](https://jwt.io/introduction/)).\nJWTs are cryptographically signed with the private key associated with\nthe tenant, and anyone can validate the signature by using the\ncorresponding public key associated with the tenant (see Tenants section\nabove). The public key for each tenant is available from the Tenants\nAPI. The core Tapis services will validate the access token sent on a\ngiven API call using the public key associated with the tenant to verify\nthe JWT signature."
{"filename": "authentication.md", "prompt": "Using a Token", "completion": "In order to use an access token in an API request to Tapis, pass the\ntoken in as the value of the `X-Tapis-Token` header. The PySDK will\nautomatically send the token via this header for you. In CURL examples\nused throughout this documentation, we assume the raw JWT string\nrepresenting an access token (like the above) has been exported as a\nshell variable; i.e.,\n\n\n\nWith that variable set, we can use the `-H` flag with curl to set the\n`X-Tapis-Token` header as follows:\n\n\n\nNote also the *claims* associated with the access token. These claims\nprovide information about the token, including the user it represents\n(`apitest` in the above example), the tenant it belongs to (`tacc`\nabove) when it expires, etc. Tapis tokens always include the following\nstandard claims:\n\n+---------------+-------------------------+---------------------------+\n| Claim         | Description             | Example Value             |\n+===============+=========================+===========================+\n| sub           | The subject of the      | <apitest@tacc>            |\n|               | token; the subject      |                           |\n|               | uniquely identifies the |                           |\n|               | user in a Tapis         |                           |\n|               | installation. The       |                           |\n|               | format is `user` @      |                           |\n|               | `tenant`                |                           |\n+---------------+-------------------------+---------------------------+\n| exp           | The expiry associated   | > 1595099456              |\n|               | with the token.         |                           |\n+---------------+-------------------------+---------------------------+\n| jti           | Unique identifier for   | f7b9b19e-60               |\n|               | the token.              | 99-4f82-a722-b610c5d0ba0c |\n+---------------+-------------------------+---------------------------+\n| iss           | The identifier (URL) of | <https:/                  |\n|               | the issuer of the JWT.  | /tacc.tapis.io/v3/tokens> |\n|               | For Tapis, the issuer   |                           |\n|               | will be a Tokens API.   |                           |\n+---------------+-------------------------+---------------------------+\n\nAdditional custom claims specific to Tapis are namespaced with `tapis/`\nat the beginning of the claim name. The authenticator for each tenant\nmay optionally choose to support one or more of these additional claims.\nThe following claims are encouraged and supported by the default OAuth2\nTapis authenticator.\n\n  ----------------------------------------------------------------------------\n  Claim                  Description               Example Value\n  ---------------------- ------------------------- ---------------------------\n  tapis/tenant_id        The tenant of the         tacc\n                         subject.                  \n\n  tapis/username         The username of the       apitest\n                         subject.                  \n\n  tapis/token_type       Type of token: `access`   access\n                         or `refresh`              \n\n  tapis/account_type     Type of account: `user`   user\n                         or `service`              \n\n  tapis/delegation       Whether a delegation flow false\n                         was used to generate this \n                         token. (`true` or         \n                         `false`).                 \n\n  tapis/delegation_sub   For a delegation token,   <superuser@tacc>\n                         the subject who actually  \n                         generated the token. In   \n                         form `user` @ `tenant`    \n\n  tapis/client_id        The id of the OAuth       tacc.CIC.tokenapp\n                         client used to generate   \n                         the token.                \n\n  tapis/grant_type       The grant type used to    authorization_code\n                         generate the token.       \n  ----------------------------------------------------------------------------\n\nThe authenticator for your tenant may include additional claims not\nlisted here."}
{"filename": "authentication.md", "prompt": "OAuth Clients {#oauth-clients-label}
", "completion": "In order to use the more advanced OAuth2 flows, including any use of the\nauthorization code grant type and to generate refresh tokens with the\npassword grant type, you must generate an OAuth2 *client*. Clients in\nOAuth2 represent applications (for example, a web or mobile application)\nthat will interact with the OAuth2 server to generate tokens on behalf\nof one or more users. Clients are created and managed using the\n`/v3/oauth2/clients` endpoints."
{"filename": "authentication.md", "prompt": "Creating Clients", "completion": "To create a client, make a POST request the the Clients API. All fields\nare optional; if you do not pass a `client_id` or `client_key` in the\nrequest, the clients API will generate random ones for you. In order to\nuse the `authorize_code` grant type you will need to set the\n`callback_url` when registering your client (see\n`auth_code`{.interpreted-text role=\"ref\"}
). For a complete list of\navailable parameters, see the API live-docs for\n[Clients](https://tapis-project.github.io/live-docs/#tag/Clients).\n\nWith PySDK:\n\n\n\nWith CURL:\n\n\n\nThe response will be similar to"
{"filename": "authentication.md", "prompt": "Listing Clients", "completion": "With PySDK:\n\n\n\nWith CURL:\n\n\n\nThe response will be similar to"}
{"filename": "authentication.md", "prompt": "Deleting Clients", "completion": "You can also delete clients you are no longer using; just pass the\n`client_id` of the client to be deleted:\n\nWith PySDK:\n\n\n\nWith CURL:\n\n\n\nA null response is returned from a successful delete request."}
{"filename": "authentication.md", "prompt": "Authorization Code Grant - Generating Tokens For Users {#auth_code}
", "completion": "An important aspect of OAuth2 is that it enables applications to\ngenerate tokens on behalf of users without the applications needing to\npossess user credentials (i.e., passwords). In this section, we discuss\nusing the OAuth2 *authorization code* grant type to do just that.\n\nAssuming a Model-View-Controller (MVC) architecture, there are two\ncontrollers that must be written to support the authorization code grant\ntype flow.\n\n1.  A controller to determine if the user already has a valid access\n    token and direct them to the OAuth2 authorization server when they\n    do not. This controller starts the authorization code process. To do\n    so, it should:\n\n> -   First inform the user that they will be asked to authenticate with\n>     their tenant username and password and then be asked to grant\n>     authorization to your client application.\n> -   Redirect the user to the OAuth2 server\\'s authorization URL. In\n>     the default Tapis authenticator, the authorization URL path is\n>     `/v3/oauth2/authorize`; for example,\n>     `https://tacc.tapis.io/v3/oauth2/authorize` in the `tacc` tenant.\n> -   The redirect request should include the following query\n>     parameters:\n>     -   `client_id`: the id of your client.\n>     -   `redirect_uri`: the URI to redirect back to with the\n>         authorization code. This must match the `callback_url`\n>         parameter associated with your client.\n>     -   `response_type`: should always have the value `code`.\n\n2.  A controller to process the authorization code returned and retrieve\n    an access token on the user's behalf. This controller receives\n    requests containing authorization codes from the OAuth2 server after\n    the user has successfully authenticated with said OAuth2 server, and\n    it immediately exchanges the code for a token.\n\n> -   Responds to `GET` requests to the URL defined in the\n>     `callback_url` parameter of your client.\n> -   Retrieves the `code` query parameter from the request.\n> -   Makes a `POST` request to the OAuth2 server\\'s tokens endpoint to\n>     generate a token. In the default Tapis authenticator, the tokens\n>     URL path is `/v3/oauth2/tokens`; for example,\n>     `https://tacc.tapis.io/v3/oauth2/tokens` in the `tacc` tenant. The\n>     POST body must include the following parameters:\n>     -   `code`: the code the controller just received in the request\n>         from the OAuth2 server.\n>     -   `redirect_uri`: should be the same as the `callback_url`\n>         parameter of your client.\n>     -   `grant_type`: should always have the value\n>         `authorization_code`.\n\nNote that many popular web frameworks support OAuth2 flows with minimal\ncustom coding required.\n\nThe final step to using the authorization code grant type is to register\na client (see above) with a `callback_url` parameter equal to the URL\nwithin your web application where it will handle converting\nauthorization codes into access tokens (i.e., controller 2 above)."
{"filename": "authentication.md", "prompt": "The Tapis Token Web Application", "completion": "Tapis provides a graphical interface via a web application that enables\nusers to generate tokens. The Tapis Web Application is available by\ndefault for any tenant using the default Tapis authenticator, including\nthe `tacc` tenant. The Tapis Token Web Application serves as an example\nof an application using the authorization code grant type.\n\nThe Tapis Token Web Application and its source code are available at the\nfollowing URLs:\n\n-   Token App (`tacc` tenant): <https://tacc.tapis.io/v3/oauth2/webapp>\n-   Token App source code:\n    <https://github.com/tapis-project/authenticator>"}
{"filename": "files.md", "prompt": "Overview", "completion": "Through the Files service users can perform file listing, uploading, and\nvarious operations such as move, copy, mkdir and delete. The service\nalso supports transferring files from one Tapis system to another.\n\nCurrently, the files service includes support for systems of type LINUX,\nS3, IRODS and GLOBUS.\n\nNote that supported functionality varies by system type.\n\nAll file operations act upon Tapis *System* resources.[^1] For more\ninformation on the Systems service please see\n[Systems](https://tapis.readthedocs.io/en/latest/technical/systems.html)"}
{"filename": "files.md", "prompt": "Basic File Operations", "completion": ""}
{"filename": "files.md", "prompt": "Listing", "completion": "Tapis supports listing files or objects on a Tapis system. The type for\nitems listed will depend on system type. For example, for LINUX they\nwill be posix files and for S3 they will be storage objects. See the\nnext section below for additional considerations for S3 type systems. On\nS3 systems, for example, the recurse flag is ignored and all objects\nwith keys matching the path as a prefix are always included.\n\nFor system types that support directory hierarchies the maximum\nrecursion depth is 20.\n\nTo list the files in the effective *rootDir* directory of a Tapis\nsystem:\n\nUsing the official Tapis Python SDK:\n\n\n\nOr using curl:\n\n\n\nAnd to list a sub-directory in the system, just add the path to the\nrequest:\n\nUsing CURL\n\n\n\nQuery Parameters\n\nlimit\n\n:   integer - Max number of results to return, default of 1000\n\noffset\n\n:   integer - Skip the first N listings\n\nThe JSON response of the API will look something like this:"}
{"filename": "files.md", "prompt": "Listings and S3 Support", "completion": "File listings on S3 type systems have some special considerations.\nObjects in an S3 bucket do not have a hierarchical structure. There are\nno directories. Everything is an object associated with a key.\n\nOne thing to note is that, as mentioned above, for S3 the recurse flag\nis ignored and all objects with keys matching the path as a prefix are\nalways included.\n\nNote that for S3 this means that when the path is an empty string all\nobjects in the bucket with a prefix matching *rootDir* will be included.\nThis is especially important to keep in mind when using the delete\noperation to remove objects matching a path.\n\nThe attribute *rootDir* is optional for S3 type systems. When defined it\nwill be prepended to all paths and the resulting path will become the\nkey.\n\n::: note\n::: title\nNote\n:::\n\nWhen *rootDir* is defined for an S3 system it typically should not begin\nwith `/`. For S3 keys are typically created and manipulated using URLs\nand do not have a leading `/`.\n:::"}
{"filename": "files.md", "prompt": "Handling of symbolic links on Linux systems", "completion": "If listing contains a symbolic link, it will show type of symbolic link:\n\n\n\nIf a listing for a path that is a symbolic link is requested, the\nsymbolic link is followed, and the information is returned for path that\nthe symbolic link points to. If the path doesn\\'t exist, and error will\nbe returned. If the link points to a file, the file\\'s information will\nbe returned (the type will be \\\"file\\\"). If the symbolic link points to\na directory, the contents of that directory will be returned."}
{"filename": "files.md", "prompt": "Move and Copy", "completion": "To move or copy a file or directory using the files service, make a PUT\nrequest using the path to the current location of the file or folder.\n\nFor example, to copy a file located at [/file1.txt]{.title-ref}
 to\n[/subdir/file1.txt]{.title-ref
\n\n\n\nwith a JSON body of"
{"filename": "files.md", "prompt": "Handling of symbolic links on Linux systems", "completion": "During a move or copy, if a symbolic link is encountered, it will be\nhandled as shown in the tables below. The first and second columns\nindicate whether the link is the source or target and if it points to a\nfile or directory.\n\nCopy\n\n+------+----+---------------------------------------------------------+\n| Symb | Po | Notes                                                   |\n| olic | in |                                                         |\n| Link | ts |                                                         |\n|      | To |                                                         |\n+======+====+=========================================================+\n| so   | fi | If the destination path is to a file (this means that   |\n| urce | le | the path ends in a component that does exist, and it\\'s |\n|      |    | a file or a component that does not exist):             |\n|      |    |                                                         |\n|      |    | > -   a new symbolic link is created that points to the |\n|      |    | >     same place as the source.                         |\n|      |    | > -   if the destination path includes directories that |\n|      |    | >     do not exist, they will be created.               |\n|      |    |                                                         |\n|      |    | If the destination path is to a directory (this means   |\n|      |    | that the path ends in a component that does exist and   |\n|      |    | it is a directory):                                     |\n|      |    |                                                         |\n|      |    | > -   If the path given is to an existing directory, a  |\n|      |    | >     new link with the same name will be created in    |\n|      |    | >     that directory, and it will point to the same     |\n|      |    | >     place as the source.                              |\n|      |    |                                                         |\n|      |    | Note that if the link is to a relative path, moving it  |\n|      |    | could change where it actually points because the exact |\n|      |    | relative path will remain the same.                     |\n+------+----+---------------------------------------------------------+\n| so   | d  | If the destination path is to a file (this means that   |\n| urce | ir | the path ends in a component that does exist, and it\\'s |\n|      | ec | a file or a component that does not exist):             |\n|      | to |                                                         |\n|      | ry | > -   a new symbolic link is created that points to the |\n|      |    | >     same place as the source.                         |\n|      |    | > -   if the destination path includes directories that |\n|      |    | >     do not exist, they will be created.               |\n|      |    |                                                         |\n|      |    | If the destination path is to a directory (this means   |\n|      |    | that the path ends in a component that does exist and   |\n|      |    | it is a directory):                                     |\n|      |    |                                                         |\n|      |    | > -   If the path given is to an existing directory, a  |\n|      |    | >     new link with the same name will be created in    |\n|      |    | >     that directory, and it will point to the same     |\n|      |    | >     place as the source.                              |\n|      |    |                                                         |\n|      |    | Note that if the link is to a relative path, moving it  |\n|      |    | could change where it actually points because the exact |\n|      |    | relative path will remain the same.                     |\n+------+----+---------------------------------------------------------+\n| des  | fi | The destination is replaced by the source. The source   |\n| tina | le | could be a file, directory, or link to a file or        |\n| tion |    | directory.                                              |\n+------+----+---------------------------------------------------------+\n| des  | d  | The new file, directory, or link is created inside of   |\n| tina | ir | the existing directory.                                 |\n| tion | ec |                                                         |\n|      | to |                                                         |\n|      | ry |                                                         |\n+------+----+---------------------------------------------------------+\n\nMove\n\n+------+----+---------------------------------------------------------+\n| Symb | Po | Notes                                                   |\n| olic | in |                                                         |\n| Link | ts |                                                         |\n|      | To |                                                         |\n+======+====+=========================================================+\n| so   | fi | If the destination path is to a file (this means that   |\n| urce | le | the path ends in a component that does exist, and it\\'s |\n|      |    | a file or a component that does not exist):             |\n|      |    |                                                         |\n|      |    | > -   the symbolic link is renamed.                     |\n|      |    | > -   if the destination path includes directories that |\n|      |    | >     do not exist, they will be created, and the new   |\n|      |    | >     link will be placed there.                        |\n|      |    |                                                         |\n|      |    | If the destination path is to a directory (this means   |\n|      |    | that the path ends in a component that does exist and   |\n|      |    | it is a directory):                                     |\n|      |    |                                                         |\n|      |    | > -   If the path given is to an existing directory,    |\n|      |    | >     the link is moved to that directory.              |\n|      |    |                                                         |\n|      |    | Note that if the link is to a relative path, moving it  |\n|      |    | could change where it actually points because the exact |\n|      |    | relative path will remain the same.                     |\n+------+----+---------------------------------------------------------+\n| so   | d  |                                                         |\n| urce | ir |                                                         |\n|      | ec |                                                         |\n|      | to |                                                         |\n|      | ry |                                                         |\n+------+----+---------------------------------------------------------+\n|      |    | If the destination path is to a file (this means that   |\n|      |    | the path ends in a component that does exist, and it\\'s |\n|      |    | a file or a component that does not exist):             |\n|      |    |                                                         |\n|      |    | > -   the symbolic link is renamed.                     |\n|      |    | > -   if the destination path includes directories that |\n|      |    | >     do not exist, they will be created.               |\n|      |    |                                                         |\n|      |    | If the destination path is to a directory (this means   |\n|      |    | that the path ends in a component that does exist and   |\n|      |    | it is a directory):                                     |\n|      |    |                                                         |\n|      |    | > -   if the path given is to an existing directory,    |\n|      |    | >     the souce link will be moved into that directory, |\n|      |    | >     and it will point to the same place as the        |\n|      |    | >     source.                                           |\n|      |    |                                                         |\n|      |    | Note that if the link is to a relative path, moving it  |\n|      |    | could change where it actually points because the exact |\n|      |    | relative path will remain the same.                     |\n+------+----+---------------------------------------------------------+\n| des  | fi | The source link is renamed, to the destination path.    |\n| tina | le | The destination is replaced. The source could be a      |\n| tion |    | file, directory, or link to a file or directory.        |\n+------+----+---------------------------------------------------------+\n| des  | d  | > The file, directory, or link is moved inside of the   |\n| tina | ir | > existing directory.                                   |\n| tion | ec |                                                         |\n|      | to |                                                         |\n|      | ry |                                                         |\n+------+----+---------------------------------------------------------+"}
{"filename": "files.md", "prompt": "Making directories", "completion": "To create a directory on a tapis system at the given path, issue a mkdir\nrequest. This is not supported for all system types. The mkdir operation\nis currently supported for LINUX, IRODS and GLOBUS type systems.\n\nUsing the Tapis Python SDK:\n\n\n\nUsing CURL:"}
{"filename": "files.md", "prompt": "Uploading", "completion": "To upload a file use a POST request. The file will be placed at the\nlocation specified in the [{path}
]{.title-ref
 parameter in the request.\nNot all system types support this operation. For example, given the\nsystem [my-system]{.title-ref
, to upload file\n[someFile.txt]{.title-ref
 to directory \\`/folderA/folderB/folderC\\`:\n\nUsing the official Tapis Python SDK:\n\n\n\n\n\nFor some system types (such as LINUX) any folders that do not exist in\nthe specified path will automatically be created.\n\nNote that for an S3 system an object will be created with a key of\n*rootDir*/{path
."
{"filename": "files.md", "prompt": "Deleting", "completion": "To delete a file or folder, issue a DELETE request for the path to be\nremoved.\n\n\n\nThe request above would delete `file1.txt`\n\nFor an S3 system, the path will represent either a single object or all\nobjects in the bucket with a prefix matching the system *rootDir* if the\npath is the empty string.\n\n::: warning\n::: title\nWarning\n:::\n\nFor an S3 system if the path is the empty string, then all objects in\nthe bucket with a key matching the prefix *rootDir* will be deleted. So\nif the *rootDir* is also the empty string, then all objects in the\nbucket will be removed.\n:::"}
{"filename": "files.md", "prompt": "Handling of symbolic links and special files on Linux Systems", "completion": "If Tapis encounters a symbolic link during a delete operation, the link\nwill be deleted. The file or directory that the link points to will be\nunaffected. If the delete encounters a special file (such as a device\nfile or fifo, etc), it will not be deleted, and an error will be\nreturned. If this is in the middle of a recursive delete operation, some\nfiles may have been already deleted."}
{"filename": "files.md", "prompt": "Creating a directory", "completion": "To create a directory, use POST and provide the path to the new\ndirectory in the request body. Not all system types support this\noperation.\n\n\n\nwith a JSON body of"}
{"filename": "files.md", "prompt": "Getting Linux stat information", "completion": "Get native stat information for a file or directory for a system of type\nLINUX.\n\nFor example, for [/subdir/file1.txt]{.title-ref}
"
{"filename": "files.md", "prompt": "Running a Linux native operation", "completion": "Run a native operation on a path. Operations are *chmod*, *chown* or\n*chgrp*. For a system of type LINUX.\n\nFor example, to change the owner of a file located at\n[/file1.txt]{.title-ref}
 to `aeinstein`\n\n\n\nwith a JSON body of"
{"filename": "files.md", "prompt": "Content", "completion": "Get file or directory contents as a stream of data. Not supported for\nall system types."}
{"filename": "files.md", "prompt": "File Contents - Serving files", "completion": "To return the actual contents (raw bytes) of a file:\n\n\n\nQuery Parameters\n\nstartByte\n\n:   integer - Start at byte N of the file\n\ncount\n\n:   integer - Return this number of bytes after startByte\n\nzip\n\n:   boolean - Zip the contents of a folder\n\nHeader Parameters\n\nmore\n\n:   integer - Return 1 KB chunks of UTF-8 encoded text from a file\n    starting after page *more*. This call can be used to page through a\n    text based file. Note that if the contents of the file are not\n    textual (such as an image file or other binary format), the output\n    will be bizarre."}
{"filename": "files.md", "prompt": "Download using ZIP", "completion": "The query parameter *zip* may be used to request a stream compressed\nusing the ZIP file format. This is not allowed if system *rootDir* plus\n*path* would result in all files on the host being included. Please\ndownload individual directories, files or objects.\n\nFor example, on a linux system a directory may be downloaded as a\ncompressed archive using a command similar to the following:\n\n\n\nThe program *unzip* may then be used to extract the contents.\n\nIf the path being downloaded is a single file and the contents are\nplaced in a file ending in the extension *.gz* then the *gunzip* utility\nmay also be used to extract the contents."}
{"filename": "files.md", "prompt": "Transfers", "completion": "File transfers are used to move data between Tapis systems. They should\nbe used for bulk data operations that are too large for the REST api to\nperform. Transfers occur *asynchronously*, and are executed concurrently\nwhere possible to increase performance. As such, the order in which the\nfiles are transferred is not deterministic.\n\nWhen a transfer is initiated, a *bill of materials* is created that\ncreates a record of all the files from the *sourceUri* that are to be\ntransferred to the *destinationUri*. Unless otherwise specified, all\nfiles in the *bill of materials* must transfer successfully in order for\nthe overall transfer to be considered successful. A transfer task has an\nattribute named *status* which is updated as the transfer progresses.\nThe possible states for a transfer are:\n\nACCEPTED\n\n:   The initial request has been processed and saved.\n\nIN_PROGRESS\n\n:   The bill of materials has been created and transfers are either in\n    flight or waiting to begin.\n\nFAILED\n\n:   The transfer failed.\n\nCOMPLETED\n\n:   The transfer completed successfully, all files have been transferred\n    to the target system.\n\nUnauthenticated HTTP endpoints are also possible to use as a source for\ntransfers. This method can be utilized to include outputs from other\nAPIs into Tapis jobs.\n\n::: note\n::: title\nNote\n:::\n\nFor transfers involving Globus, both the source and destination system\nmust be of type GLOBUS.\n:::\n\nThe number of files included in the *bill of materials* will depend on\nthe system types and the *sourceUri* values provided in the transfer\nrequest. If the source system supports directories and *sourceUri* is a\ndirectory then the directory will be processed recursively and all files\nwill be added to the *bill of materials*. If the source system is of\ntype S3 then all objects matching the *sourceUri* path as a prefix will\nbe included."}
{"filename": "files.md", "prompt": "System types and supported functionality", "completion": "As discussed above, the files included in a transfer will depend on the\nsource system types and the *sourceUri* values provided in the transfer\nrequest. Here is a summary of the behavior:\n\n*LINUX/IRODS to LINUX/IRODS*\n\n:   When the *sourceUri* is a directory a recursive listing is made and\n    the files and directory structure are replicated on the\n    *destinationUri* system.\n\n*S3 to LINUX/IRODS*\n\n:   All objects matching the *sourceUri* path as a prefix will be\n    created as files on the *destinationUri* system.\n\n*LINUX/IRODS to S3*\n\n:   When the *sourceUri* is a directory a recursive listing is made. For\n    each entry in the listing the path relative to the source system\n    rootDir is mapped to a key for the S3 destination system. In other\n    words, a recursive listing is made for the directory on the\n    *sourceUri* system and for each non-directory entry an object is\n    created on the S3 *destinationUri* system.\n\n*S3 to S3*\n\n:   All objects matching the *sourceUri* path as a prefix will be\n    re-created as objects on the *destinationUri* system.\n\n*HTTP/S to ANY*\n\n:   Transfer of a directory is not supported. Destination system may not\n    be of type GLOBUS. The content of the object from the *sourceUri*\n    URL is used to create a single file or object on the\n    *destinationUri* system.\n\n*ANY to HTTP/S*\n\n:   Transfers not supported. Tapis does not support the use of protocol\n    http/s for the *destinationUri*."}
{"filename": "files.md", "prompt": "Creating Transfers", "completion": "Lets say our user `aturing` needs to transfer data between two systems\nthat are registered in tapis. The source system has an id of\n`aturing-storage` with the results of an experiment located in directory\n`/experiments/experiment-1/` that should be transferred to a system with\nid `aturing-compute`\n\n\n\n\n\nThe request above will initiate a transfer that copies all files and\nfolders in the `experiment-1` folder on the source system to the root\ndirectory of the destination system `aturing-compute`"}
{"filename": "files.md", "prompt": "HTTP Source", "completion": "Unauthenticated HTTP/S endpoints can also be used as a source for a file\ntransfer request. This can be useful, for instance, when the inputs for\na job are from a separate web service, or perhaps stored in a public S3\nbucket. Note that in this case the *sourceUri* does not refer to a Tapis\nsystem.\n\n\n\n\n\nThe request above will place the output of the source URI into a file\ncalled `inputs.csv` in the `aturing-compute` system."}
{"filename": "files.md", "prompt": "Getting transfer information", "completion": "To retrieve information about a transfer including status and bytes\ntransferred, simply make a GET request to the transfers API with the\nUUID of the transfer.\n\n\n\nThe JSON response should look something like :"}
{"filename": "files.md", "prompt": "Handling of symbolic links on Linux Systems", "completion": "Transfer will always \\\"follow links\\\". If the source of the transfer is\na symbolic link to a file or directory, it transfer the file or\ndirectory that is pointed to. If it doesn\\'t exist, it will be an error\n(i.e. the link points to something that doesn\\'t exist). In the case of\na directory transfer where one of the entries in the directory\nencountered is a symbolic link it will be resolved in exactly the same\nway - the file is added to the archive to be downloaded, or the\ndirectory is walked, and it\\'s content added to the archive. Symbolic\nlinks can create situations where infinite recursion can occur - for\nexample, suppose you have a directory with a link that points to\n\\\"../\\\". That means that each time it\\'s expanded the current directory\nwill be added, and the link will be expanded again. Transfers (and\nreally all file operations that involve recursing subdirectories) are\nlimited by a recursion depth. The current maximum depth is 20."}
{"filename": "files.md", "prompt": "Support for Globus", "completion": "Please note that your Tapis site installation must have been configured\nby the site administrator to support Globus. Please see\n[Globus_Config](https://tapis.readthedocs.io/en/latest/deployment/deployer.html#configuring-support-for-globus).\n\nThe integration of Globus and Tapis allows users to configure and use\nGlobus endpoints and collections just as they would other types of\nstorage systems defined in Tapis. As mentioned previously, not all\noperations are supported for all system types. For systems of type\nGLOBUS, the following operations are supported:\n\n-   listing\n-   mkdir\n-   move\n-   delete\n-   transfer between GLOBUS systems\n\nFor more information on setting up and registering credentials for a\nsystem of type GLOBUS, please see\n[Systems_Globus](https://tapis.readthedocs.io/en/latest/technical/systems.html#registering-credentials-for-a-globus-system#)."}
{"filename": "files.md", "prompt": "File Permissions", "completion": "The permissions model allows for fine grained access control of paths on\na Tapis system. The system owner may grant READ and MODIFY permission to\nspecific users. MODIFY implies READ.\n\nPlease note that Tapis permissions are independent of native permissions\nenforced by the underlying system host."}
{"filename": "files.md", "prompt": "Getting permissions", "completion": "Get the Tapis permissions for a user for the system and path. If no user\nspecified then permissions are retrieved for the user making the\nrequest."}
{"filename": "files.md", "prompt": "Granting permissions", "completion": "Lets say our user `aturing` has a system with ID `aturing-storage`. Alan\nwishes to allow his collaborator `aeinstein` to view the results of an\nexperiment located at `/experiment1`\n\n\n\nwith a JSON body with the following shape:\n\n\n\nOther users can also be granted permission to write to the system by\ngranting the `MODIFY` permission. The JSON body would then be:"}
{"filename": "files.md", "prompt": "Revoking permissions", "completion": "Our user `aturing` now wishes to revoke his former collaborators access\nto the folder above. He can issue a DELETE request on the path and\nspecify the username in order to revoke access:"}
{"filename": "files.md", "prompt": "File Path Sharing", "completion": "In addition to fine grained permissions support, Tapis also supports a\nhigher level approach to granting access. This approach is known simply\nas *sharing*. The sharing API allows you to share a path with a set of\nusers as well as share publicly with all users in a tenant. Sharing a\npath grants users READ access to the path or, in the context of running\na job, it grants users READ and MODIFY access to the path.\n\nPlease note that the underlying host associated with a system typically\nalso has it\\'s own access controls."}
{"filename": "files.md", "prompt": "Getting share information", "completion": "Retrieve all sharing information for a path on a system. This includes\nall users with whom the path has been shared and whether or not the path\nhas been made publicly available."}
{"filename": "files.md", "prompt": "Sharing a path with users", "completion": "Create or update sharing information for a path on a system. The path\nwill be shared with the list of users provided in the request body.\nRequester must be owner of the system. For LINUX systems path sharing is\nhierarchical. Sharing a path grants users READ access to the path or, in\nthe context of running a job, it grants users READ and MODIFY access to\nthe path.\n\n\n\nwith a JSON body with the following shape:"}
{"filename": "files.md", "prompt": "Sharing a path publicly", "completion": "Share a path on a system with all users in the tenant. Requester must be\nowner of the system. Sharing a path grants users READ access to the path\nor, in the context of running a job, it grants users READ and MODIFY\naccess to the path."}
{"filename": "files.md", "prompt": "Unsharing a path with users", "completion": "Update sharing information for a path on a system. The path will be\nunshared with the list of users provided in the request body. Requester\nmust be owner of the system.\n\n\n\nwith a JSON body with the following shape:"}
{"filename": "files.md", "prompt": "Unsharing a path publicly", "completion": "Remove public sharing for a path on a system. Requester must be owner of\nthe system."}
{"filename": "files.md", "prompt": "Removing all shares for a path", "completion": "Remove all shares for a path on a system including public access. If the\npath is a directory this will also be done for all sub-paths."}
{"filename": "files.md", "prompt": "PostIts", "completion": "The PostIts service is a URL service that allows you to create\npre-authenticated, disposable URLs to files, directories, buckets, etc\nin the Tapis Platform\\'s Files service. You have control over the\nlifetime and number of times the URL can be redeemed, and you can expire\na PostIt at any time. The most common use of PostIts is to create URLs\nto files so that you can share with others without having to upload them\nto a third-party service."}
{"filename": "files.md", "prompt": "Creating PostIts", "completion": "To create a PostIt, send a POST request to the Files service\\'s Create\nPostIt endpoint. The url will contain the systemId and the path that\nwill be shared. The body of the post will contain a json document which\ndescribes how long the PostIt is valid, and how many times it can be\nredeemed. There are default values for each of these parameters if they\nare not included. If the number of times the PostIt may be redeemed\n(allowedUses) is set to -1, the PostIt may be redeemed an unlimited\nnumber of times. The expiration (validSeconds) must always contain a\nvalue. If one is not provided, there is a default that is used. The\nmaximum value of \\'validSeconds\\' is the maximum integer value in Java\n(Integer.MAX_VALUE = 2147483647).\n\n::: note\n::: title\nNote\n:::\n\nThe maximum value would result in a PostIt that would be valid for\nnearly 70 years, however it is important to remember that the\nauthentication and authorization are built into the PostIt redeem url.\nThis means anyone who has the url could redeem it. For this reason,\nit\\'s advisable to keep the uses and expiration times to the minimum\nrequired.\n:::\n\nDefault parameters:\n\n-   allowedUses: 1 (One use)\n-   validSeconds: 2592000 (30 days)\n\nAPPLICATION/JSON examples\n\nCreating a postit with the default expiration and uses on a system\ncalled \\\"tapisv3-storage\\\" for the path \\\"/myDirectory/myFile.txt\\\"\n\nUsing curl\n\n\n\nUsing python\n\n\n\nCreating a postit supplying expiration (validSeconds 600) and uses\n(allowedUses 3) on a system called \\\"tapisv3-storage\\\" for the path\n\\\"/myDirectory/myFile.txt\\\"\n\nUsing curl\n\n\n\nUsing python\n\n\n\nCreating a postit supplying allowing unlimited uses (allowedUses -1) and\nthe default value for expiration (default value for validSeconds is 30\ndays) on a system called \\\"tapisv3-storage\\\" for the path\n\\\"/myDirectory/myFile.txt\\\"\n\nUsing curl\n\n\n\nUsing python\n\n\n\nExample Postit Creation Response\n\n\n\n::: note\n::: title\nNote\n:::\n\nThe PostIt returned by the create will contain the redeemUrl. This url\nmay be used to download the content pointed to by the PostIt. No\nAuthentication will be done during this call. The credentials used to\naccess this content will be the credentials of the PostIt owner. If the\nowner\\'s permissions change between creating the PostIt and redeeming\nthe PostIt so that the owner is no longer allowed to read the content,\nredeeming the PostIt will fail.\n:::"}
{"filename": "files.md", "prompt": "Create PostIt parameters", "completion": "---------------------------------------------------------------------------------------\n  Name           Type      Location   Default      Description\n  -------------- --------- ---------- ------------ --------------------------------------\n  systemId       String    url        \\<none\\>     The systemId of the system containing\n                                                   the path to create the PostIt for.\n\n  path           String    url        \\<none\\>     The path to create the PostIt for.\n\n  allowedUses    integer   body       1            The number of times a postit can be\n                                                   redeemed. Valid values are 1 -\n                                                   2147483647, or -1 for unlimited uses.\n\n  validSeconds   integer   body       2147483647   The number of seconds from creation\n                                                   that the PostIt will be redeemable. An\n                                                   expiration time is computed by adding\n                                                   this value to the current date and\n                                                   time.\n  ---------------------------------------------------------------------------------------"}
{"filename": "files.md", "prompt": "Listing PostIts", "completion": "PostIts can be listed by authenticated users. By default a listing of\nall PostIts owned by the authenticated user will be returned.\n\nUsing curl\n\n\n\nUsing python\n\n\n\nTo list all PostIts that are visible to the authenticated user, supply\nthe query parameter listType and set it\\'s value to ALL. Typically users\nwill only be able to see PostIts that they own, however tenant admins\nwill be allowed to see all PostIts in their tenant.\n\nUsing curl\n\n\n\nUsing python\n\n\n\nPaging is handled by the query parameters limit, skip, and startAfter.\nBy default 100 PostIts are returned, however this can be changed by\nsetting the query parameter \\\"limit\\\". Skip is used to determine how\nmany PostIts to skip. For example to get the second page of a list of\nPostIts containing 10 items per page, you would need to set the limit to\n10 (10 items per page), and set skip to 10 (skip the first page). It\\'s\nprobably a good practice to set orderBy also, so that the list is\nordered in the same way each time. You could for example set orderBy to\nid.\n\nUsing curl\n\n\n\nUsing python\n\n\n\nUsing startAfter is similar to using skip. When using startAfter, you\nmust provide a PostIt id, and the list will start immediately after that\nPostIt. You **must** provide the orderBy parameter. You may not use skip\nand startAfter together.\n\nTo control which fields are returned, you can supply the select query\nparameter, and select only certain fields. Setting the select query\nparameter to id, redeemUrl and expiration would return only those fields\n(select=id,redeemUrl,expiration). Setting \\\"select\\\" to allAttributes\nwill return all attributes, and setting \\\"select\\\" to summaryAttributes\nwill only return a preset collection of attributes. The default is\nsummaryAttributes. You can also set the value to summaryAttributes with\nadditional attributes (select=summaryAttributes,updated).\n\nUsing curl\n\n\n\nUsing python"}
{"filename": "files.md", "prompt": "Retrieving a Single PostIt", "completion": "Retrieving a single PostIt can be done by issuing a GET request\ncontaining the id of the PostIt. This is **not** the same as redeeming,\nand does not add to the redeem count. This will allow the owner of the\nPostIt or a tenant admin to view the PostIt. This could be used to see\nthe number of times it\\'s been retrieved, total number of uses allowed,\nexpiration date, etc.\n\nUsing curl\n\n\n\nUsing python\n\n\n\nFor tenant admins, any PostIts in the tenant can be retreived in this\nway. For other users only PostIts that are owned by that user may be\nretreived, since access to the redeem url allows redemption of the\nPostIt."}
{"filename": "files.md", "prompt": "Updating PostIts {#Updating PostIts}
", "completion": "The creator of a PostIt and tenant admins can update a PostIt. When\nupdating a PostIt, the id of the posted is sent as part of the url, and\na body containing allowedUses and/or validSeconds can be specified.\n\n::: note\n::: title\nNote\n:::\n\nThe validSeconds parameter will add to the date and time as of the\nupdate request to compute the new expiration. It does **not** extend the\ncurrent expiration by that many seconds.\n:::\n\nIf you need to update the url, you will need to delete or expire this\nPostIt and create a new one.\n\nUpdate a PostIt to allow for 10 uses and to expire in 1 hour\n\nUsing curl / PATCH\n\n\n\nUsing curl / POST\n\n\n\nUsing python"
{"filename": "files.md", "prompt": "Redeeming PostIts", "completion": "To redeem a PostIt, use the redeemUrl from the PostIt to make a\nnon-authenticated HTTP GET request.\n\nUsing curl\n\n\n\n::: note\n::: title\nNote\n:::\n\nThe options -J and -O (specified above as -JO) tell curl to download the\nfile content and use the filename in the content-disposition header.\nIt\\'s worth noting that using this filename is not entirely without risk\nas it could overwrite a file of the same name. If you would prefer to\nuse a name that you specify, you could replace -JO with \\--output\nfilename\n:::\n\nUsing python\n\n\n\nBy default if the PostIt you are redeeming points to a path that is a\ndirectory, you will get a zip file, and if it\\'s a regular file, you\nwill get an uncompressed file. If you want to force a file to be\ncompressed, you can specify the query parameter zip and set it to true.\n\nUsing curl\n\n\n\nUsing python\n\n\n\n::: note\n::: title\nNote\n:::\n\nIf you specify zip=false for a PostIt that points to a directory, you\nwill get an error. Directories can\\'t be returned unless they are\ncompressed.\n:::\n\nThe redeem URL can also be pasted into the address bar of your favorite\nbrowser, and it will download the file pointed to by the PostIt."}
{"filename": "files.md", "prompt": "Handling of symbolic links on Linux Systems", "completion": "Redeeming PostIts handles symbolic links exactly the same as\n[Transfers](#transfers)"}
{"filename": "files.md", "prompt": "Expiring PostIts", "completion": "There is no special endpoint for expiring a PostIt. To Expire a PostIt\njust update see `Updating PostIts`{.interpreted-text role=\"ref\"}
 and set\nvalidSeconds to 0, or allowedUses to 0."
{"filename": "files.md", "prompt": "Deleting PostIts", "completion": "PostIts can be deleted by specifying the PostIt id in the url. This is a\nhard delete, and cannot be undone. Only an owner or tenant admin can\ndelete a PostIt.\n\nUsing curl\n\n\n\nUsing python\n\n\n\n**Footnotes**\n\n[^1]: With the exception of the *sourceUri* in a transfer request when\n    the protocol is *http* or *https*."}
{"filename": "jobs.md", "prompt": "Introduction to Jobs", "completion": "The Tapis v3 Jobs service is specialized to run containerized\napplications on any host that supports container runtimes. Currently,\nDocker and Singularity containers are supported. The Jobs service uses\nthe Systems, Apps, Files and Security Kernel services to process jobs."}
{"filename": "jobs.md", "prompt": "Implementation Status", "completion": "The following table describes the current state of the Beta release of\nJobs. All UrlPaths shown start with /v3/jobs. The unauthenticated health\ncheck, ready and hello APIs do not require a Tapis JWT in the request\nheader.\n\n+------------------+--------+-----------------------------------------+-------------+\n| Name             | Method | UrlPath                                 | Status      |\n+==================+========+=========================================+=============+\n| Submit           | POST   | /submit                                 | Implemented |\n+------------------+--------+-----------------------------------------+-------------+\n| Resubmit         | POST   | /{JobUUID}
/resubmit                     | Implemented |\n+------------------+--------+-----------------------------------------+-------------+\n| List             | GET    | /list                                   | Implemented |\n+------------------+--------+-----------------------------------------+-------------+\n| Search           | GET    | /search                                 | Implemented |\n+------------------+--------+-----------------------------------------+-------------+\n| Search           | POST   | /search                                 | Implemented |\n+------------------+--------+-----------------------------------------+-------------+\n| Get              | GET    | /{JobUUID
                              | Implemented |\n+------------------+--------+-----------------------------------------+-------------+\n| Get Status       | GET    | /{JobUUID
/status                       | Implemented |\n+------------------+--------+-----------------------------------------+-------------+\n| Get History      | GET    | /{JobUUID
/history                      | Implemented |\n+------------------+--------+-----------------------------------------+-------------+\n| Get Output list  | GET    | /{JobUUID
/output/list/{outputPath
     | Implemented |\n+------------------+--------+-----------------------------------------+-------------+\n| Download Output  | GET    | /{JobUUID
/output/download/{outputPath
 | Implemented |\n+------------------+--------+-----------------------------------------+-------------+\n| Resubmit Request | GET    | /{JobUUID
/resubmit_request             | Implemented |\n+------------------+--------+-----------------------------------------+-------------+\n| Cancel           | POST   | > /{JobUUID
/cancel                     | Implemented |\n+------------------+--------+-----------------------------------------+-------------+\n| Hide             | POST   | > /{JobUUID
/hide                       | Implemented |\n+------------------+--------+-----------------------------------------+-------------+\n| Unhide           | POST   | > /{JobUUID
/unhide                     | Implemented |\n+------------------+--------+-----------------------------------------+-------------+\n| SendEvent        | POST   | > /{JobUUID
/sendEvent                  | Implemented |\n+------------------+--------+-----------------------------------------+-------------+\n| Post Share       | POST   | > /{JobUUID
/share                      | Implemented |\n+------------------+--------+-----------------------------------------+-------------+\n| Get Share        | GET    | > /{JobUUID
/share                      | Implemented |\n+------------------+--------+-----------------------------------------+-------------+\n| Delete Share     | DELETE | > /{JobUUID
/share/{user
               | Implemented |\n+------------------+--------+-----------------------------------------+-------------+\n| Health Check     | GET    | > /healthcheck                          | Implemented |\n+------------------+--------+-----------------------------------------+-------------+\n| Ready            | GET    | > /ready                                | Implemented |\n+------------------+--------+-----------------------------------------+-------------+\n| Hello            | GET    | > /hello                                | Implemented |\n+------------------+--------+-----------------------------------------+-------------+"
{"filename": "jobs.md", "prompt": "Job Processing Overview", "completion": "Before discussing the details of how to construct a job request, we take\nthis opportunity to describe overall lifecycle of a job. When a job\nrequest is recieved as the payload of an POST call, the following steps\nare taken:\n\n1.  **Request authorization** - The tenant, owner, and user values from\n    the request and Tapis JWT are used to authorize access to the\n    application, execution system and, if specified, archive system.\n2.  **Request validation** - Request values are checked for missing,\n    conflicting or improper values; all paths are assigned; required\n    paths are created on the execution system; and macro substitution is\n    performed to finalize all job parameters.\n3.  **Job creation** - A Tapis job object is written to the database.\n4.  **Job queuing** - The Tapis job is queue on an internal queue\n    serviced by one or more Job Worker processes.\n5.  **Response** - The initial Job object is sent back to the caller in\n    the response. This ends the synchronous portion of job submission.\n\nOnce a response to the submission request is sent to the caller, job\nprocessing proceeds asynchronously. Job worker processes read jobs from\ntheir work queues. The number of workers and queues is limited only by\nhardware resource constraints. Each job is assigned a worker thread.\nThis thread shepards a job through its lifecycle until the job\ncompletes, fails or becomes blocked due to a transient resource\nconstraint. The job lifecycle is reflected in the [Job\nStatus](#job-status) and generally progresses as follows:\n\n    a) Stage inputs to execution system\n    b) Stage application artifacts to execution system\n    c) Queue or invoke job on execution system\n    d) Monitor job until it terminates\n    e) Collect job exit code\n    f) Archive job output"}
{"filename": "jobs.md", "prompt": "Simple Job Submission Example", "completion": "The POST payload for the simplest job submission request looks like\nthis:\n\n    {\n     \"name\": \"myJob\"\n     \"appId\": \"myApp\"\n     \"appVersion\": \"1.0\"\n    }
\n\nIn this example, all input and output directories are either specified\nin the *myApp* definition or are assigned their default values.\nCurrently, the execution system on which an application runs must be\nspecified in either the application definition or job request. Our\nexample assumes that *myApp* assigns the execution system. Future\nversions of the Jobs service will support dynamic execution system\nselection.\n\nAn archive system can also be specified in the application or job\nrequest; the default is for it to be the same as the execution system.\n\n------------------------------------------------------------------------"
{"filename": "jobs.md", "prompt": "The Job Submission Request", "completion": "A job submission request must contain the name, appId and appVersion\nvalues as shown in the [Simple Job Submission\nExample](#simple-job-submission-example). Those values are marked\n*Required* in the list below, a list of all possible values allowed in a\nsubmission request. If a parameter has a default value, that value is\nalso shown.\n\nIn addition, some parameters can inherit their values from the\napplication or system definitions as discussed in [Parameter\nPrecedence](#parameter-precedence). These parameters are marked\n*Inherit*. Parameters that merge inherited values (rather than override\nthem) are marked *InheritMerge*.\n\nParameters that do not need to be set are marked *Not Required*.\nFinally, parameters that allow macro substitution are marked\n*MacroEnabled* (see [Macro Substitution](#macro-substitution) for\ndetails).\n\n**name**\n\n:   The user chosen name of the job. *MacroEnabled*, *Required.*\n\n**appId**\n\n:   The Tapis application to execute. *Required.*\n\n**appVersion**\n\n:   The version of the application to execute. *Required.*\n\n**jobType**\n\n:   A job\\'s type can be either FORK or BATCH.\n\n**owner**\n\n:   User ID under which the job runs. Administrators can designate a\n    user other than themselves.\n\n**tenant**\n\n:   Tenant of job owner. Default is job owner\\'s tenant.\n\n**description**\n\n:   Human readable job description. *MacroEnabled*, *Not Required*\n\n**archiveOnAppError**\n\n:   Whether archiving should proceed even when the application reports\n    an error. Default is *true*.\n\n**dynamicExecSystem**\n\n:   Whether the best fit execution system should be chosen using\n    *execSystemConstraints*. Default is *false*.\n\n**execSystemId**\n\n:   Tapis execution system ID. *Inherit*.\n\n**execSystemExecDir**\n\n:   Directory into which application assets are staged. *Inherit*, see\n    [Directories](#directories) for default.\n\n**execSystemInputDir**\n\n:   Directory into which input files are staged. *Inherit*, see\n    [Directories](#directories) for default.\n\n**execSystemOutputDir**\n\n:   Directory into which the application writes its output. *Inherit*,\n    see [Directories](#directories) for default.\n\n**execSystemLogicalQueue**\n\n:   Tapis-defined queue that corresponds to a batch queue on the\n    execution system. *Inherit* when applicable.\n\n**archiveSystemId**\n\n:   Tapis archive system ID. *Inherit*, defaults to *execSystemId*.\n\n**archiveSystemDir**\n\n:   Directory into which output files are archived after application\n    execution. *Inherit*, see [Directories](#directories) for default.\n\n**nodeCount**\n\n:   Number of nodes required for application execution. *Inherit*,\n    default is 1.\n\n**coresPerNode**\n\n:   Number of cores to use on each node. *Inherit*, default is 1.\n\n**memoryMB**\n\n:   Megabytes of memory to use on each node. *Inherit*, default is 100.\n\n**maxMinutes**\n\n:   Maximum number of minutes allowed for job execution. *Inherit*,\n    default is 10.\n\n**fileInputs**\n\n:   Input files that need to be staged for the application.\n    *InheritMerge*.\n\n**fileInputArrays**\n\n:   Arrays of input files that need to be staged for the application.\n    *InheritMerge*.\n\n**parameterSet**\n\n:   Runtime parameters organized by category. *Inherit*.\n\n**execSystemConstraints**\n\n:   Constraints applied against execution system capabilities to\n    validate application/system compatibility. *InheritMerge*.\n\n**subscriptions**\n\n:   Subscribe to the job\\'s events. *InheritMerge*.\n\n**tags**\n\n:   An array of user-chosen strings that are associated with a job.\n    *InheritMerge*.\n\n**notes**\n\n:   A JSON object containing any user-chosen data. *Inherit*.\n\n**isMpi**\n\n:   Indicates whether this job is an MPI job. *Inherit*, default is\n    false.\n\n**mpiCmd**\n\n:   Specify the MPI launch command. Conflicts with cmdPrefix if isMpi is\n    set. *Inherit*.\n\n**cmdPrefix**\n\n:   String prepended to the application invocation command. Conflicts\n    with mpiCmd if isMpi is set. *Inherit*.\n\n**notes**\n\n:   Optional JSON object containing arbitrary user data, maximum length\n    65536 bytes. *Inherit*.\n\nThe following subsections discuss the meaning and usage of each of the\nparameters available in a job request. The [schema]() and its referenced\n[library]() comprise the actual JSON schema definition for job requests."}
{"filename": "jobs.md", "prompt": "Parameter Precedence", "completion": "The runtime environment of a Tapis job is determined by values in system\ndefinitions, the app definition and the job request, in low to high\nprecedence order as listed. Generally speaking, for values that can be\nassigned in multiple definitions, the values in job requests override\nthose in app definitions, which override those in system definitions.\nThere are special cases, however, where the values from different\ndefinitions are merged.\n\nSee the jobs/apps/systems parameter\n[matrix](https://drive.google.com/file/d/1BrY6tHzOegwsgDMrhcKE7RHH7HRAA0Do/view?usp=sharing)\nfor a detailed description of how each parameter is handled."}
{"filename": "jobs.md", "prompt": "Job Type", "completion": "An execution system can run jobs using a batch scheduler (e.g., Slurm or\nCondor) or a native runtime (e.g., Docker or Singularity) or both. Users\nspecify how to run a job using the *jobType* parameter, which is set to\n\\\"BATCH\\\" to use a batch scheduler or \\\"FORK\\\" to use a native runtime.\nThe jobType can also be specified in application definitions. The final\nvalue assigned to the jobType of a job is calculated as follows:\n\n    1. If the user specifies jobType in the job request, use it.\n    2. Otherwise, if the app.jobType != null, use it.\n    3. Otherwise, query the execution system and set jobType=BATCH if execSys.canRunBatch==true.\n    4. Otherwise, set jobType=FORK."}
{"filename": "jobs.md", "prompt": "Directories", "completion": "The execution and archive system directories are calculated before the\nsubmission response is sent. This calculation can include the use of\nmacro definitions that get replaced by values at submission request\ntime. The [Macro Substitution](#macro-substitution) section discusses\nwhat macro definitions are available and how substitution works. In this\nsection, we document the default directory assignments which may include\nmacro definitions."}
{"filename": "jobs.md", "prompt": "Directory Definitions {#dir-definitions}
", "completion": "The directories assigned when a system is defined:\n\n    rootDir - the root of the file system that is accessible through this Tapis system.\n    jobWorkingDir - the default directory for temporary files used or created during job execution.\n    dtnMountPoint - the path relative to the execution system's rootDir where the DTN file system is mounted.\n\nAn execution system may define a *Data Transfer Node* (DTN). A DTN is a\nhigh throughput node used to stage job inputs and to archive job\noutputs. The goal is to improve transfer performance. The execution\nsystem mounts the DTN\\'s file system at the *dtnMountPoint* so that\nexecuting jobs have access to its data, but Tapis will connect to the\nDTN rather than the execution system during transfers. See [Data\nTransfer Nodes](#data-transfer-nodes) for details.\n\nThe directories assigned in application definitions and/or in a job\nsubmission requests:\n\n    execSystemExecDir\n    execSystemInputDir\n    execSystemOutputDir\n    archiveSystemDir"
{"filename": "jobs.md", "prompt": "Directory Assignments", "completion": "The rootDir and jobWorkingDir are always assigned upon system creation,\nso they are available for use as macros when assigning directories in\napplications or job submission requests.\n\nWhen a job request is submitted, each of the job\\'s four execution and\narchive system directories are assigned as follows:\n\n1.  If the job submission request assigns the directory, that value is\n    used. Otherwise,\n2.  If the application definition assigns the directory, that value is\n    used. Otherwise,\n3.  The default values shown below are assigned:\n\n\n    No DTN defined:\n      execSystemExecDir:    ${JobWorkingDir}
/jobs/${JobUUID
\n      execSystemInputDir:   ${JobWorkingDir
/jobs/${JobUUID
\n      execSystemOutputDir:  ${JobWorkingDir
/jobs/${JobUUID
/output\n      archiveSystemDir:     /jobs/${JobUUID
/archive                 (if archiveSystemId is set)\n    DTN defined:\n      execSystemExecDir:    ${DtnMountPoint
/jobs/${JobUUID
\n      execSystemInputDir:   ${DtnMountPoint
/jobs/${JobUUID
\n      execSystemOutputDir:  ${DtnMountPoint
/jobs/${JobUUID
/output\n      archiveSystemDir:     ${DtnMountPoint
/jobs/${JobUUID
/archive (if archiveSystemId is set)"
{"filename": "jobs.md", "prompt": "FileInputs", "completion": "The *fileInputs* in\n[Applications](https://tapis.readthedocs.io/en/latest/technical/apps.html)\ndefinitions are merged with those in job submission requests to produce\nthe complete list of inputs to be staged for a job. The following rules\ngovern how job inputs are calculated.\n\n> 1.  The effective inputs to a job are the combined inputs from the\n>     application and job request.\n> 2.  Only named inputs are allowed in application definitions.\n> 3.  Application defined inputs are either REQUIRED, OPTIONAL or FIXED.\n> 4.  Applications can restrict the number and definitions of inputs\n>     (*strictFileInputs=true*).\n> 5.  Anonymous (unnamed) inputs can be specified in the job request\n>     unless prohibited by the application definition\n>     (*strictFileInputs=true*).\n> 6.  Job request inputs override values set in the application except\n>     for FIXED inputs.\n> 7.  The *tapislocal* URL scheme specifies in-place inputs for which\n>     transfers are not performed.\n\nThe fileInputs array in job requests contains elements that conform to\nthe following JSON schema.\n\n    \"JobFileInput\": {\n        \"$comment\": \"Used to specify file inputs on Jobs submission requests\",\n        \"type\": \"object\",\n            \"properties\": {\n                \"name\": { \"type\": \"string\", \"minLength\": 1, \"maxLength\": 80 }
,\n                \"description\": { \"type\": \"string\", \"minLength\": 1, \"maxLength\": 8096 
,\n                \"autoMountLocal\": { \"type\": \"boolean\"
,\n                \"sourceUrl\":  {\"type\": \"string\", \"minLength\": 1, \"format\": \"uri\"
,\n                \"targetPath\": {\"type\": \"string\", \"minLength\": 0
,\n                \"notes\": {\"type\": \"string\", \"minLength\": 0
\n            
,\n        \"additionalProperties\": false\n    
\n\nJobFileInputs can be named or unnamed. When the *name* field is\nassigned, Jobs will look for an input with the same name in the\napplication definition (all application inputs are named). When a match\nis found, values from the AppFileInput are merged into unassigned fields\nin the JobFileInput.\n\nThe *name* must start with an alphabetic character or an underscore (\\_)\nfollowed by zero or more alphanumberic or underscore characters. If the\nname does not match one of the input names defined in the application,\nthen the application must have *strictFileInputs=false*. If the name\nmatches an input name defined in the application, then the\napplication\\'s inputMode must be REQUIRED or OPTIONAL. An error occurs\nif the inputMode is FIXED and there is a name match\\--job inputs cannot\noverride FIXED application inputs.\n\nThe optional *notes* field can contain any valid user-specified JSON\nobject.\n\nExcept for in-place inputs discussed below, the *sourceUrl* is the\nlocation from which data are copied to the *targetPath*. In Posix\nsystems the sourceUrl can reference a file or a directory. When a\ndirectory is specified, the complete directory subtree is copied.\n\nAny URL protocol accepted by the Tapis\n[Files](https://tapis.readthedocs.io/en/latest/technical/files.html)\nservice can be used in a *sourceUrl*. The most common protocols used are\ntapis, http, and https. The standard tapis URL format is\n*tapis://\\<tapis-system\\>/\\<path\\>*; please see the\n[Files](https://tapis.readthedocs.io/en/latest/technical/files.html)\nservice for the complete list of supported protocols.\n\nThe *targetPath* is the location to which data are copied from the\n*sourceUrl*. The target is rooted at the *execSystemInputDir* except,\npossibly, when HOST_EVAL() is used, in which case it is still relative\nto the execution system\\'s rootDir.\n\nA JobFileInput object is **complete** when its *sourceUrl* and\n*targetPath* are assigned; this provides the minimal information needed\nto effect a transfer. If only the *sourceUrl* is set, Jobs will use the\nsimple directory or file name from the URL to automatically assign the\n*targetPath*. Specifying a *targetPath* as \\\"*\\\" results in the same\nautomatic assignment. Whether assigned by the user or Jobs, all job\ninputs that are not in-place and do not use the HOST_EVAL() function are\ncopied into the*execSystemInputDir\\* subtree.\n\nAfter application inputs are added to or merged with job request inputs,\nall complete JobFileInput objects are designated for staging. Incomplete\nobjects are ignored only if they were specified as OPTIONAL in the\napplication definition. Otherwise, an incomplete input object causes the\njob request to be rejected."
{"filename": "jobs.md", "prompt": "In-Place Inputs (tapislocal)", "completion": "Job inputs already present on an execution system do not need to be\ntransferred, yet users may still want to declare them for documentation\npurposes or to control how they are mounted into containers. It\\'s\ncommon, for example, for large data sets that cannot reasonably be\ncopied to be mounted directly onto execution systems. The Jobs and\nApplications services provide custom syntax that allows such input to be\ndeclared, but instructs the Jobs service to **not** copy that input.\n\nTapis introduces a new URL scheme, *tapislocal*, that is only recognized\nby the Applications and Jobs services. Here are example URLs:\n\n    tapislocal://exec.tapis/home/bud/mymri.dcm\n    tapislocal://exec.tapis/corral/repl/shared\n\nLike the *tapis* scheme and all common schemes (https, sftp, etc.), the\nfirst segment following the double slashes designates a host. For\n*tapislocal*, the host is always the literal **exec.tapis**, which\nserves as a placeholder for a job\\'s execution system. The remainder of\nthe URL is the path on the Tapis system. All paths on Tapis systems,\nincluding those using the HOST_EVAL() function and the tapislocal URL,\nare rooted at the Tapis system\\'s rootDir.\n\nA *tapislocal* URL can only appear in the sourceUrl field of\nAppFileInput and JobFileInput parameters.\n\nThe *tapislocal* scheme indicates to Jobs that a filepath already exists\non the execution system and, therefore, does not require data transfer\nduring job execution. If targetPath is \\\"\\*\\\", the Jobs service will\nassign the target path inside the container to be the last segment of\nthe tapislocal URL path (/mymri.dcm and /shared in the examples above).\n\nIn container systems that require the explicit mounting of host\nfilepaths, such as Docker, the Jobs service can mount the filepath into\nthe container. Both application definitions and job requests support the\n*autoMountLocal* boolean parameter. This parameter is true by default,\nwhich causes Jobs to automatically mount the filepath into containers.\nSetting autoMountLocal to false allows the user complete control over\nmounting using a *containerArgs* parameter."}
{"filename": "jobs.md", "prompt": "FileInputArrays", "completion": "The *fileInputArrays* parameter provides an alternative syntax for\nspecifying inputs in\n[Applications](https://tapis.readthedocs.io/en/latest/technical/apps.html)\nand job requests. This syntax is convenient for specifying multiple\ninputs destined for the same target directory, an I/O pattern sometimes\nrefered to as *scatter-gather*. Generally, input arrays support the same\nsemantics as [FileInputs](#fileinputs) with some restrictions.\n\nThe fileInputArrays parameter in job requests contains elements that\nconform to the following JSON schema.\n\n    \"JobFileInputArray\": {\n         \"type\": \"object\",\n         \"additionalProperties\": false,\n         \"properties\": {\n             \"name\": { \"type\": \"string\", \"minLength\": 1, \"maxLength\": 80 }
,\n             \"description\": { \"type\": \"string\", \"minLength\": 1, \"maxLength\": 8096
,\n             \"sourceUrls\": { \"type\": [\"array\", \"null\"],\n                             \"items\": { \"type\": \"string\", \"format\": \"uri\", \"minLength\": 1 
,\n             \"targetDir\": { \"type\": \"string\", \"minLength\": 1 
,\n             \"notes\": {\"type\": \"string\", \"minLength\": 0
\n         
\n    
\n\nA fileInputArrays parameter is an array of JobFileInputArray objects,\neach of which contains an array of *sourceUrls* and a single\n*targetDir*. One restriction is that *tapislocal* URLs cannot appear in\n*sourceUrls* fields.\n\nAn application\\'s fileInputArrays are added to or merged with those in a\njob request following the same rules established for fileInputs in the\nprevious section. In particular, when names match, the *sourceUrls*\ndefined in a job request override (i.e., completely replace) those\ndefined in an application. After merging, each JobFileInputArray must\nhave a non-empty *sourceUrls* array. See [FileInputs](#fileinputs) and\n[Applications](https://tapis.readthedocs.io/en/latest/technical/apps.html)\nfor related information.\n\nEach *sourceUrls* entry is a location from which data is copied to the\n*targetDir*. In Posix systems each URL can reference a file or a\ndirectory. In the latter case, the complete directory subtree is\ntransferred. All URLs recognized by the Tapis\n[Files](https://tapis.readthedocs.io/en/latest/technical/files.html)\nservice can be used (*tapislocal* is not recognized by Files).\n\nThe *targetDir* is the directory into which all *sourceUrls* are copied.\nThe *targetDir* is always rooted at the *ExecSystemInputDir* and if\n*targetDir* is \\\"*\\\" or not specified, then it is\nassigned*ExecSystemInputDir\\*. The simple name of each *sourceUrls*\nentry is the destination name used in *targetDir*. Use different\nJobFileInputArrays with different targetDir\\'s if name conflicts between\n*sourceUrls* entries exist.\n\nThe optional *notes* field can contain any valid user-specified JSON\nobject."
{"filename": "jobs.md", "prompt": "ParameterSet", "completion": "The job *parameterSet* argument is comprised of these objects:\n\n  Name               JSON Schema Type                      Description\n  ------------------ ------------------------------------- -----------------------------------------------------------\n  appArgs            [JobArgSpec](#jobargspec) array       Arguments passed to user\\'s application\n  containerArgs      [JobArgSpec](#jobargspec) array       Arguments passed to container runtime\n  schedulerOptions   [JobArgSpec](#jobargspec) array       Arguments passed to HPC batch scheduler\n  envVariables       [KeyValuePair](#keyvaluepair) array   Environment variables injected into application container\n  archiveFilter      object                                File archiving selector\n  logConfig          [LogConfig](#logconfig)               User-specified stdout and stderr redirection\n\nEach of these objects can be specifed in Tapis application definitions\nand/or in job submission requests. In addition, the execution system can\nalso specify environment variable settings."}
{"filename": "jobs.md", "prompt": "appArgs", "completion": "Specify one or more command line arguments for the user application\nusing the *appArgs* parameter. Arguments specified in the application\ndefinition are appended to those in the submission request."}
{"filename": "jobs.md", "prompt": "containerArgs", "completion": "Specify one or more command line arguments for the container runtime\nusing the *containerArgs* parameter. Arguments specified in the\napplication definition are appended to those in the submission request."}
{"filename": "jobs.md", "prompt": "schedulerOptions", "completion": "Specify HPC batch scheduler arguments for the container runtime using\nthe *schedulerOptions* parameter. Arguments specified in the application\ndefinition are appended to those in the submission request. The\narguments for each scheduler are passed using that scheduler\\'s\nconventions.\n\nTapis defines a special scheduler option, **\\--tapis-profile**, to\nsupport local scheduler conventions. Data centers sometimes customize\ntheir schedulers or restrict how those schedulers can be used. The\n[Systems](https://tapis.readthedocs.io/en/latest/technical/systems.html)\nservice manages *SchedulerProfile* resources that are separate from any\nsystem definition, but can be referenced from system definitions. The\nJobs service uses directives contained in profiles to tailor application\nexecution to local requirements.\n\nAs an example, below is the JSON input used to create the TACC scheduler\nprofile. The *moduleLoads* array contains one or more objects. Each\nobject contains a *moduleLoadCommand*, which specifies the local command\nused to load each of the modules (in order) in its *modulesToLoad* list.\n*hiddenOptions* identifies scheduler options that the local\nimplementation prohibits. In this case, \\\"MEM\\\" indicates that the\n*\\--mem* option should never be passed to Slurm.\n\n    {\n        \"name\": \"TACC\",\n        \"owner\": \"user1\",\n        \"description\": \"Test profile for TACC Slurm\",\n        \"moduleLoads\": [\n            {\n                \"moduleLoadCommand\": \"module load\",\n                \"modulesToLoad\": [\"tacc-singularity\"]\n            }
\n        ],\n        \"hiddenOptions\": [\"MEM\"]\n    
\n\n**Scheduler-Specific Processing**\n\nJobs will perform [macro-substitution](#macro-substitution) on Slurm\nscheduler options *\\--job-name* or *-J*. This substitution allows Slurm\njob names to be dynamically generated before submitting them."
{"filename": "jobs.md", "prompt": "envVariables", "completion": "Specify key/value pairs that will be injected as environment variables\ninto the application\\'s container when it\\'s launched. Key/value pairs\nspecified in the execution system definition, application definition,\nand job submission request are aggregated using precedence ordering\n(system \\< app \\< request) to resolve conflicts."}
{"filename": "jobs.md", "prompt": "archiveFilter", "completion": "The *archiveFilter* conforms to this JSON schema:\n\n    \"archiveFilter\": {\n       \"type\": \"object\",\n       \"properties\": {\n          \"includes\": {\"type\": \"array\", \"items\": {\"type\": \"string\", \"minLength\": 1}
, \"uniqueItems\": true
,\n          \"excludes\": {\"type\": \"array\", \"items\": {\"type\": \"string\", \"minLength\": 1
, \"uniqueItems\": true
,\n          \"includeLaunchFiles\": {\"type\": \"boolean\"
\n       
,\n       \"additionalProperties\": false\n    
\n\nAn *archiveFilter* can be specified in the application definition and/or\nthe job submission request. The *includes* and *excludes* arrays are\nmerged by appending entries from the application definition to those in\nthe submission request.\n\nThe *excludes* filter is applied first, so it takes precedence over\n*includes*. If *excludes* is empty, then no output file or directory\nwill be explicitly excluded from archiving. If *includes* is empty, then\nall files in *execSystemOutputDir* will be archived unless explicitly\nexcluded. If *includes* is not empty, then only files and directories\nthat match an entry and not explicitly excluded will be archived.\n\nEach *includes* and *excludes* entry is a string, a string with\nwildcards or a regular expression. Entries represent directories or\nfiles. The wildcard semantics are that of glob (*), which is commonly\nused on the command line. Tapis implements Java glob\\_ semantics. To\nfilter using a regular expression, construct the pattern using Java\nregex\\_ semantics and then preface it withREGEX:*\\* (case sensitive).\nHere are examples of globs and regular expressions that could appear in\na filter:\n\n    \"myfile.*\"\n    \"*2021-*-events.log\"\n    \"REGEX:^[\\\\p{IsAlphabetic
\\\\p{IsDigit
_\\\\.\\\\-]+$\"\n    \"REGEX:\\\\s+\"\n\nWhen *includeLaunchFiles* is true (the default), then the script\n(*tapisjob.sh*) and environment (*tapisjob.env*) files that Tapis\ngenerates in the *execSystemExecDir* are also archived. These launch\nfiles provide valuable information about how a job was configured and\nlaunched, so archiving them can help with debugging and improve\nreproducibility. Since these files may contain application secrets, such\ndatabase passwords or other credentials, care must be taken to not\nexpose private data through archiving.\n\nIf no filtering is specified at all, then all files in\n*execSystemOutputDir* and the launch files are archived."
{"filename": "jobs.md", "prompt": "logConfig Spec", "completion": "A [LogConfig](#logconfig) can be supplied in the job submission request\nand/or in the application definition, with the former overriding the\nlatter when both are supplied. In supported runtimes (currently\nSingularity), the *logConfig* parameter can be used to redirect the\napplication container\\'s stdout and stderr to user-specified files."}
{"filename": "jobs.md", "prompt": "MPI and Related Support", "completion": "On many systems, running Message Passing Interface (MPI) jobs is simply\na matter of launching programs that have been configured or compiled\nwith the proper MPI libraries. Most of the work in employing MPI\ninvolves parallelizing program logic and specifying the correct\nlibraries for the target execution system. Once that\\'s done, a command\nsuch as *mpirun* (or on TACC systems, *ibrun*) is passed the program\\'s\npathname and arguments to kick off parallel execution.\n\nTapis\\'s *mpiCmd* parameter lets users set the MPI launch command in a\nsystem definition, application definition and/or job submission request\n(lowest to highest priority). For example, if *mpiCmd=mpirun*, then the\nstring \\\"mpirun \\\" will be prepended to the command normally used to\nexecute the application. Some MPI launchers have their own parameters,\nfor instance, *mpiCmd=ibrun -n 4* requests 4 MPI tasks.\n\nThe *isMpi* parameter is specified in an application definition and/or\njob request to toggle MPI launching on or off. This switch allows the\nsame system to run both MPI and non-MPI jobs depending on the needs of\nparticular jobs or applications. The *isMpi* default is false, so this\nswitch must be explicitly turned on to run an MPI job. When turned on,\n*isMpi* requires *cmdMpi* be assigned in the system, application and/or\njob request.\n\nThe *cmdPrefix* parameter provides generalized support for launchers and\nis available in application definitions and job submission requests.\nLike *mpiCmd*, a *cmdPrefix* value is simply prepended to a program\\'s\npathname and arguments. Being more general, *cmdPrefix* could specify an\nMPI launcher, but it\\'s not supported in system definitions and does not\nhave a toggle to control usage.\n\n*mpiCmd* and *cmdPrefix* are mutually exclusive; so if *isMpi* is true,\nthen *cmdPrefix* must not be set."}
{"filename": "jobs.md", "prompt": "ExecSystemConstraints", "completion": "Not implementated yet."}
{"filename": "jobs.md", "prompt": "Subscriptions", "completion": "Users can subscribe to job execution events. Subscriptions specified in\nthe application definition and those specified in the job request are\nmerged to produce a job\\'s initial subscription list. New subscriptions\ncan be added while a job is running, but not after the job has\nterminated. A job\\'s subscriptions can be listed and deleted. Only job\nowners or tenant administrators can subscribe to a job, see the\n[subscription](https://tapis-project.github.io/live-docs/?service=Jobs#tag/subscriptions)\nAPIs for details.\n\nWhen creating a subscription the *ttlminutes* parameter can specify up\nto 4 weeks. If the parameter is not specified or if it\\'s set to 0, a\ndefault value of 1 week is used.\n\nSubscribers are notified of job events by the\n[Notifications](https://tapis-project.github.io/live-docs/?service=Notifications)\nservice. Currently, only email and webhook delivery methods are\nsupported. The event types to which users can subscribe are:\n\n  Event Type                   Description\n  ---------------------------- ---------------------------------------------------\n  JOB_NEW_STATUS               When the job transitions to a new status\n  JOB_INPUT_TRANSACTION_ID     When an input file staging request is made\n  JOB_ARCHIVE_TRANSACTION_ID   When an archive file transfer request is made\n  JOB_SUBSCRIPTION             When a change to the job\\'s subscriptions is made\n  JOB_SHARE_EVENT              When a job resource has been shared or unshared\n  JOB_ERROR_MESSAGE            When the job experienced an error\n  JOB_USER_EVENT               When a user sends the job a custom event\n  ALL                          When any of the above occur\n\nAll event types other than JOB_USER_EVENT are generated by Tapis. See\n[Notification Messages](#notification-messages) for a description of\nwhat Jobs returns for each of the Tapis-generated event.\n\nA JOB_USER_EVENT contains a user-specified payload that can be sent to\nan active job using the job\\'s UUID. The payload must contain a JSON key\nnamed *eventData* and a string value of at least 1 character and no more\nthan 16,384 characters. The string can be unstructured or structured\n(such as a JSON object) as determined by the sender. The payload can\noptionally contain an *eventDetail* key with a string value of no more\nthan 64 characters. This key is used to further categorize events and,\nif not provided, will default to \\\"DEFAULT\\\". User events are always\nadded to the job history and notifications are sent to subscribers\ninterested in those events."}
{"filename": "jobs.md", "prompt": "Shared Components", "completion": ""}
{"filename": "jobs.md", "prompt": "JobArgSpec", "completion": "Simple argument strings can be specified in application definitions\n(AppArgSpec) and in job submission requests (JobArgSpec). These argument\nstrings are passed to specific components in the runtime system, such as\nthe batch scheduler ([schedulerOptions](#scheduleroptions)), the\ncontainer runtime ([containerArgs](#containerargs)) or the user\\'s\napplication ([appArgs](#appargs)).\n\nThe following rules govern how job arguments are calculated.\n\n> 1.  All argument in application definitions must be named.\n> 2.  Application arguments are either REQUIRED, FIXED or one of two\n>     optional types.\n> 3.  Anonymous (unnamed) argument can be specified in job requests.\n> 4.  Job request argument override values set in the application except\n>     for FIXED arguments.\n> 5.  The final argument ordering is the same as the order specified in\n>     the definitions, with application arguments preceding those from\n>     the job request. Application arguments maintain their place even\n>     when overridden in the job request.\n> 6.  The notes field can be any JSON object, i.e., JSON that begins\n>     with a brace (\\\"{\\\").\n\nWe define a **complete** AppArgSpec as one that has a non-empty name and\narg value. We define a **complete** JobArgSpec as one that has a\nnon-empty arg value. A JobArgSpec with the same name as an AppArgSpec\ninherits from the application and may override the AppArgSpec values.\n\nThis is the JSON schema used to define runtime arguments in\n[ParameterSet](#parameterset).\n\n    \"JobArgSpec\": {\n        \"$comment\": \"Used to specify parameters on Jobs submission requests\",\n        \"type\": \"object\",\n            \"properties\": {\n                \"name\": { \"type\": \"string\", \"minLength\": 1, \"maxLength\": 80 }
,\n                \"description\": { \"type\": \"string\", \"minLength\": 1, \"maxLength\": 8096 
,\n                \"include\": { \"type\": \"boolean\" 
,\n                \"arg\":  {\"type\": \"string\", \"minLength\": 1
,\n                \"notes\": {\"type\": object
\n            
,\n        \"required\": [\"arg\"],\n        \"additionalProperties\": false\n    
\n\nAs mentioned, the JobArgSpec is used in conjunction with the AppArgSpec\ndefined in\n[Applications](https://tapis.readthedocs.io/en/latest/technical/apps.html).\nArguments in application definitions are merged into job request\narguments using the same name matching alorithm as in\n[FileInputs](#fileinputs).\n\nThe *name* identifies the input argument. If present, the name must\nstart with an alphabetic character or an underscore (\\_) followed by\nzero or more alphanumeric or underscore characters.\n\nThe *description* is used to convey usage information to job requester.\nIf both application and request descriptions are provided, then the\nrequest description is appended as a separate paragraph to the\napplication description.\n\nThe required *arg* value is an arbitrary string and is used as-is. If\nthis argument\\'s name matches that of an application argument, this\n*arg* value overrides the application\\'s value except when\n*inputMode=FIXED* in the application.\n\nThe *include* field applies only on named arguments that are also\ndefined in the application definition with *inputMode* INCLUDE_ON_DEMAND\nor INCLUDE_BY_DEFAULT; this parameter is ignored on all other\ninputModes. Argument inclusion is discussed in greater detail in\nfollowing subsection."
{"filename": "jobs.md", "prompt": "Argument Processing", "completion": "[Applications](https://tapis.readthedocs.io/en/latest/technical/apps.html)\nuse their AppArgSpecs to pass default values to job requests. The\nAppArgSpec\\'s *inputMode* determines how to handle arguments during job\nprocessing. An *inputMode* field can have these values:\n\nREQUIRED\n\n:   The argument must be provided for the job to run. If an arg value is\n    not specified in the application definition, then it must be\n    specified in the job request. When provided in both, the job request\n    arg value overrides the one in application.\n\nFIXED\n\n:   The argument is completely defined in the application and not\n    overridable in a job request.\n\nINCLUDE_ON_DEMAND\n\n:   The argument, if complete, will only be included in the final\n    argument list constructed by Jobs if it\\'s explicitly referenced and\n    included in the Job request. This is the default value.\n\nINCLUDE_BY_DEFAULT\n\n:   The argument, if complete, will automatically be included in the\n    final argument list constructed by Jobs unless explicitly excluded\n    in the Job request.\n\nThe truth table below defines how the AppArgSpec\\'s *inputMode* and\nJobArgSpec\\'s *include* settings interact to determine whether an\nargument is accepted or ignored during job processing.\n\n  ------------------------------------------------\n  AppArgSpec           JobArgSpec    Meaning\n  *inputMode*          *include*     \n  -------------------- ------------- -------------\n  INCLUDE_ON_DEMAND    True          include arg\n\n  INCLUDE_ON_DEMAND    False         exclude arg\n\n  INCLUDE_ON_DEMAND    undefined     include arg\n\n  INCLUDE_BY_DEFAULT   True          include arg\n\n  INCLUDE_BY_DEFAULT   False         exclude arg\n\n  INCLUDE_BY_DEFAULT   undefined     include arg\n  ------------------------------------------------\n\nThe JobArgSpec *include* value has no effect on REQUIRED or FIXED\narguments. In the cases where the value does apply, not specifying\n*include* in a named JobArgSpec that matches an AppArgSpec is\neffectively the same as setting *include=True*. By setting\n*include=False*, a JobArgSpec can exclude any INCLUDE_ON_DEMAND or\nINCLUDE_BY_DEFAULT arguments."}
{"filename": "jobs.md", "prompt": "KeyValuePair", "completion": "The JSON schema for defining key/value pairs of strings in various\n[ParameterSet](#parameterset) components is below.\n\n    \"KeyValuePair\": {\n        \"$comment\": \"A simple key/value pair\",\n        \"type\": \"object\",\n            \"properties\": {\n                \"key\":   {\"type\": \"string\", \"minLength\": 1}
,\n                \"value\": {\"type\": \"string\", \"minLength\": 0
,\n                \"description\": {\"type\": \"string\", \"minLength\": 1, \"maxLength\": 2048
\n            
,\n         \"required\": [\"key\", \"value\"],\n         \"additionalProperties\": false\n    
\n\nBoth the *key* and *value* are required, though the *value* can be an\nempty string. Descriptions are optional."
{"filename": "jobs.md", "prompt": "LogConfig", "completion": "The JSON schema for used to redirect stdout and stderr to named file(s)\nin supported runtimes.\n\n    \"logConfig\": {\n        \"$comment\": \"Log file redirection and customization in supported runtimes\",\n        \"type\": \"object\",\n        \"required\": [ \"stdoutFilename\", \"stderrFilename\" ],\n        \"additionalProperties\": false,\n            \"properties\": {\n                \"stdoutFilename\": {\"type\": \"string\", \"minLength\": 1}
,\n                \"stderrFilename\": {\"type\": \"string\", \"minLength\": 1
\n            
\n    
\n\nCurrently, only the Singularity (Apptainer) runtime is supported.\n\nWhen specified, both file name fields must be explicitly assigned,\nthough they can be assigned to the same file. If a *logConfig* object is\nnot specified, or in runtimes where it\\'s not supported, then both\nstdout and stderr are directed to the default **tapisjob.out** file in\nthe job\\'s output directory. Output files, even when *logConfig* is\nused, are always relative to the ExecSystemOuputDir (see [Directory\nDefinitions](#directory-definitions)).\n\n------------------------------------------------------------------------"
{"filename": "jobs.md", "prompt": "Job Execution", "completion": ""}
{"filename": "jobs.md", "prompt": "Environment Variables", "completion": "The following standard environment variables are passed into each\napplication container run by Tapis as long as they have been assigned a\nvalue.\n\n    _tapisAppId - Tapis app ID\n    _tapisAppVersion - Tapis app version\n    _tapisArchiveOnAppError - true means archive even if the app returns a non-zero exit code\n    _tapisArchiveSystemDir - the archive system directory on which app output is archived\n    _tapisArchiveSystemId - Tapis system used for archiving app output\n    _tapisCoresPerNode - number of cores used per node by app\n    _tapisDtnMountPoint - the mountpoint on the execution system for the source DTN directory\n    _tapisDtnMountSourcePath - the directory exported by the DTN and mounted on the execution system\n    _tapisDtnSystemId - the Data Transfer Node system ID\n    _tapisDynamicExecSystem - true if dynamic system selection was used\n    _tapisEffeciveUserId - the user ID under which the app runs\n    _tapisExecSystemExecDir - the exec system directory where app artifacts are staged\n    _tapisExecSystemHPCQueue - the actual batch queue name on an HPC host\n    _tapisExecSystemId - the Tapis system where the app runs\n    _tapisExecSystemInputDir - the exec system directory where input files are staged\n    _tapisExecSystemLogicalQueue - the Tapis queue definition that specifies an HPC queue\n    _tapisExecSystemOutputDir - the exec system directory where the app writes its output\n    _tapisJobCreateDate - ISO 8601 date, example: 2021-04-26Z\n    _tapisJobCreateTime - ISO 8601 time, example: 18:44:55.544145884Z\n    _tapisJobCreateTimestamp - ISO 8601 timestamp, example: 2021-04-26T18:44:55.544145884Z\n    _tapisJobName - the user-chosen name of the Tapis job\n    _tapisJobOwner - the Tapis job's owner\n    _tapisJobUUID - the UUID of the Tapis job\n    _tapisJobWorkingDir - exec system directory that the app should use for temporary files\n    _tapisMaxMinutes - the maximum number of minutes allowed for the job to run\n    _tapisMemoryMB - the memory required per node by the app\n    _tapisNodes - the number of nodes on which the app runs\n    _tapisSysBatchScheduler - the HPC scheduler on the execution system\n    _tapisSysBucketName - an object store bucket name\n    _tapisSysHost - the IP address or DNS name of the exec system\n    _tapisSysRootDir - the root directory on the exec system\n    _tapisTenant - the tenant in which the job runs"}
{"filename": "jobs.md", "prompt": "Macro Substitution", "completion": "Tapis defines macros or template variables that get replaced with actual\nvalues at well-defined points during job creation. The act of replacing\na macro with a value is often called macro substitution or macro\nexpansion. The complete list of Tapis macros can be found at\n[JobTemplateVariables](https://github.com/tapis-project/tapis-jobs/blob/dev/tapis-jobslib/src/main/java/edu/utexas/tacc/tapis/jobs/model/enumerations/JobTemplateVariables.java).\n\nThere is a close relationship between these macro definitions and the\nTapis environment variables just discussed: Macros that have values\nassigned are passed as environment variables into application\ncontainers. This makes macros used during job creation available to\napplications at runtime.\n\nMost macro definitions are *ground* definitions because their values do\nnot depend on any other macros. On the other hand, *derived* macro\ndefinitions can include other macro definitions. For example, in\n[Directory Assignments](#directory-assignments) the default input file\ndirectory is constructed with two macro definitions:\n\n    execSystemInputDir = ${JobWorkingDir}
/jobs/${JobUUID
\n\nMacro values are referenced using the \\${Macro-name
 notation. Since\nderived macro definitions reference other macros, there is the\npossibility of circular references. Tapis detects these errors and\naborts job creation.\n\nBelow is the complete, ordered list of derived macros. Each macro in the\nlist can be defined using any ground macro and any macro that preceeds\nit in the list. Result are undefined if a derived macro references a\nmacro that follows it in the derived list.\n\n1.  JobName\n2.  JobWorkingDir\n3.  ExecSystemInputDir\n4.  ExecSystemExecDir\n5.  ExecSystemOutputDir\n6.  ArchiveSystemDir\n\nFinally, macro substitution is applied to the job *description* field,\nwhether the description is specified in an application or a submission\nrequest."
{"filename": "jobs.md", "prompt": "Macro Functions", "completion": "Directory assignments in systems, applications and job requests can also\nuse the **HOST_EVAL(\\$var)** function at the beginning of their path\nassignments. This function dynamically extracts the named environment\nvariable\\'s value from an execution or archive host *at the time the job\nrequest is made*. Specifically, the environment variable\\'s value is\nretrieved by logging into the host as the Job owner and issuing \\\"echo\n\\$var\\\". The example in [Data Transfer Nodes](#data-transfer-nodes) uses\nthis function.\n\nTo increase application portability, an optional default value can be\npassed into the **HOST_EVAL** function. The function\\'s complete\nsignature with the optional path parameter is:\n\n> **HOST_EVAL(\\$VAR, path)**\n\nIf the environment variable VAR does not exist on the host, then the\nliteral path parameter is returned by the function. This added\nflexibility allows applications to run in different environments, such\nas on TACC HPC systems that automatically expose certain environment\nvariables and VMs that might not. If the environment variable does not\nexist and no optional path parameter is provided, the job fails due to\ninvalid input."}
{"filename": "jobs.md", "prompt": "Job Status", "completion": "The list below contains all possible states of a Tapis job, which are\nindicated in the *status* field of a job record. The initial state is\nPENDING. Terminal states are FINISHED, CANCELLED and FAILED. The BLOCKED\nstate indicates that the job is recovering from a resource constraint,\nnetwork problem or other transient problem. When the problem clears, the\njob will restart from the state in which blocking occurred. :\n\n    PENDING - Job processing beginning\n    PROCESSING_INPUTS - Identifying input files for staging\n    STAGING_INPUTS - Transferring job input data to execution system\n    STAGING_JOB - Staging runtime assets to execution system\n    SUBMITTING_JOB - Submitting job to execution system\n    QUEUED - Job queued to execution system queue\n    RUNNING - Job running on execution system\n    ARCHIVING - Transferring job output to archive system\n    BLOCKED - Job blocked\n    PAUSED - Job processing suspended\n    FINISHED - Job completed successfully\n    CANCELLED - Job execution intentionally stopped\n    FAILED - Job failed\n\nNormal processing of a successfully executing job proceeds as follows:\n\n    PENDING->PROCESSING_INPUTS->STAGING_INPUTS->STAGING_JOB->SUBMITTING_JOB->\n      QUEUED->RUNNING->ARCHIVING->FINISHED"}
{"filename": "jobs.md", "prompt": "Notification Messages", "completion": "Notifications are the messages sent to subscribers who have registered\ninterest in certain job events. See [Subscriptions](#subscriptions) for\nan introduction to the different event types and a discussion of\nJOB_USER_EVENT content. In this section, we specify the messages sent to\nsubscribers for Jobs-generated events.\n\nFor events generated by the Jobs service, the *data* field in\nnotification messages received by subscribers contains a JSON object\nthat always include these fields:\n\n-   *jobName* - the user-specified job name\n-   *jobOwner* - the user who submitted the job\n-   *jobUuid* - the unique job ID\n-   *message* - a human readable message\n\nEach of the Job event types also include additional fields as shown:\n\n  ----------------------------------------------------------------\n  Job Event Type               Additional Fields\n  ---------------------------- -----------------------------------\n  JOB_NEW_STATUS               newJobStatus, oldJobStatus\n\n  JOB_INPUT_TRANSACTION_ID     transferStatus, transactionId\n\n  JOB_ARCHIVE_TRANSACTION_ID   transferStatus, transactionId\n\n  JOB_SUBSCRIPTION             action, numSubscriptions\n\n  JOB_SHARE_EVENT              resourceType, shareType, grantee,\n                               grantor\n\n  JOB_ERROR_MESSAGE            jobStatus\n  ----------------------------------------------------------------\n\nAdditionally, when either of these conditions hold:\n\n1.  JOB_NEW_STATUS messages indicate a **terminal** *newJobStatus*, or\n2.  JOB_ERROR_MESSAGE messages have *eventDetail* = \\\"FINAL_MESSAGE\\\",\n\nthen the following additional fields are included in the notification:\n\n-   *blockedCount* - the number of times the job blocked (JSON number)\n-   *remoteJobId* - execution system job id (ex: pid, slurm id, docker\n    hash, etc.)\n-   *remoteJobId2* - execution system auxilliary id associated with a\n    job\n-   *remoteOutcome* - FINISHED, FAILED, FAILED_SKIP_ARCHIVE\n-   *remoteResultInfo* - application exit code\n-   *remoteQueue* - execution system scheduler queue\n-   *remoteSubmitted* - time job was submitted on remote system\n-   *remoteStarted* - time job started running on remote system\n-   *remoteEnded* - time job stopped running on remote system\n\nJob terminal statuses are FINISHED, CANCELLED and FAILED."}
{"filename": "jobs.md", "prompt": "Dynamic Execution System Selection", "completion": "Not implementated yet."}
{"filename": "jobs.md", "prompt": "Data Transfer Nodes", "completion": "A Tapis system can be designated as a Data Transfer Node (DTN) as part\nof its definition. When an execution system specifies DTN usage in its\ndefinition, then the Jobs service will use the DTN to stage input files\nand archive output files.\n\nThe DTN usage pattern is effective when (1) the DTN has high performance\nnetwork and storage capabilities, and (2) an execution system can mount\nthe DTN\\'s file system. In this situation, bulk data transfers performed\nby Jobs benefit from the DTN\\'s high performance capabilities, while\napplications continue to access their execution system\\'s files as\nusual. From an application\\'s point of view, its data are simply where\nthey are expected to be, though they may have gotten there in a more\nexpeditious manner.\n\nDTN usage requires the coordinated configuration of a DTN, an execution\nsystem and a job. In addition, outside of Tapis, a system administrator\nmust mount the exported DTN file system at the expected mountpoint on an\nexecution system. We use the example below to illustrate DTN\nconfiguration and usage.\n\n    System: ds-exec\n      rootDir: /execRoot\n      dtnMountSourcePath: tapis://corral-dtn/\n      dtnMountPoint: /corral-repl\n      jobWorkingDir: HOST_EVAL($SCRATCH)\n\n    System: corral-dtn\n      host: cic-dtn01\n      isDtn: true\n      rootDir: /gpfs/corral3/repl\n\n    Job Request effective values:\n      execSystemId:         ds-exec\n      execSystemExecDir:    ${JobWorkingDir}
/jobs/${JobUUID
\n      execSystemInputDir:   ${DtnMountPoint
/projects/NHERI/shared/${JobOwner
/jobs/${JobUUID
\n      execSystemOutputDir:  ${DtnMountPoint
/projects/NHERI/shared/${JobOwner
/jobs/${JobUUID
/output\n\n    NFS Mount on ds-exec (done outside of Tapis):\n      mount -t nfs cic-dtn01:/gpfs/corral3/repl /execRoot/corral-repl\n\nThe example execution system, **ds-exec**, defines two DTN related\nvalues (both required to configure DTN usage):\n\n**dtnMountSourcePath**\n\n:   The tapis URL specifying the exported DTN path; the path is relative\n    to the DTN system\\'s rootDir (which is just \\\"/\\\" in this example).\n\n**dtnMountPoint**\n\n:   The path relative to the execution system\\'s rootDir where the\n    DtnMountSourcePath is mounted.\n\nThe execution system\\'s jobWorkingDir is defined to be the runtime value\nof the \\$SCRATCH environment variable; its rootDir is defined at\n/execRoot.\n\nThe Tapis DTN system, **corral-dtn**, host machine is cic-dtn01. The\nDTN\\'s rootDir (/gpfs/corral3/repl) is the directory prefix used on all\nmounts. Mounting takes place outside of Tapis by system administrators.\nThe actual NFS mount command has this general format:\n\n    mount -t nfs <dtn_host>:/<dtn_root_dir>/<path> <exec_system_mount_point>\n\nThe Job Request effective values depend on the DTN configuration are\nalso shown. These values could have been set in the application\ndefinition, the job request or in both. Values set in the job request\nare given priority. The execSystemId refers to the **ds-exec** system,\nwhich in this case specifies a DTN.\n\nContinuing with the above example, let\\'s say user *Bud* issues an\nOpensees job request that creates a job with id 123. The Jobs service\nwill stage the application\\'s input files using the DTN. The transfer\nrequest to the\n[Files](https://tapis.readthedocs.io/en/latest/technical/files.html)\nservice will write to this target URL:\n\n> tapis://corral-dtn/gpfs/corral3/repl/projects/NHERI/shared/Bud/jobs/123\n\nThis is the standard tapis URL format:\ntapis://\\<tapis-system\\>/\\<path\\>. After inputs are staged, the Job\nservice will inject this environment variable value (among others) into\nthe launched job\\'s container:\n\n> execSystemInputDir=/corral-repl/projects/NHERI/shared/Bud/jobs/123\n\nSince **ds-exec** mounts the corral root directory, the files staged to\ncorral /gpfs/corral3/repl are accessible at execSystemInputDir on\n**ds-exec**, relative to rootDir /execRoot. A similar approach would be\nused to transfer files to an archive system using the DTN, except this\ntime **corral-dtn** is the source of the file transfers rather than the\ntarget.\n\n------------------------------------------------------------------------"
{"filename": "jobs.md", "prompt": "Container Runtimes", "completion": "The Tapis v3 Jobs service currently supports Docker and Singularity\ncontainers run natively (i.e., not run using a batch scheduler like\nSlurm). In general, Jobs launches an application\\'s container on a\nremote system, monitors the container\\'s execution, and captures the\napplication\\'s exit code after it terminates. Jobs uses SSH to connect\nto the execution system to issue Docker, Singularity or native operating\nsystem commands.\n\nTo launch a job, the Jobs service creates a bash script,\n**tapisjob.sh**, with the runtime-specific commands needed to execute\nthe container. This script references **tapisjob.env**, a file Jobs\ncreates to pass environment variables to application containers. Both\nfiles are staged in the job\\'s execSystemExecDir and, by default, are\narchived with job output on the archive system. See\n[archiveFilter](#archivefilter) to override this default behavior,\nespecially if archives will be shared and the scripts pass sensitive\ninformation into containers."}
{"filename": "jobs.md", "prompt": "Docker", "completion": "To launch a Docker container, the Jobs service will SSH to the target\nhost and issue a command using this template:\n\n    docker run [docker options] image[:tag|@digest] [application args]\n\n1.  docker options: (optional) user-specified arguments passed to docker\n2.  image: (required) user-specified docker application image\n3.  application arguments: (optional) user-specified command line\n    arguments passed to the application\n\nThe docker\n[run-command](https://docs.docker.com/engine/reference/commandline/run/)\noptions *\\--cidfile*, *-d*, *-e*, *\\--env*, *\\--name*, *\\--rm*, and\n*\\--user* are reserved for use by Tapis. Most other Docker options are\navailable to the user. The Jobs service implements these calling\nconventions:\n\n1.  The container name is set to the job UUID.\n2.  The container\\'s user is set to the user ID used to establish the\n    SSH session.\n3.  The container ID file is specified as *\\<JobUUID\\>.cid* in the\n    execSystemExecDir, i.e., the directory from which the container is\n    launched.\n4.  The container is removed after execution using the *-rm* option or\n    by calling *docker rm*."}
{"filename": "jobs.md", "prompt": "Logging", "completion": "Logging should be considered up front when defining Tapis applications\nto run under Docker. Since Jobs removes Docker containers after they\nexecute, the container\\'s log is lost under the default Docker\n[logging](https://docs.docker.com/config/containers/logging/)\nconfiguration. Typically, Docker pipes *stdout* and *stderr* to the\ncontainer\\'s log, which requires the application to take deliberate\nsteps to preserve these outputs.\n\nAn application can maintain control over its log output by logging to a\nfile outside of the container. The application can do this by\nredirecting *stdout* and *stderr* or by explicitly writing to a file. As\ndiscussed in [dir-definitions](#dir-definitions), the application always\nhas read/write access to the host\\'s *execSystemOutputDir*, which is\nmounted at /TapisOutput in the container (see next section).\n\nOn the other hand, applications can run on machines where the default\nDocker log driver is configured to write to files or services outside of\ncontainers. In addition, Tapis passes any user-specified *log-driver*\nand *log-opts* options to *docker run*, so all\n[customizations](https://docs.docker.com/config/containers/logging/configure/)\nsupported by Docker are possible."}
{"filename": "jobs.md", "prompt": "Volume Mounts", "completion": "In addition to the above conventions,\n[bind](https://docs.docker.com/storage/bind-mounts/) mounts are used to\nmount the execution system\\'s standard Tapis directories at the same\nlocations in every application container.\n\n    execSystemExecDir   on host is mounted at /TapisExec in the container.\n    execSystemInputDir  on host is mounted at /TapisInput in the container.\n    execSystemOutputDir on host is mounted at /TapisOutput in the container."}
{"filename": "jobs.md", "prompt": "Singularity", "completion": "Tapis provides two distinct ways to launch a Singularity containers,\nusing *singluarity instance start* or *singularity run*."}
{"filename": "jobs.md", "prompt": "Singularity Start", "completion": "Singularity\\'s support for detached processes and services is\nimplemented natively by its instance\n[start](https://sylabs.io/guides/3.7/user-guide/cli/singularity_instance_start.html),\n[stop](https://sylabs.io/guides/3.7/user-guide/cli/singularity_instance_stop.html)\nand\n[list](https://sylabs.io/guides/3.7/user-guide/cli/singularity_instance_list.html)\ncommands. To launch a container, the Jobs service will SSH to the target\nhost and issue a command using this template:\n\n    singularity instance start [singularity options] <image id> [application arguments] <job uuid>\n\nwhere:\n\n1.  singularity options: (optional) user-specified argument passed to\n    singularity start\n2.  image id: (required) user-specified singularity application image\n3.  application arguments: (optional) user-specified command line\n    arguments passed to the application\n4.  job uuid: the job uuid used to name the instance (always set by\n    Jobs)\n\nThe singularity options *\\--pidfile*, *\\--env* and *\\--name* are\nreserved for use by Tapis. Users specify the environment variables to be\ninjected into their application containers via the\n[envVariables](#envvariables) parameter. Most other singularity options\nare available to users.\n\nJobs will then issue *singularity instance list* to obtain the\ncontainer\\'s process id (PID). Jobs determines that the application has\nterminated when the PID is no longer in use by the operating system.\n\nBy convention, Jobs will look for a **tapisjob.exitcode** file in the\nJob\\'s output directory after containers terminate. If found, the file\nshould contain only the integer code the application reported when it\nexited. If not found, Jobs assumes the application exited normally with\na zero exit code.\n\nFinally, Jobs issues a *singularity instance stop \\<job uuid\\>* to clean\nup the singularity runtime environment and terminate all processes\nassociated with the container."}
{"filename": "jobs.md", "prompt": "Singularity Run", "completion": "Jobs also supports a more do-it-yourself approach to running containers\non remote system using singularity\n[run](https://sylabs.io/guides/3.7/user-guide/cli/singularity_run.html).\nTo launch a container, the Jobs service will SSH to the target host and\nissue a command using this template:\n\n    nohup singularity run [singularity options.] <image id> [application arguments] > tapisjob.out 2>&1 &\n\nwhere:\n\n1.  [nohup](https://en.wikipedia.org/wiki/Nohup): allows the background\n    process to continue running even if the SSH session ends.\n2.  singularity options: (optional) user-specified arguments passed to\n    singularity run.\n3.  image id: (required) user-specified singularity application image.\n4.  application arguments: (optional) user-specified command line\n    arguments passed to the application.\n5.  redirection: stdout and stderr are redirected to **tapisjob.out** in\n    the job\\'s output directory.\n\nThe singularity *\\--env* option is reserved for use by Tapis. Users\nspecify the environment variables to be injected into their application\ncontainers via the [envVariables](#envvariables) parameter. Most other\nsingularity options are available to users.\n\nJobs will use the PID returned when issuing the background command to\nmonitor the container\\'s execution. Jobs determines that the application\nhas terminated when the PID is no longer in use by the operating system.\n\nJobs uses the same **TapisJob.exitcode** file convention introduced\nabove to attain the application\\'s exit code (if the file exists)."}
{"filename": "jobs.md", "prompt": "Required Scripts", "completion": "The Singularity Start and Singularity Run approaches boath allow SSH\nsessions between Jobs and execution hosts to end without interrupting\ncontainer execution. Each approach, however, requires that the\napplication image be appropriately constructed. Specifically,\n\n    Singularity start requires the startscript to be defined in the image.\n    Singularity run requires the runscript to be defined in the image."}
{"filename": "jobs.md", "prompt": "Required Termination Order", "completion": "Since Jobs monitors container execution by querying the operating system\nusing the PID obtained at launch time, the initially launched program\nshould be the last part of the application to terminate. The program\nspecified in the image script can spawn any number of processes (and\nthreads), but it should not exit before those processes complete."}
{"filename": "jobs.md", "prompt": "Optional Exit Code Convention", "completion": "Applications are not required to support the **TapisJob.exitcode** file\nconvention as described above, but it is the only way in which Jobs can\nreport the application specified exit status to the user.\n\n------------------------------------------------------------------------"}
{"filename": "jobs.md", "prompt": "Querying Jobs", "completion": ""}
{"filename": "jobs.md", "prompt": "Get Jobs list", "completion": "With PySDK:\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n    {\n    \"result\": [\n        {\n            \"uuid\": \"731b65f4-43e9-4a7a-b3a0-68644b53c1cb-007\",\n            \"name\": \"SyRunSleepSecondsNoIPFiles-2\",\n            \"owner\": \"testuser2\",\n            \"appId\": \"SyRunSleepSecondsNoIPFiles-2\",\n            \"created\": \"2021-07-21T19:56:02.163984Z\",\n            \"status\": \"FINISHED\",\n            \"remoteStarted\": \"2021-07-21T19:56:18.628448Z\",\n            \"ended\": \"2021-07-21T19:56:52.637554Z\",\n            \"tenant\": \"dev\",\n            \"execSystemId\": \"tapisv3-exec2\",\n            \"archiveSystemId\": \"tapisv3-exec2\",\n            \"appVersion\": \"0.0.1\",\n            \"lastUpdated\": \"2021-07-21T19:56:52.637554Z\"\n        }
,\n        {\n            \"uuid\": \"79dfaba5-bfb4-4c6d-a198-643bda211dbf-007\",\n            \"name\": \"SlurmSleepSeconds\",\n            \"owner\": \"testuser2\",\n            \"appId\": \"SlurmSleepSecondsVM\",\n            \"created\": \"2021-07-21T19:16:02.019916Z\",\n            \"status\": \"FINISHED\",\n            \"remoteStarted\": \"2021-07-21T19:16:35.102868Z\",\n            \"ended\": \"2021-07-21T19:16:57.909940Z\",\n            \"tenant\": \"dev\",\n            \"execSystemId\": \"tapisv3-exec2-slurm\",\n            \"archiveSystemId\": \"tapisv3-exec2-slurm\",\n            \"appVersion\": \"0.0.1\",\n            \"lastUpdated\": \"2021-07-21T19:16:57.909940Z\"\n        
\n    ],\n    \"status\": \"success\",\n    \"message\": \"JOBS_LIST_RETRIVED Jobs list for the user testuser2 in the tenant dev retrived.\",\n    \"version\": \"1.0.0-rc1\",\n    \"metadata\": {\n        \"recordCount\": 2,\n        \"recordLimit\": 2,\n        \"recordsSkipped\": 0,\n        \"orderBy\": \"lastUpdated(desc),name(asc)\",\n        \"startAfter\": null,\n        \"totalCount\": 1799\n    
\n    
"
{"filename": "jobs.md", "prompt": "Get Job Details", "completion": "With PySDK:\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n    {\n    \"result\": {\n       \"id\": 1711,\n       \"name\": \"SyRunSleepSecondsNoIPFiles-2\",\n       \"owner\": \"testuser2\",\n       \"tenant\": \"dev\",\n       \"description\": \"Sleep for a specified amount of time\",\n       \"status\": \"FINISHED\",\n       \"lastMessage\": \"Setting job status to FINISHED.\",\n       \"created\": \"2021-07-12T23:56:01.790165Z\",\n       \"ended\": \"2021-07-12T23:56:55.962694Z\",\n       \"lastUpdated\": \"2021-07-12T23:56:55.962694Z\",\n       \"uuid\": \"ba34f946-8a18-44c4-9b25-19e21dfadf69-007\",\n       \"appId\": \"SyRunSleepSecondsNoIPFiles-2\",\n       \"appVersion\": \"0.0.1\",\n       \"archiveOnAppError\": true,\n       \"dynamicExecSystem\": false,\n       \"execSystemId\": \"tapisv3-exec2\",\n       \"execSystemExecDir\": \"/workdir/jobs/ba34f946-8a18-44c4-9b25-19e21dfadf69-007\",\n       \"execSystemInputDir\": \"/workdir/jobs/ba34f946-8a18-44c4-9b25-19e21dfadf69-007\",\n       \"execSystemOutputDir\": \"/workdir/jobs/ba34f946-8a18-44c4-9b25-19e21dfadf69-007/output\",\n       \"execSystemLogicalQueue\": null,\n       \"archiveSystemId\": \"tapisv3-exec\",\n       \"archiveSystemDir\": \"/jobs/ba34f946-8a18-44c4-9b25-19e21dfadf69-007/archive\",\n       \"dtnSystemId\": null,\n       \"dtnMountSourcePath\": null,\n       \"dtnMountPoint\": null,\n       \"nodeCount\": 1,\n       \"coresPerNode\": 1,\n       \"memoryMB\": 100,\n       \"maxMinutes\": 240,\n       \"fileInputs\": \"[]\",\n       \"parameterSet\": \"{\\\"appArgs\\\": [], \\\"envVariables\\\": [{\\\"key\\\": \\\"_tapisAppId\\\", \\\"value\\\": \\\"SyRunSleepSecondsNoIPFiles-2\\\"}
, {\\\"key\\\": \\\"_tapisAppVersion\\\", \\\"value\\\": \\\"0.0.1\\\"
, {\\\"key\\\": \\\"_tapisArchiveOnAppError\\\", \\\"value\\\": \\\"true\\\"
, {\\\"key\\\": \\\"_tapisArchiveSystemDir\\\", \\\"value\\\": \\\"/jobs/ba34f946-8a18-44c4-9b25-19e21dfadf69-007/archive\\\"
, {\\\"key\\\": \\\"_tapisArchiveSystemId\\\", \\\"value\\\": \\\"tapisv3-exec\\\"
, {\\\"key\\\": \\\"_tapisCoresPerNode\\\", \\\"value\\\": \\\"1\\\"
, {\\\"key\\\": \\\"_tapisDynamicExecSystem\\\", \\\"value\\\": \\\"false\\\"
, {\\\"key\\\": \\\"_tapisEffeciveUserId\\\", \\\"value\\\": \\\"testuser2\\\"
, {\\\"key\\\": \\\"_tapisExecSystemExecDir\\\", \\\"value\\\": \\\"/workdir/jobs/ba34f946-8a18-44c4-9b25-19e21dfadf69-007\\\"
, {\\\"key\\\": \\\"_tapisExecSystemId\\\", \\\"value\\\": \\\"tapisv3-exec2\\\"
, {\\\"key\\\": \\\"_tapisExecSystemInputDir\\\", \\\"value\\\": \\\"/workdir/jobs/ba34f946-8a18-44c4-9b25-19e21dfadf69-007\\\"
, {\\\"key\\\": \\\"_tapisExecSystemOutputDir\\\", \\\"value\\\": \\\"/workdir/jobs/ba34f946-8a18-44c4-9b25-19e21dfadf69-007/output\\\"
, {\\\"key\\\": \\\"_tapisJobCreateDate\\\", \\\"value\\\": \\\"2021-07-12Z\\\"
, {\\\"key\\\": \\\"_tapisJobCreateTime\\\", \\\"value\\\": \\\"23:56:01.790165454Z\\\"
, {\\\"key\\\": \\\"_tapisJobCreateTimestamp\\\", \\\"value\\\": \\\"2021-07-12T23:56:01.790165454Z\\\"
, {\\\"key\\\": \\\"_tapisJobName\\\", \\\"value\\\": \\\"SyRunSleepSecondsNoIPFiles-2\\\"
, {\\\"key\\\": \\\"_tapisJobOwner\\\", \\\"value\\\": \\\"testuser2\\\"
, {\\\"key\\\": \\\"_tapisJobUUID\\\", \\\"value\\\": \\\"ba34f946-8a18-44c4-9b25-19e21dfadf69-007\\\"
, {\\\"key\\\": \\\"_tapisJobWorkingDir\\\", \\\"value\\\": \\\"workdir\\\"
, {\\\"key\\\": \\\"_tapisMaxMinutes\\\", \\\"value\\\": \\\"240\\\"
, {\\\"key\\\": \\\"_tapisMemoryMB\\\", \\\"value\\\": \\\"100\\\"
, {\\\"key\\\": \\\"_tapisNodes\\\", \\\"value\\\": \\\"1\\\"
, {\\\"key\\\": \\\"_tapisSysHost\\\", \\\"value\\\": \\\"129.114.17.113\\\"
, {\\\"key\\\": \\\"_tapisSysRootDir\\\", \\\"value\\\": \\\"/home/testuser2\\\"
, {\\\"key\\\": \\\"_tapisTenant\\\", \\\"value\\\": \\\"dev\\\"
, {\\\"key\\\": \\\"JOBS_PARMS\\\", \\\"value\\\": \\\"15\\\"
, {\\\"key\\\": \\\"MAIN_CLASS\\\", \\\"value\\\": \\\"edu.utexas.tacc.testapps.tapis.SleepSecondsSy\\\"
], \\\"archiveFilter\\\": {\\\"excludes\\\": [], \\\"includes\\\": [\\\"Sleep*\\\", \\\"tapisjob.*\\\"], \\\"includeLaunchFiles\\\": true
, \\\"containerArgs\\\": [], \\\"schedulerOptions\\\": []
\",\n       \"execSystemConstraints\": null,\n       \"subscriptions\": \"[]\",\n       \"blockedCount\": 0,\n       \"remoteJobId\": \"1466046\",\n       \"remoteJobId2\": null,\n       \"remoteOutcome\": \"FINISHED\",\n       \"remoteResultInfo\": \"0\",\n       \"remoteQueue\": null,\n       \"remoteSubmitted\": null,\n       \"remoteStarted\": \"2021-07-12T23:56:20.900039Z\",\n       \"remoteEnded\": \"2021-07-12T23:56:42.411522Z\",\n       \"remoteSubmitRetries\": 0,\n       \"remoteChecksSuccess\": 3,\n       \"remoteChecksFailed\": 0,\n       \"remoteLastStatusCheck\": \"2021-07-12T23:56:42.382661Z\",\n       \"inputTransactionId\": null,\n       \"inputCorrelationId\": null,\n       \"archiveTransactionId\": \"66bc6c9a-210b-4ee6-9da3-252922928e7b\",\n       \"archiveCorrelationId\": \"87f62e69-c180-4ad1-9aa7-ac5ada78e1b6\",\n       \"tapisQueue\": \"tapis.jobq.submit.DefaultQueue\",\n       \"visible\": true,\n       \"createdby\": \"testuser2\",\n       \"createdbyTenant\": \"dev\",\n       \"tags\": [\n           \"singularity\",\n           \"sleep\",\n           \"test\"\n       ],\n       \"_fileInputsSpec\": null,\n       \"_parameterSetModel\": null\n    
,\n    \"status\": \"success\",\n    \"message\": \"JOBS_RETRIEVED Job ba34f946-8a18-44c4-9b25-19e21dfadf69-007 retrieved.\",\n    \"version\": \"1.0.0-rc1\",\n    \"metadata\": null\n    
"
{"filename": "jobs.md", "prompt": "Get Job Status", "completion": "With PySDK:\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n    {\n      \"result\": {\n        \"status\": \"FINISHED\"\n        }
,\n        \"status\": \"success\",\n        \"message\": \"JOBS_STATUS_RETRIEVED Status of the Job ba34f946-8a18-44c4-9b25-19e21dfadf69-007 retrieved.\",\n        \"version\": \"1.0.0-rc1\",\n        \"metadata\": null\n        
"
{"filename": "jobs.md", "prompt": "Get Job History", "completion": "With PySDK:\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n    {\n    \"result\": [\n        {\n            \"event\": \"JOB_NEW_STATUS\",\n            \"created\": \"2021-07-12T23:56:02.365996Z\",\n            \"jobStatus\": \"PENDING\",\n            \"description\": \"The job has transitioned to a new status: PENDING.\",\n            \"transferTaskUuid\": null,\n            \"transferSummary\": {}
\n        
,\n        {\n            \"event\": \"JOB_NEW_STATUS\",\n            \"created\": \"2021-07-12T23:56:02.799166Z\",\n            \"jobStatus\": \"PROCESSING_INPUTS\",\n            \"description\": \"The job has transitioned to a new status: PROCESSING_INPUTS. The previous job status was PENDING.\",\n            \"transferTaskUuid\": null,\n            \"transferSummary\": {
\n        
,\n        {\n            \"event\": \"JOB_NEW_STATUS\",\n            \"created\": \"2021-07-12T23:56:10.203007Z\",\n            \"jobStatus\": \"STAGING_INPUTS\",\n            \"description\": \"The job has transitioned to a new status: STAGING_INPUTS. The previous job status was PROCESSING_INPUTS.\",\n            \"transferTaskUuid\": null,\n            \"transferSummary\": {
\n        
,\n        {\n            \"event\": \"JOB_NEW_STATUS\",\n            \"created\": \"2021-07-12T23:56:10.226013Z\",\n            \"jobStatus\": \"STAGING_JOB\",\n            \"description\": \"The job has transitioned to a new status: STAGING_JOB. The previous job status was STAGING_INPUTS.\",\n            \"transferTaskUuid\": null,\n            \"transferSummary\": {
\n        
,\n        {\n            \"event\": \"JOB_NEW_STATUS\",\n            \"created\": \"2021-07-12T23:56:20.720637Z\",\n            \"jobStatus\": \"SUBMITTING_JOB\",\n            \"description\": \"The job has transitioned to a new status: SUBMITTING_JOB. The previous job status was STAGING_JOB.\",\n            \"transferTaskUuid\": null,\n            \"transferSummary\": {
\n        
,\n        {\n            \"event\": \"JOB_NEW_STATUS\",\n            \"created\": \"2021-07-12T23:56:20.888569Z\",\n            \"jobStatus\": \"QUEUED\",\n            \"description\": \"The job has transitioned to a new status: QUEUED. The previous job status was SUBMITTING_JOB.\",\n            \"transferTaskUuid\": null,\n            \"transferSummary\": {
\n        
,\n        {\n            \"event\": \"JOB_NEW_STATUS\",\n            \"created\": \"2021-07-12T23:56:20.902511Z\",\n            \"jobStatus\": \"RUNNING\",\n            \"description\": \"The job has transitioned to a new status: RUNNING. The previous job status was QUEUED.\",\n            \"transferTaskUuid\": null,\n            \"transferSummary\": {
\n        
,\n        {\n            \"event\": \"JOB_NEW_STATUS\",\n            \"created\": \"2021-07-12T23:56:42.427492Z\",\n            \"jobStatus\": \"ARCHIVING\",\n            \"description\": \"The job has transitioned to a new status: ARCHIVING. The previous job status was RUNNING.\",\n            \"transferTaskUuid\": null,\n            \"transferSummary\": {
\n        
,\n        {\n            \"event\": \"JOB_NEW_STATUS\",\n            \"created\": \"2021-07-12T23:56:55.966883Z\",\n            \"jobStatus\": \"FINISHED\",\n            \"description\": \"The job has transitioned to a new status: FINISHED. The previous job status was ARCHIVING.\",\n            \"transferTaskUuid\": null,\n            \"transferSummary\": {
\n        
\n    ],\n    \"status\": \"success\",\n    \"message\": \"JOBS_HISTORY_RETRIEVED Job ba34f946-8a18-44c4-9b25-19e21dfadf69-007 history retrieved for user testuser2 tenant dev\",\n    \"version\": \"1.0.0-rc1\",\n    \"metadata\": {\n        \"recordCount\": 9,\n        \"recordLimit\": 100,\n        \"recordsSkipped\": 0,\n        \"orderBy\": null,\n        \"startAfter\": null,\n        \"totalCount\": -1\n    
\n    
"
{"filename": "jobs.md", "prompt": "Get Job Output Listing", "completion": "With PySDK:\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n    {\n    \"result\": [\n        {\n            \"mimeType\": null,\n            \"type\": \"file\",\n            \"owner\": \"1003\",\n            \"group\": \"1003\",\n            \"nativePermissions\": \"rw-rw-r--\",\n            \"uri\": \"tapis://dev/tapisv3-exec/jobs/ba34f946-8a18-44c4-9b25-19e21dfadf69-007/archive/SleepSeconds.out\",\n            \"lastModified\": \"2021-07-12T23:56:54Z\",\n            \"name\": \"SleepSeconds.out\",\n            \"path\": \"/jobs/ba34f946-8a18-44c4-9b25-19e21dfadf69-007/archive/SleepSeconds.out\",\n            \"size\": 3538\n        }
,\n        {\n            \"mimeType\": null,\n            \"type\": \"file\",\n            \"owner\": \"1003\",\n            \"group\": \"1003\",\n            \"nativePermissions\": \"rw-rw-r--\",\n            \"uri\": \"tapis://dev/tapisv3-exec/jobs/ba34f946-8a18-44c4-9b25-19e21dfadf69-007/archive/tapisjob.env\",\n            \"lastModified\": \"2021-07-12T23:56:53Z\",\n            \"name\": \"tapisjob.env\",\n            \"path\": \"/jobs/ba34f946-8a18-44c4-9b25-19e21dfadf69-007/archive/tapisjob.env\",\n            \"size\": 1051\n        
,\n        {\n            \"mimeType\": null,\n            \"type\": \"file\",\n            \"owner\": \"1003\",\n            \"group\": \"1003\",\n            \"nativePermissions\": \"rw-rw-r--\",\n            \"uri\": \"tapis://dev/tapisv3-exec/jobs/ba34f946-8a18-44c4-9b25-19e21dfadf69-007/archive/tapisjob.exitcode\",\n            \"lastModified\": \"2021-07-12T23:56:54Z\",\n            \"name\": \"tapisjob.exitcode\",\n            \"path\": \"/jobs/ba34f946-8a18-44c4-9b25-19e21dfadf69-007/archive/tapisjob.exitcode\",\n            \"size\": 1\n        
,\n        {\n            \"mimeType\": null,\n            \"type\": \"file\",\n            \"owner\": \"1003\",\n            \"group\": \"1003\",\n            \"nativePermissions\": \"rw-rw-r--\",\n            \"uri\": \"tapis://dev/tapisv3-exec/jobs/ba34f946-8a18-44c4-9b25-19e21dfadf69-007/archive/tapisjob.out\",\n            \"lastModified\": \"2021-07-12T23:56:54Z\",\n            \"name\": \"tapisjob.out\",\n            \"path\": \"/jobs/ba34f946-8a18-44c4-9b25-19e21dfadf69-007/archive/tapisjob.out\",\n            \"size\": 3566\n        
,\n        {\n            \"mimeType\": \"application/x-shar\",\n            \"type\": \"file\",\n            \"owner\": \"1003\",\n            \"group\": \"1003\",\n            \"nativePermissions\": \"rw-rw-r--\",\n            \"uri\": \"tapis://dev/tapisv3-exec/jobs/ba34f946-8a18-44c4-9b25-19e21dfadf69-007/archive/tapisjob.sh\",\n            \"lastModified\": \"2021-07-12T23:56:54Z\",\n            \"name\": \"tapisjob.sh\",\n            \"path\": \"/jobs/ba34f946-8a18-44c4-9b25-19e21dfadf69-007/archive/tapisjob.sh\",\n            \"size\": 979\n        
\n    ],\n    \"status\": \"success\",\n    \"message\": \"JOBS_OUTPUT_FILES_LIST_RETRIEVED Job ba34f946-8a18-44c4-9b25-19e21dfadf69-007 output files list retrieved for the user testuser2 in the tenant dev.\",\n    \"version\": \"1.0.0-rc1\",\n    \"metadata\": {\n        \"recordCount\": 5,\n        \"recordLimit\": 100,\n        \"recordsSkipped\": 0,\n        \"orderBy\": null,\n        \"startAfter\": null,\n        \"totalCount\": 0\n    
\n    
\n\nThe Job output list API retrieves job\\'s output files list for a\npreviously submitted job by its UUID. By default, the job must be in a\nterminal state (FINISHED or FAILED or CANCELLED) for the API to list the\njob\\'s output files . There is a query parameter allowIfRunning set to\nfalse by default. If allowIfRunning=true, the API returns the job output\nfiles list even if the job is not in the terminal state. Note that if a\nfile is being written, still the file is listed."
{"filename": "jobs.md", "prompt": "Get Job Output Download", "completion": "With PySDK:\n\n\n\nWith CURL:\n\n\n\nAll the files in the the requested outputPath get downloaded in a zip\nfile.\n\nThe Jobs output download API retrieves the job\\'s output files for a\npreviously submitted job by its UUID. By default, the job must be in a\nterminal state (FINISHED or FAILED or CANCELLED) for the API to download\nthe job\\'s output files. There is a query parameter allowIfRunning set\nto false by default. If allowIfRunning=true, the API allows downloading\nthe job output files even if the job is not in the terminal state. Note\nthat if a file is being written at the time of the request, the file is\nstill downloaded with the current content."}
{"filename": "jobs.md", "prompt": "Dedicated Search Endpoint", "completion": "The jobs service provides dedicated search end-points to query jobs\nbased on different conditions. The GET end-point allows to specify the\nquery in the query parameters while the POST end-point allows complex\nqueries in the request body using SQL-like syntax."}
{"filename": "jobs.md", "prompt": "Search using GET on Dedicated Endpoint", "completion": "With CURL:\n\n\n\nThe response will look something like the following:\n\n    {\n     \"result\": [\n          {\n              \"uuid\": \"79234b2a-0995-4632-956e-b940d10607ba-007\",\n              \"name\": \"SyRunSleepSecondsNoIPFiles-2\",\n              \"owner\": \"testuser2\",\n              \"appId\": \"SyRunSleepSecondsNoIPFiles-2\",\n              \"created\": \"2021-07-20T23:56:02.616Z\",\n              \"status\": \"FINISHED\",\n              \"remoteStarted\": \"2021-07-20T23:56:20.368Z\",\n              \"ended\": \"2021-07-20T23:56:54.409Z\",\n              \"tenant\": \"dev\",\n              \"execSystemId\": \"tapisv3-exec2\",\n              \"archiveSystemId\": \"tapisv3-exec\",\n              \"appVersion\": \"0.0.1\",\n              \"lastUpdated\": \"2021-07-20T23:56:54.409Z\"\n          }
,\n          {\n              \"uuid\": \"432f7018-070d-41c3-ba0e-a685f7f11e5c-007\",\n              \"name\": \"SlurmSleepSeconds\",\n              \"owner\": \"testuser2\",\n              \"appId\": \"SlurmSleepSecondsVM\",\n              \"created\": \"2021-07-20T23:16:01.629Z\",\n              \"status\": \"FINISHED\",\n              \"remoteStarted\": \"2021-07-20T23:16:24.781Z\",\n              \"ended\": \"2021-07-20T23:16:58.745Z\",\n              \"tenant\": \"dev\",\n              \"execSystemId\": \"tapisv3-exec2-slurm\",\n              \"archiveSystemId\": \"tapisv3-exec\",\n              \"appVersion\": \"0.0.1\",\n              \"lastUpdated\": \"2021-07-20T23:16:58.745Z\"\n          
\n      ],\n      \"status\": \"success\",\n      \"message\": \"JOBS_SEARCH_RESULT_LIST_RETRIEVED Jobs search list for the user testuser2 in the tenant dev retrieved.\",\n      \"version\": \"1.0.0-rc1\",\n      \"metadata\": {\n          \"recordCount\": 2,\n          \"recordLimit\": 2,\n          \"recordsSkipped\": 0,\n          \"orderBy\": \"lastUpdated(desc),name(asc)\",\n          \"startAfter\": null,\n          \"totalCount\": 246\n      
\n      
"
{"filename": "jobs.md", "prompt": "Search using POST on Dedicated Endpoint", "completion": "A user can make complex queries to Jobs service by specifying SQL-like\nsyntax in the request body to the end-point /v3/jobs/search. An example\nrequest body in json format is shown below:\n\n    {\n      \"search\":\n        [\n          \"(status = 'FINISHED' AND name = 'SleepSeconds') \",\n          \" OR (tags IN ('test'))\"\n        ]\n    }
\n\nWith cURL:\n\n\n\nThe response looks like this:\n\n    {\n    \"result\": [\n        {\n            \"name\": \"SleepSecondsLoadTest\",\n            \"status\": \"FINISHED\",\n            \"appId\": \"SleepSeconds-Load\",\n            \"tags\": [\n                \"sleep\",\n                \"test\"\n            ],\n            \"uuid\": \"e17edea6-33f8-441c-867c-d9d23509dd55-007\"\n        
,\n        {\n            \"name\": \"SleepSecondsLoadTest\",\n            \"status\": \"FINISHED\",\n            \"appId\": \"SleepSeconds-Load\",\n            \"tags\": [\n                \"sleep\",\n                \"test\"\n            ],\n            \"uuid\": \"a3a539a9-c0e5-4a02-82c3-0dfbcada47f9-007\"\n        
\n    ],\n    \"status\": \"success\",\n    \"message\": \"JOBS_SEARCH_RESULT_LIST_RETRIEVED Jobs search list for the user testuser2 in the tenant dev retrieved. \",\n    \"version\": \"1.3.0\",\n    \"commit\": \"ee1b3342\",\n    \"build\": \"2023-03-01T15:42:55Z\",\n    \"metadata\": {\n        \"recordCount\": 2,\n        \"recordLimit\": 2,\n        \"recordsSkipped\": 0,\n        \"orderBy\": null,\n        \"startAfter\": null,\n        \"totalCount\": 27345\n    
\n\n------------------------------------------------------------------------"
{"filename": "jobs.md", "prompt": "Job Actions", "completion": ""}
{"filename": "jobs.md", "prompt": "Job Cancel", "completion": "A previously submitted job not in terminal state can be cancelled by its\nUUID.\n\nWith PySDK:\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n    {\n    \"result\": {\n        \"message\": \"JOBS_JOB_CANCEL_ACCEPTED Request to cancel job 19b06299-4e7c-4b27-ae77-2258e9dc4734-007 has been accepted. \"\n    }
,\n    \"status\": \"success\",\n    \"message\": \"JOBS_JOB_CANCEL_ACCEPTED_DETAILS Request to cancel job 19b06299-4e7c-4b27-ae77-2258e9dc4734-007 has been accepted. If the job is in a terminal state, the request will have no effect. If the job is transitioning between active and blocked states, another cancel request may need to be sent.\",\n    \"version\": \"1.2.1\",\n    \"metadata\": null\n    
"
{"filename": "jobs.md", "prompt": "Hide Job", "completion": "With PySDK:\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n    {\n    \"result\": {\n        \"message\": \"JOBS_JOB_CHANGED_VISIBILITY Job 19b06299-4e7c-4b27-ae77-2258e9dc4734-007 has been changed to hidden.\"\n    }
,\n    \"status\": \"success\",\n    \"message\": \"JOBS_JOB_CHANGED_VISIBILITY Job 19b06299-4e7c-4b27-ae77-2258e9dc4734-007 has been changed to hidden.\",\n    \"version\": \"1.2.1\",\n    \"metadata\": null\n    
"
{"filename": "jobs.md", "prompt": "Unhide Job", "completion": "With PySDK:\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n    {\n    \"result\": {\n        \"message\": \"JOBS_JOB_CHANGED_VISIBILITY Job 19b06299-4e7c-4b27-ae77-2258e9dc4734-007 has been changed to unhidden.\"\n    }
,\n    \"status\": \"success\",\n    \"message\": \"JOBS_JOB_CHANGED_VISIBILITY Job 19b06299-4e7c-4b27-ae77-2258e9dc4734-007 has been changed to unhidden.\",\n    \"version\": \"1.2.1\",\n    \"metadata\": null\n    
\n\n------------------------------------------------------------------------"
{"filename": "jobs.md", "prompt": "Job Sharing", "completion": ""}
{"filename": "jobs.md", "prompt": "Share a Job", "completion": "A previously submitted job can be shared with a user in the same tenant.\nJob resources that can shared are: JOB_HISTORY, JOB_RESUBMIT_REQUEST,\nJOB_OUTPUT. Currently only READ permission on the resources are allowed.\n\nWith PySDK:\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n    {\n    \"result\": {\n        \"message\": \"JOBS_JOB_SHARED The job ccec730b-22ad-4088-a87e-bb8cfb2ab2e6-007 resource is shared by testuser2 to testuser6 in tenant dev\"\n    }
,\n    \"status\": \"success\",\n    \"message\": \"JOBS_JOB_SHARED The job ccec730b-22ad-4088-a87e-bb8cfb2ab2e6-007 resource is shared by testuser2 to testuser6 in tenant dev\",\n    \"version\": \"1.2.1\",\n    \"metadata\": null\n    
"
{"filename": "jobs.md", "prompt": "Get Job Share Information", "completion": "With PySDK:\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n    {\n    \"result\": [\n        {\n            \"tenant\": \"dev\",\n            \"createdby\": \"testuser2\",\n            \"jobUuid\": \"ccec730b-22ad-4088-a87e-bb8cfb2ab2e6-007\",\n            \"grantee\": \"testuser5\",\n            \"jobResource\": \"JOB_HISTORY\",\n            \"jobPermission\": \"READ\",\n            \"created\": \"2022-06-16T14:53:31.899199Z\"\n        }
,\n        {\n            \"tenant\": \"dev\",\n            \"createdby\": \"testuser2\",\n            \"jobUuid\": \"ccec730b-22ad-4088-a87e-bb8cfb2ab2e6-007\",\n            \"grantee\": \"testuser5\",\n            \"jobResource\": \"JOB_OUTPUT\",\n            \"jobPermission\": \"READ\",\n            \"created\": \"2022-06-16T14:53:32.004831Z\"\n        
,\n        {\n            \"tenant\": \"dev\",\n            \"createdby\": \"testuser2\",\n            \"jobUuid\": \"ccec730b-22ad-4088-a87e-bb8cfb2ab2e6-007\",\n            \"grantee\": \"testuser6\",\n            \"jobResource\": \"JOB_HISTORY\",\n            \"jobPermission\": \"READ\",\n            \"created\": \"2022-06-16T17:17:50.981844Z\"\n        
,\n        {\n            \"tenant\": \"dev\",\n            \"createdby\": \"testuser2\",\n            \"jobUuid\": \"ccec730b-22ad-4088-a87e-bb8cfb2ab2e6-007\",\n            \"grantee\": \"testuser6\",\n            \"jobResource\": \"JOB_RESUBMIT_REQUEST\",\n            \"jobPermission\": \"READ\",\n            \"created\": \"2022-06-16T17:17:51.059726Z\"\n        
,\n        {\n            \"tenant\": \"dev\",\n            \"createdby\": \"testuser2\",\n            \"jobUuid\": \"ccec730b-22ad-4088-a87e-bb8cfb2ab2e6-007\",\n            \"grantee\": \"testuser6\",\n            \"jobResource\": \"JOB_OUTPUT\",\n            \"jobPermission\": \"READ\",\n            \"created\": \"2022-07-14T19:57:15.838019Z\"\n        
\n    ],\n    \"status\": \"success\",\n    \"message\": \"JOBS_JOB_SHARE_INFO_RETRIEVED Share information retrieved for the job ccec730b-22ad-4088-a87e-bb8cfb2ab2e6-007 for user testuser2 in tenant dev.\",\n    \"version\": \"1.2.1\",\n    \"metadata\": {\n        \"recordCount\": 5,\n        \"recordLimit\": 100,\n        \"recordsSkipped\": 0,\n        \"orderBy\": null,\n        \"startAfter\": null,\n        \"totalCount\": 5\n    
\n    
"
{"filename": "jobs.md", "prompt": "Unshare Job", "completion": "With PySDK:\n\n\n\nWith CURL:\n\n\n\nThe response will look something like this:\n\n    {\n    \"result\": {\n        \"message\": \"JOBS_JOB_UNSHARED The job ccec730b-22ad-4088-a87e-bb8cfb2ab2e6-007 resource is unshared by testuser2 to testuser6 in tenant dev\"\n    }
,\n    \"status\": \"success\",\n    \"message\": \"JOBS_JOB_UNSHARED The job ccec730b-22ad-4088-a87e-bb8cfb2ab2e6-007 resource is unshared by testuser2 to testuser6 in tenant dev\",\n    \"version\": \"1.2.1\",\n    \"metadata\": null\n    
"
{"filename": "jobs.md", "prompt": "List Shared Jobs", "completion": "The query parameter listType=SHARED_JOBS in the jobs list end-point\nallows to list all shared jobs for a user, say testuser6, using\ntestuser6\\'s JWT. Note testuser6 is not the owner of the jobs listed.\nDefault value for listType is MY_JOBS and to list all jobs including\nboth shared and testuser6 owned job, listType=ALL_JOBS.\n\nWith PySDK:\n\n\n\nWith CURL:\n\n\n\nThe response will look something like this:\n\n    {\n     \"result\": [\n      {\n          \"uuid\": \"ccec730b-22ad-4088-a87e-bb8cfb2ab2e6-007\",\n          \"name\": \"SleepSeconds\",\n          \"owner\": \"testuser2\",\n          \"appId\": \"SleepSeconds\",\n          \"created\": \"2021-04-09T02:47:57.760Z\",\n          \"status\": \"FINISHED\",\n          \"remoteStarted\": \"2021-04-09T02:48:08.946Z\",\n          \"ended\": \"2021-04-09T02:48:16.669Z\",\n          \"tenant\": \"dev\",\n          \"execSystemId\": \"tapisv3-exec2\",\n          \"archiveSystemId\": \"tapisv3-exec\",\n          \"appVersion\": \"0.0.1\",\n          \"lastUpdated\": \"2021-04-09T02:48:16.669Z\"\n      }
\n      ],\n      \"status\": \"success\",\n      \"message\": \"JOBS_LIST_RETRIVED Jobs list for the user testuser6 in the tenant dev retrived.\",\n      \"version\": \"1.2.1\",\n      \"metadata\": {\n      \"recordCount\": 1,\n      \"recordLimit\": 100,\n      \"recordsSkipped\": 0,\n      \"orderBy\": null,\n      \"startAfter\": null,\n      \"totalCount\": -1\n      
\n      
"
{"filename": "jobs.md", "prompt": "Job Search on Shared Job", "completion": "Job search on a list of jobs can be performed using the query parameter\nlistType=SHARED_JOBS in the search end-point.\n\nIn the following example, we use testuser5 JWT to do job search on list\nof jobs shared with testuser5.\n\nWith CURL:\n\n\n\nThe response will look something like this:\n\n    {\n    \"result\": [\n        {\n            \"uuid\": \"3b9cb514-4962-44e8-a851-55e933e558c0-007\",\n            \"name\": \"SleepSeconds\",\n            \"owner\": \"testuser2\",\n            \"appId\": \"SleepSeconds\",\n            \"created\": \"2022-07-05T21:34:11.355Z\",\n            \"status\": \"FINISHED\",\n            \"remoteStarted\": \"2022-07-05T21:34:48.776Z\",\n            \"ended\": \"2022-07-05T21:35:31.808Z\",\n            \"tenant\": \"dev\",\n            \"execSystemId\": \"tapisv3-exec2\",\n            \"archiveSystemId\": \"tapisv3-exec\",\n            \"appVersion\": \"0.0.1\",\n            \"lastUpdated\": \"2022-07-05T21:35:31.808Z\"\n        }
,\n        {\n            \"uuid\": \"eacde4c4-1d93-4393-b220-63aea509b32c-007\",\n            \"name\": \"SleepSeconds\",\n            \"owner\": \"testuser2\",\n            \"appId\": \"SleepSeconds\",\n            \"created\": \"2022-07-05T21:43:05.371Z\",\n            \"status\": \"FINISHED\",\n            \"remoteStarted\": \"2022-07-05T21:43:44.899Z\",\n            \"ended\": \"2022-07-05T21:44:27.509Z\",\n            \"tenant\": \"dev\",\n            \"execSystemId\": \"tapisv3-exec2\",\n            \"archiveSystemId\": \"tapisv3-exec\",\n            \"appVersion\": \"0.0.1\",\n            \"lastUpdated\": \"2022-07-05T21:44:27.509Z\"\n        
\n    ],\n    \"status\": \"success\",\n    \"message\": \"JOBS_SEARCH_RESULT_LIST_RETRIEVED Jobs search list for the user testuser5 in the tenant dev retrieved.\",\n    \"version\": \"1.2.1\",\n    \"metadata\": {\n        \"recordCount\": 2,\n        \"recordLimit\": 100,\n        \"recordsSkipped\": 0,\n        \"orderBy\": null,\n        \"startAfter\": null,\n        \"totalCount\": -1\n    
\n    
"
{"filename": "jobs.md", "prompt": "Share Job Output", "completion": "As shown in the previous example of share a job, job output resource can\nbe shared with a user, say testuser5, in the same tenant. This includes\ntestuser5 can do job output listing and job output download with its own\nJWT even though its not the owner of the job."}
{"filename": "jobs.md", "prompt": "Share Job History", "completion": "As shown in the previous example of share a job, job history can be\nshared with a user, say testuser5, in the same tenant. This includes\ntestuser5 can get job\\'s history, status and job\\'s detail information\nfor jobs that are shared with it using its own JWT."}
{"filename": "meta.md", "prompt": "Why Meta V3", "completion": "Meta V2 functionality was built on MongoDB version 1.0 technology.\nAlthough limited, it brought the basic functions of a document store to\nAgave/Tapis platform. Some projects decided to use basic key/value\nstorage to add metadata to Jobs, Apps, Systems and Files. Other projects\nstored complex document data to associate Jobs and other entities to add\na richer information layer to their portals. The limited search\nfunctionality and imposed document structure created impediments to\nusing MongoDB as projects had envisioned. Meta V3 removes these\nimpediments.\n\nMeta V3 is built on top of MongoDB version 4.2 technology. The REST API\nopens up the full functionality of MongoDB as a document store. and\ndelivers MongoDB as a service so that projects are free to create\nmetadata and documents in a fashion that fits their needs. Some of the\nMeta V3 advantages over Meta V2 include:\n\n-   Any valid MongoDB document structure can be used.\n-   If a search runs in MongoDB CLI, it should run from the API.\n-   Aggregations are available.\n-   Database, collection and document creation can be managed by tenant\n    administrator.\n-   Performance is many times faster."}
{"filename": "meta.md", "prompt": "Migration from Meta V2 to V3", "completion": "Migration can simply be accomplished by creating a new database with one\nor more collections for your project. The tenant administrator can\nrequest the initial permissions setup. Once your collection(s) are in\nplace move the result and associatedIds into your new document model."}
{"filename": "meta.md", "prompt": "Overview", "completion": "Meta V3 is:\n\nA Stateless Microservice. With Meta V3 projects can focus on building\nAngular or other frontend applications, because most of the server-side\nlogic necessary to manage database operations, authentication /\nauthorization and related APIs is automatically handled, without the\nneed to write any server-side code except for the UX/UI.\n\nFor example, to insert data into MongoDB a developer has to just create\nclient-side JSON documents and then execute POST operations via HTTP to\nMeta V3. Other functions of a modern MongoDB installation like flexible\nschema, geoJson and aggregation pipelines ease the development process.\n\nEvery tenant will have access to at least one database where they can\nstore and manage json documents. Documents are the trailing end of a\nnested hierarchy of data that begins with a database that houses one or\nmore collections. The collections house json documents the structure of\nwhich is left up to the administrators of the tenant database.\n\nPermissions for access to databases, collections and documents must be\npredefined before accessing those resources. The definitions for access\nare defined within the Security Kernel API of Tapis V3 and must be added\nby a tenant or service administrator. See the Permissions section below\nfor some examples of permissions definitions and access to resources in\nthe Meta V3 API."}
{"filename": "meta.md", "prompt": "Getting Started", "completion": ""}
{"filename": "meta.md", "prompt": "Create a document", "completion": "We have a database named MyTstDB and a collection name MyCollection. To\nadd a json document to MyCollection, we can do the following:\n\nWith CURL:\n\n\n\nThe response will have an empty response body with a status code of 201\n\\\"Created\\\" unless the \\\"basic\\\" url query parameter is set to true.\nSetting the \\\"basic\\\" parameter to true will give a Tapis Basic response\nalong with the \\\"\\_id\\\" of the newly created document. A more detailed\ndiscussion of autogenerated ids and specified ids can be found in the\n\\\"Create Document\\\" section of \\\"Document Resources\\\".\n\n::: {.container .foldable}
\n\n:::"
{"filename": "meta.md", "prompt": "List documents", "completion": "Using our MyTstDb/MyCollection resources we can ask for a default list\nof documents in MongoDB default sorted order. The document we created\nearlier should be listed with a new \\\"\\_id\\\" field that was\nautogenerated by MongoDB.\n\nWith CURL:\n\n\n\nThe response will be an array of json documents from MyCollection :\n\n::: {.container .foldable}
\n\n:::"
{"filename": "meta.md", "prompt": "Get a document", "completion": "If we know the \\\"\\_id\\\" of a created document, we can ask for it\ndirectly.\n\nWith CURL:\n\n\n\nThe response will be a json document from MyCollection with the \\\"\\_id\\\"\nof 5f1892ece37f7b5a692285e9 :\n\n::: {.container .foldable}
\n\n:::"
{"filename": "meta.md", "prompt": "Find a document", "completion": "We can pass a query parameter named \\\"filter\\\" and set the value to a\njson MongoDB query document. Let\\'s find a document by a specific\n\\\"name\\\".\n\nWith CURL:\n\n\n\nThe response will be an array of json documents from MyCollection :\n\n::: {.container .foldable}
\n\n:::"
{"filename": "meta.md", "prompt": "Resources", "completion": ""}
{"filename": "meta.md", "prompt": "General resources", "completion": "An unauthenticated Health check is in included in the Meta V3 API to let\nany user know the current condition of the service.\n\n**Health Check**\n\nAn unauthenticated request for the health status of Meta V3 API.\n\nWith pySDK operation:\n\n\n\nWith CURL:\n\n\n\nThe response will be a Basic Tapis response on health:\n\n::: {.container .foldable}
\n\n:::"
{"filename": "meta.md", "prompt": "Root resources", "completion": "The Root resource space represents the root namespace for databases on\nthe MongoDB host. All databases are located here. Requests to this space\nare limited to READ only for tenant administrators.\n\n**List DB Names**\n\nA request to the Root resource will list Database names found on the\nserver. This request has been limited to those users with tenant\nadministrative roles.\n\nWith pySDK operation:\n\n\n\nWith CURL:\n\n\n\nThe response will a json list of database names:\n\n::: {.container .foldable}
\n\n:::"
{"filename": "meta.md", "prompt": "Database resources", "completion": "The Database resource is the top level for many tenant projects. The\nresource maps directly to a MongoDB named database in the database\nserver. Case matters for matching the name of the database and must be\nspecified when making requests for collections or documents. Currently\n\n**List Collection Names**\n\nThis request will return a list of collection names from the specified\ndatabase {db}
. The permissions for access to the database are set prior\nto access.\n\nWith pySDK operation:\n\n\n\nWith CURL:\n\n\n\nHere is an example response:\n\n::: {.container .foldable
\n\n:::\n\n**Get DB Metadata**\n\nThis request will return the metadata properties associated with the\ndatabase. The core server generates an etag in the \\_properties\ncollection for a database that is necessary for future deletion.\n\nWith pySDK operation:\n\n\n\nWith CURL:\n\n\n\nHere is an example response:\n\n::: {.container .foldable
\n\n:::\n\n**Create DB**\n\nTODO: this implementation is not exposed. Creation of a database by\ntenant administrators is scheduled for inclusion in an administrative\ninterface API in a future release.\n\nThis request will create a new named database in the MongoDB root space\nby a tenant or service administrator.\n\nWith pySDK operation:\n\n\n\nWith CURL:\n\n\n\nHere is an example response:\n\n::: {.container .foldable
\n\n\nStatus: 200 ok\n:::\n\n**Delete a DB** TODO: this implementation is not exposed. Deletion of a\ndatabase by tenant administrators is scheduled for inclusion in an\nadministrative interface API in a future release.\n\nThis request will delete a named database in the MongoDB root space by a\ntenant or service administrator.\n\nWith pySDK operation:\n\n\n\nWith CURL:\n\n\n\nHere is an example response:\n\n::: {.container .foldable
\n\n:::"
{"filename": "meta.md", "prompt": "Collection Resources", "completion": "The Collection resource allows requests for managing and querying json\ndocuments within a MongoDB collection.\n\n**Create a Collection**\n\nYou can create a new collection of documents by specifying a collection\nname under a specific database. /v3/meta/{db}
/{collection
\n\nWith pySDK operation:\n\n\n\nWith CURL:\n\n\n\nHere is an example response:\n\n::: {.container .foldable
\n\n\nStatus: 201 Collection created. An Etag header value is returned for\ncollection identification\n:::\n\n**List Documents**\n\nA default number of documents found in the collection are returned in an\narray of documents.\n\nWith pySDK operation:\n\n\n\nWith CURL:\n\n\n\nThe response will look like the following:\n\n::: {.container .foldable
\n\n:::\n\n**List Documents Large Query**\n\nA default number of documents found in the collection are returned in an\narray of documents.\n\nWith pySDK operation:\n\n\n\nWith CURL:\n\n\n\nThe response will look like the following:\n\n::: {.container .foldable
\n\n:::\n\n**Delete a Collection**\n\nThis administrative method is only available to tenant or meta\nadministrators and requires an If-Match header parameter of the Etag for\nthe collection. The Etag value, if not already known, can be retrieved\nfrom the \\\"\\_meta\\\" call for a collection.\n\nWith pySDK operation:\n\n\n\nWith CURL:\n\n\n\nHere is an example response:\n\n::: {.container .foldable
\n\n\nStatus: 204 Deleted named collection from the database\n:::\n\n**Get Collection Size**\n\nYou can find the given size or number of documents in a given collection\nby calling \\\"\\_size\\\" on a collection.\n\nWith pySDK operation:\n\n\n\nWith CURL:\n\n\n\nHere is an example response:\n\n::: {.container .foldable
\n\n\nStatus: 200 ok\n\n{\n\n:   \\\"\\_size\\\": 1 
\n:::\n\n**Get Collection Metadata**\n\nYou can find the metadata properties of a given collection by calling\n\\\"\\_meta\\\" on a collection. This would include the Etag value for a\ncollection that is needed for deletion.\n\nWith pySDK operation:\n\n\n\nWith CURL:\n\n\n\nHere is an example response:\n\n::: {.container .foldable
\n\n:::"
{"filename": "meta.md", "prompt": "Document Resources", "completion": "Document resources are json documents found in a collection. Reading,\ncreating, deleting and updating documents along with batch processing\nmake up the operations that can be applied to documents in a collection.\nThere various ways to retrieve one or more documents from a collection,\nincluding using a filter query parameter and value in the form of a\nMongoDB query document. Batch addition of documents, as well as, batch\nupdates based on queries is also allowed.\n\n**Create a Document**\n\nCreating a new document within a collection. Submitting a json document\nwithin the request body of a POST request will create a new document\nwithin the specified collection with a MongoDB autogenerated \\\"\\_id\\\".\nBatch document addition is possible by POSTing an array of new documents\nwith a request body for the specified collection. The rules for \\\"\\_id\\\"\ncreation operates the same way on multiple documents as they do with a\nsingle document.\n\nThe default representation returned is an empty response body along with\na 201 Http status code \\\"Created\\\". However if an additional query\nparameter named \\\"basic\\\" is added with the value of \\\"true\\\" a basic\nTapis response is returned along with the newly created \\\"\\_id\\\" of the\ndocument.\n\nWith pySDK operation:\n\n\n\nWith CURL:\n\n\n\nHere is an example response:\n\n::: {.container .foldable}
\n\n\nStatus: 201 Document created\n:::\n\nMultiple documents can be added to a collection by POSTing a json array\nof documents. The batch addition of documents only supports the default\nresponse.\n\nWith pySDK operation:\n\n\n\nWith CURL:\n\n\n\nThe response body will be empty:\n\n::: {.container .foldable
\n\n\nStatus: 201 Document created\n:::\n\n**Get a Document**\n\nGet a specific document by its \\\"\\_id\\\".\n\nWith pySDK operation:\n\n\n\nWith CURL:\n\n\n\nThe response will be the standard json response:\n\n::: {.container .foldable
\n\n\nStatus: 200 Document returned\n\n:   \n\n    {\n\n    :   \\\"\\_id\\\"
\n:::\n\n**Replace a Document**\n\nThis call replaces an existing document identified by document id\n(\\\"\\_id\\\"), with the json supplied in the request body.\n\nWith pySDK operation:\n\n\n\nWith CURL:\n\n\n\nHere is an example response:\n\n::: {.container .foldable
\n\n:::\n\n**Modify a Document**\n\nThis call will replace a portion of a document identified by document id\n(\\\"\\_id\\\") with the supplied json.\n\nWith pySDK operation:\n\n\n\nWith CURL:\n\n\n\nHere is an example response:\n\n::: {.container .foldable
\n\n:::\n\n**Delete Document**\n\nDeleting a document with a specific document id (\\\"\\_id\\\"), removes it\nfrom the collection.\n\nWith pySDK operation:\n\n\n\nWith CURL:\n\n\n\nHere is an example response:\n\n::: {.container .foldable
\n\n:::"
{"filename": "meta.md", "prompt": "Index Resources", "completion": "Indexes can help speed up queries of your collection and the API gives\nyou the ability to define and manage your indexes. You can create an\nindex for a collection, list indexes for a collection and delete an\nindex. Indexes can\\'t be updated they must be deleted and recreated.\n\n**List Indexes**\n\nList the indexes defined for a collection.\n\nWith pySDK operation:\n\n\n\nWith CURL:\n\n\n\nHere is an example response:\n\n::: {.container .foldable}
\n\n\n\\]\n:::\n\n**Create Index**\n\nCreate a new Index with a new name. To create an index you have to\nspecify the keys and the index options. Let\\'s create an unique, sparse\nindex on property qty and name our index \\\"qtyIndex\\\".\n\nPUT /v3/meta/{db
/{collection
/\\_indexes/qtyIndex\n\n\n\nWith pySDK operation:\n\n\n\nWith CURL:\n\n\n\nHere is an example response:\n\n::: {.container .foldable
\n\n\nStatus: 201 Empty Response\n:::\n\n**Delete Index**\n\nRemove a named Index from the index list.\n\nWith pySDK operation:\n\n\n\nWith CURL:\n\n\n\nHere is an example response:\n\n::: {.container .foldable
\n\n\nStatus: 204 Deleted index from collection\n:::"
{"filename": "meta.md", "prompt": "Aggregation Resources", "completion": "Aggregations operations process data records and return computed\nresults. Aggregation operations group values from multiple documents\ntogether, and can perform a variety of operations on the grouped data to\nreturn a single result. Aggregations in the API are predefined and added\nto a collections properties. They may also be parameterized for use with\nmultiple sets of inputs.\n\n**Create an Aggregation**\n\nCreate an aggregation pipeline by adding the aggregation to the\ncollection for future execution. The aggregation may have variables that\nare defined so that a future request may pass variable values for\naggregation execution. See \\\"Execute an Aggregation\\\".\n\n\n\n+------+--------+-----------------------------------------------------+\n| Prop | Man    | Description                                         |\n| erty | datory |                                                     |\n+======+========+=====================================================+\n| type | yes    | -   for aggregation pipeline operations is          |\n|      |        |     \\\"pipeline\\\"                                    |\n+------+--------+-----------------------------------------------------+\n| uri  | yes    | -   specifies the URI when the operation is bound   |\n|      |        |     under the path /\\<db\\>/\\<collection\\>/\\_aggrs.  |\n+------+--------+-----------------------------------------------------+\n| st   | yes    | -   the MongoDB aggregation pipeline stages.        |\n| ages |        |                                                     |\n+------+--------+-----------------------------------------------------+\n\nFor more information refer to\n<https://docs.mongodb.org/manual/core/aggregation-pipeline/>\n\nWith pySDK operation:\n\n\n\nWith CURL:\n\n\n\nHere is an example response:\n\n::: {.container .foldable}
\n\n:::\n\n**Execute an Aggregation**\n\nTODO\n\nWith pySDK operation:\n\n\n\nWith CURL:\n\n\n\nHere is an example response:\n\n::: {.container .foldable
\n\n:::\n\n**Delete an Aggregation**\n\nTODO\n\nWith pySDK operation:\n\n\n\nWith CURL:\n\n\n\nHere is an example response:\n\n::: {.container .foldable
\n\n:::"
{"filename": "notifications.md", "prompt": "Overview", "completion": "In Tapis, a notification *event* represents an occurrence that may be of\ninterest to other parties. The events are delivered asynchronously using\na publish-subscribe model. Once an interested party has created a\n*subscription*, matching *events* are delivered to the interested party\nas part of a *notification* object. Deliveries are made via webhook or\nemail.\n\nCurrently only services may directly create subscriptions and post\nevents.\n\nThe model for a notification event is based on the CloudEvent\nspecification version 1.0. For more information about CloudEvents and\nthe specification, please see <https://cloudevents.io> and\n<https://github.com/cloudevents/spec>.\n\nPlease note that although currently the Notifications service is only\naccessed by users through the Jobs service, this document discusses the\ngeneral design and details of the service in order to provide\ninformation for future planned development."}
{"filename": "notifications.md", "prompt": "Event Model", "completion": "An event contains the following information:\n\n*source*\n\n:   Context in which the event happened. For example, for a job related\n    event originating from Tapis at the primary TACC site, the source\n    would be <https://tapis.io/v3/jobs>.\n\n*type*\n\n:   Type of event. Used for routing notifications. A series of 3 fields\n    separated by the dot character. The first field is the service name,\n    the second field is the category and the third field is the detail.\n    For example, when a job transistions to the FINISHED state the type\n    is `jobs.JOB_NEW_STATUS.FINISHED`.\n\n*subject*\n\n:   Subject of event in context of service. Examples: job Id, system Id,\n    file path, role name, etc.\n\n*seriesId*\n\n:   Optional Id that groups events from the same source in a series,\n    thereby preserving event order during notification delivery.\n\n*timestamp*\n\n:   When the event happened.\n\n*data*\n\n:   Optional additional information associated with the event. For\n    example, data specific to the service associated with the event.\n\nNote that events are not persisted by the front end api service process.\nWhen events are received they are sent to a message broker for\nasynchronous processing. The back end process will persist events\ntemporarily in order to support recovery."}
{"filename": "notifications.md", "prompt": "Event Type", "completion": "An event type represents a channel through which users and services\nreceive events. Services and users create subscriptions with an event\ntype filter in order to select the events delivered to them. The\nidentifier for each event type must have three parts in the format\n**\\<service\\>.\\<category\\>.\\<detail\\>**. For example\n*jobs.JOB_NEW_STATUS.PENDING*, *apps.APP.UPDATE* or\n*files.OBJECT.DELETE*."}
{"filename": "notifications.md", "prompt": "Subscription", "completion": "A service can create a subscription for an event type in order to allow\nusers and services to receive events. Two delivery methods are\nsupported, WEBHOOK and EMAIL.\n\nAt a high level, a subscription contains the following information:\n\n*name*\n\n:   Optional short descriptive name. *owner+name* must be unique.\n    Composed of alphanumeric characters and the following special\n    characters: `-._~`. If not provided, the service will create one.\n\n*owner*\n\n:   A specific user set at create time. Default is *\\${apiUserId}
*.\n\n*description*\n\n:   An optional more verbose description.\n\n*typeFilter*\n\n:   Filter to use when matching events. Matches against event type. Has\n    three parts for matching *\\<service\\>.\\<category\\>.\\<detail\\>*. Each\n    field may be a specific type or the wildcard character `*`.\n\n*subjectFilter*\n\n:   Filter to use when matching events. Matches against event subject.\n    This may be a specific subject such as a job Id or the wildcard\n    character, `*`.\n\n*deliveryTargets*\n\n:   List of targets to be used when delivering an event. Each target\n    includes delivery method (EMAIL or WEBHOOK) and delivery address.\n\n*ttlMinutes*\n\n:   Time to live in minutes. Specified when subscription created.\n    Default is one week from creation. A TTL of 0 or less indicates no\n    expiration. May be updated through an API call. Attribute *expiry*\n    is recomputed when this attribute is updated.\n\n*expiry*\n\n:   Time at which the subscription expires. Maintained by the service.\n    Computed at create time and recomputed when attribute *ttlMinutes*\n    is updated.\n\nThe attributes *typeFilter*, *subjectFilter* and *deliveryTargets* are\nrequired."
{"filename": "notifications.md", "prompt": "Subscription Name", "completion": "For each owner the name must be unique and can be composed of\nalphanumeric characters and the following special characters: `-._~`. If\nthe attribute *name* is not provided, then the service will generate one\nusing the template:\n\n    <jwtUser>~<owner>~<oboTenant>~<subjectFilter>~<random4>\n\nFor example:\n\n    jobs~testuser1~dev~jobs.JOB_NEW_STATUS.ALL~m4Kx\n\nNote that when constructing the name:\n\n-   *subjectFilter* will be truncated to 40 characters\n-   If *subjectFilter* is the wildcard character `*`, it is replaced\n    with the string `ALL` when constructing the name.\n-   The last 4 characters are generated at random from the set of\n    alphanumeric characters including upper case, lower case and digits."}
{"filename": "notifications.md", "prompt": "Delivery Target", "completion": "Each subscription will contain a list of delivery targets for use in\ndelivering events. The list must contain at least one item. WEBHOOK and\nEMAIL deliveries are supported.\n\nA delivery target contains the following information:\n\n-   *deliveryMethod* - The type of delivery method: WEBHOOK, EMAIL\n-   *deliveryAddress* - URL for WEBHOOK or email address for EMAIL"}
{"filename": "notifications.md", "prompt": "Notification Model", "completion": "A notification is an object encapsulating the information sent to a\ndelivery target. It contains the following:\n\n-   *uuid* - Unique identifier for the notification.\n-   *event* - All information contained in the event. See above under\n    the section [Event Model](#event-model).\n-   *eventUuid* - Unique identifier for the event.\n-   *tenant* - tenant associated with the notification.\n-   *subscriptionName* - Name of subscription associated with the\n    notification.\n-   *deliveryTarget* - the delivery target\n-   *created* Timestamp for when the notification was created.\n\nExample of a notification sent to a webhook:\n\n    {\n      \"uuid\": \"30d70395-d5e9-43a4-ae90-2306b6bb00d6\",\n      \"tenant\": \"admin\",\n      \"subscriptionName\": \"4d0abbce-5cec-4d6e-8065-cdc5b2777389\",\n      \"eventUuid\": \"50cfb971-c4b3-4d33-89c3-2b0f56f16e19\",\n      \"event\": {\n        \"source\": \"notifications\",\n        \"type\": \"notifications.test.begin\",\n        \"subject\": \"4d0abbce-5cec-4d6e-8065-cdc5b2777389\",\n        \"data\": null,\n        \"seriesId\": null,\n        \"timestamp\": \"2023-09-15T14:47:50.287792699Z\",\n        \"deleteSubscriptionsMatchingSubject\": false,\n        \"tenant\": \"admin\",\n        \"user\": \"notifications\",\n        \"uuid\": \"50cfb971-c4b3-4d33-89c3-2b0f56f16e19\"\n      }
,\n      \"deliveryTarget\": {\n        \"deliveryMethod\": \"WEBHOOK\",\n        \"deliveryAddress\": \"https://admin.develop.tapis.io/v3/notifications/test/callback/4d0abbce-5cec-4d6e-8065-cdc5b2777389/\"\n      
,\n      \"created\": \"2023-09-15T14:47:50.315188203Z\"\n    
\n\nExample of a notification sent to an email address:\n\n    {\n      \"uuid\": \"befe2475-58ad-4a5c-bcf2-593f04e49a20\",\n      \"tenant\": \"dev\",\n      \"subscriptionName\": \"jobs~testuser2~dev~ef9004c3-09d5-41d5-acd3-be7c9fd3daf6-007~cxh2\",\n      \"eventUuid\": \"1d16202d-2248-4690-bcc9-a0134a4089cd\",\n      \"event\": {\n        \"source\": \"https://tapis.io/jobs\",\n        \"type\": \"jobs.JOB_NEW_STATUS.FINISHED\",\n        \"subject\": \"ef9004c3-09d5-41d5-acd3-be7c9fd3daf6-007\",\n        \"data\": \"{\\\"newJobStatus\\\":\\\"FINISHED\\\",\\\"oldJobStatus\\\":\\\"ARCHIVING\\\",\\\"blockedCount\\\":0,\\\"remoteJobId\\\":\\\"35299a7d78f1591e395fdcec9dc6b1f3606be9f56f38453129b6ccc383ed9759\\\",\\\"remoteJobId2\\\":null,\\\"remoteOutcome\\\":\\\"FINISHED\\\",\\\"remoteResultInfo\\\":\\\"0\\\",\\\"remoteQueue\\\":null,\\\"remoteSubmitted\\\":\\\"2023-09-15T15:11:18.354731067Z\\\",\\\"remoteStarted\\\":null,\\\"remoteEnded\\\":null,\\\"jobName\\\":\\\"Tapis V3 smoketest job\\\",\\\"jobUuid\\\":\\\"ef9004c3-09d5-41d5-acd3-be7c9fd3daf6-007\\\",\\\"jobOwner\\\":\\\"testuser2\\\",\\\"message\\\":\\\"The job has transitioned to a new status: FINISHED. The previous job status was ARCHIVING.\\\"
\",\n        \"seriesId\": \"ef9004c3-09d5-41d5-acd3-be7c9fd3daf6-007\",\n        \"timestamp\": \"2023-09-15T15:11:23.947827477Z\",\n        \"deleteSubscriptionsMatchingSubject\": true,\n        \"tenant\": \"dev\",\n        \"user\": \"jobs\",\n        \"uuid\": \"1d16202d-2248-4690-bcc9-a0134a4089cd\"\n      
,\n      \"deliveryTarget\": {\n        \"deliveryMethod\": \"EMAIL\",\n        \"deliveryAddress\": \"me@example.com\"\n      
,\n      \"created\": \"2023-09-15T15:11:23.965413696Z\"\n    
"
{"filename": "notifications.md", "prompt": "Notification Delivery", "completion": ""}
{"filename": "notifications.md", "prompt": "Background", "completion": "When events are published to the Notifications front end api service,\nthey are initially placed on a message broker queue to be picked up\nasynchronously by a back end worker process known as the dispatch\nservice. Currently RabbitMQ is used as the message broker.\n\nThe dispatch service reads events from the queue and assigns them to\nworkers known as *delivery bucket managers*. Delivery bucket managers\nare threads that receive their assigned events from in-memory queues.\nThe dispatch service assigns events to a bucket manager by taking a hash\nof the event *source*, *subject* and *seriesId*.\n\nWhen a bucket manager worker receives an event to process, it first\nfinds all matching subscriptions by querying a database. As discussed\nabove, the matching is based on the *typeFilter* and *subjectFilter*\ndefined in a subscription.\n\nFor each delivery target in each matching subscription, the worker\ncreates a Notification object and persists it to a database. By\npersisting to a database we are able to support recovery and retries.\nThe worker then begins the process of delivering the notifications."}
{"filename": "notifications.md", "prompt": "Configuring for EMAIL Delivery", "completion": "Supporting delivery by EMAIL involves configuring the Tapis\nNotifications service to use an SMTP relay. This must be done by the\nTapis systems administrator. Parameters for the relay are set as\nenvironment variables to be picked up by the dispatcher service when it\nis started during a deployment. For more information on deployer\nconfiguration please see\n[Notifications_Email_Config](https://tapis.readthedocs.io/en/latest/deployment/deployer.html#configuring-support-for-email-notifications).\n\nPlease note that deployer currently only supports template variables for\nTAPIS_MAIL_PROVIDER, TAPIS_SMTP_HOST and TAPIS_SMTP_PORT. Other\nenvironment variables must be set manually in the deployment.\n\nThe environment variables used to configure email delivery are:\n\n*TAPIS_MAIL_PROVIDER*\n\n:   Optional. Supported values: SMTP, LOG, NONE. Default is LOG. This\n    should typically be set to SMTP. Setting to LOG results in the\n    dispatcher generating a log message showing the email information.\n    Setting to NONE results in delivery being a NO-OP.\n\n*TAPIS_SMTP_HOST*\n\n:   Required if provider is SMTP. Host to use as relay when sending\n    email via SMTP.\n\n*TAPIS_SMTP_PORT*\n\n:   Optional. Port used when sending email using SMTP. Default is 25.\n\n*TAPIS_SMTP_FROM_NAME*\n\n:   Optional. Name for the email [From:]{.title-ref}
 field. Default\n    value is *Tapis Notifications Service*.\n\n*TAPIS_SMTP_FROM_ADDRESS*\n\n:   Optional. Address for the email [From:]{.title-ref
 field. Default\n    value is *no-reply@nowhere.com*.\n\n*TAPIS_SMTP_AUTH*\n\n:   Optional. Boolean indicating if SMTP server requires a username and\n    password. Default is *false*.\n\n*TAPIS_SMTP_USER*\n\n:   Required if TAPIS_SMTP_AUTH is *true*.\n\n*TAPIS_SMTP_PASSWORD*\n\n:   Required if TAPIS_SMTP_AUTH is *true*."
{"filename": "notifications.md", "prompt": "EMAIL Delivery", "completion": "When the notification delivery method is of type EMAIL, the dispatch\nworker will send an email using SMTP.\n\nThe `To:` field for the email will be the notification delivery address.\n\nThe `From:` field for the email will depend on the configuration\nparameters, as discussed above in the section [Configuring for EMAIL\nDelivery](#configuring-for-email-delivery). By default this will be:\n\n    Tapis Notifications Service <no-reply@nowhere.com>\n\nThe `Subject:` of the email will have the following format:\n\n    Tapis v3 notification. Event type: <event_type> subject: <subject>\n\nIf the event has no *subject* then the email subject will not have the\nsubject portion.\n\nAn example email subject for the case where the event contains a\n*subject* attribute:\n\n    Tapis v3 notification. Event type: jobs.JOB_NEW_STATUS.FINISHED subject: 1451b0ef-c057-4177-acd5-51a4901acb07-007\n\nThe body of the email will contain the notification data as json. An\nexample may be found above under the section [Notification\nModel](#notification-model)."}
{"filename": "notifications.md", "prompt": "WEBHOOK Delivery", "completion": "When the notification delivery method is of type WEBHOOK, the dispatch\nworker will deliver the notification using an HTTP POST request. The\nmedia type for the request will be *application/json* and the following\nheader will be added: `User-Agent: Tapis/v3`.\n\nThe request body will be a json structure with the notification\ninformation. An example may be found above under the section\n[Notification Model](#notification-model)."}
{"filename": "notifications.md", "prompt": "Tables", "completion": ""}
{"filename": "notifications.md", "prompt": "Subscription Attributes", "completion": "+--------+--------+----------+---------------------------------------+\n| Att    | Type   | Example  | Notes                                 |\n| ribute |        |          |                                       |\n+========+========+==========+=======================================+\n| tenant | String | de       | -   Name of the tenant associated     |\n|        |        | signsafe |     with the subscription.            |\n|        |        |          | -   *tenant* + *owner* + *name* must  |\n|        |        |          |     be unique.                        |\n|        |        |          | -   Determined by the service at      |\n|        |        |          |     creation time.                    |\n+--------+--------+----------+---------------------------------------+\n| name   | String | my-ema   | -   Optional short descriptive name.  |\n|        |        | il-ntf-1 | -   *tenant* + *owner* + *name* must  |\n|        |        |          |     be unique.                        |\n|        |        |          | -   Allowed characters: Alphanumeric  |\n|        |        |          |     \\[0-9a-zA-Z\\] and `-._~`.         |\n|        |        |          | -   If not provided the service will  |\n|        |        |          |     create one.                       |\n+--------+--------+----------+---------------------------------------+\n| owner  | String | jdoe     | -   username of *owner*.              |\n|        |        |          | -   Variable references:              |\n|        |        |          |     *\\${apiUserId}
*. Resolved at      |\n|        |        |          |     create time.                      |\n|        |        |          | -   By default this is the resolved   |\n|        |        |          |     value for *\\${apiUserId
*.        |\n+--------+--------+----------+---------------------------------------+\n| descr  | String | My email | -   Optional more verbose             |\n| iption |        |          |     description. Maximum length of    |\n|        |        |          |     2048 characters.                  |\n+--------+--------+----------+---------------------------------------+\n| e      | b      | FALSE    | -   Indicates if subscription is      |\n| nabled | oolean |          |     active.                           |\n|        |        |          | -   May be updated using the          |\n|        |        |          |     enable/disable endpoints.         |\n|        |        |          | -   By default this is *true*.        |\n+--------+--------+----------+---------------------------------------+\n| type   | String | apps.AP  | -   Filter to use when matching       |\n| Filter |        | P.DELETE |     events.                           |\n|        |        |          | -   Matches against event type.       |\n|        |        |          | -   Has three dot separated parts:    |\n|        |        |          |     *                                 |\n|        |        |          | \\<service\\>.\\<category\\>.\\<detail\\>*. |\n|        |        |          | -   Each part may be a specific type  |\n|        |        |          |     or the wildcard character \\*.     |\n+--------+--------+----------+---------------------------------------+\n| s      | String | \\<       | -   Filter to use when matching       |\n| ubject |        | job-id\\> |     events.                           |\n| Filter |        |          | -   Matches against event subject.    |\n|        |        |          | -   Can be specific for an exact      |\n|        |        |          |     match or the wildcard character   |\n|        |        |          |     \\*.                               |\n+--------+--------+----------+---------------------------------------+\n| del    |        |          | -   List of delivery targets to be    |\n| iveryT |        |          |     used when delivering a matching   |\n| argets |        |          |     event.                            |\n|        |        |          | -   Must have at least one.           |\n|        |        |          | -   Each target includes delivery     |\n|        |        |          |     method and delivery address.      |\n|        |        |          | -   Delivery methods supported:       |\n|        |        |          |     WEBHOOK, EMAIL                    |\n+--------+--------+----------+---------------------------------------+\n| ttlM   | int    | 60       | -   Time to live in minutes. Can be   |\n| inutes |        |          |     updated.                          |\n|        |        |          | -   Service will compute expiry based |\n|        |        |          |     on this attribute.                |\n|        |        |          | -   Default is one week from          |\n|        |        |          |     creation.                         |\n|        |        |          | -   Value of 0 indicates no           |\n|        |        |          |     expiration.                       |\n+--------+--------+----------+---------------------------------------+\n| expiry | Tim    | 2020     | -   Time at which the subscription    |\n|        | estamp | -06-26T1 |     expires and will be deleted.      |\n|        |        | 5:10:43Z | -   Maintained by the service.        |\n|        |        |          | -   Computed at create time.          |\n|        |        |          | -   Recomputed when attribute         |\n|        |        |          |     *ttlMinutes* is updated.          |\n+--------+--------+----------+---------------------------------------+\n| uuid   | UUID   |          | -   Auto-generated by service.        |\n+--------+--------+----------+---------------------------------------+\n| c      | Tim    | 2020     | -   When the subscription was         |\n| reated | estamp | -06-19T1 |     created. Maintained by service.   |\n|        |        | 5:10:43Z |                                       |\n+--------+--------+----------+---------------------------------------+\n| u      | Tim    | 2020     | -   When the subscription was last    |\n| pdated | estamp | -06-20T2 |     updated. Maintained by service.   |\n|        |        | 3:21:22Z |                                       |\n+--------+--------+----------+---------------------------------------+"
{"filename": "notifications.md", "prompt": "Event Attributes", "completion": "+------+----+--------------+-----------------------------------------+\n| A    | Ty | Example      | Notes                                   |\n| ttri | pe |              |                                         |\n| bute |    |              |                                         |\n+======+====+==============+=========================================+\n| so   | St | <h           | -   Context in which event happened.    |\n| urce | ri | ttps://tapis |                                         |\n|      | ng | .io/v3/jobs> |                                         |\n+------+----+--------------+-----------------------------------------+\n| type | St | app          | -   Type of event. Used for routing     |\n|      | ri | s.APP.DELETE |     notifications.                      |\n|      | ng |              | -   Pattern is                          |\n|      |    |              |     [\\<service                          |\n|      |    |              | \\>.\\<category\\>.\\<detail\\>]{.title-ref}
 |\n+------+----+--------------+-----------------------------------------+\n| sub  | St | > \\<job-id\\> | -   Subject of event in the context of  |\n| ject | ri |              |     the service.                        |\n|      | ng |              | -   Examples: job Id, app Id, file      |\n|      |    |              |     path, role name, etc.               |\n+------+----+--------------+-----------------------------------------+\n| data | St |              | -   Optional additional information     |\n|      | ri |              |     associated with the event.          |\n|      | ng |              | -   Data specific to the service        |\n|      |    |              |     associated with the event.          |\n+------+----+--------------+-----------------------------------------+\n| seri | St |              | -   Optional Id that groups events from |\n| esId | ri |              |     the same source in a series.        |\n|      | ng |              | -   Preserves event order during        |\n|      |    |              |     notification delivery.              |\n+------+----+--------------+-----------------------------------------+\n| t    | St | 2020-06-     | -   When the event happened.            |\n| imes | ri | 19T15:10:43Z |                                         |\n| tamp | ng |              |                                         |\n+------+----+--------------+-----------------------------------------+"
{"filename": "notifications.md", "prompt": "Notification Attributes", "completion": "------------------------------------------------------------------------------\n  Attribute          Type     Notes\n  ------------------ -------- --------------------------------------------------\n  uuid               String   Unique identifier for the notification.\n\n  tenant             String   Tenant associated with the notification.\n\n  subscriptionName   String   Name of subscription associated with the\n                              notification.\n\n  eventUuid          String   Unique identifier for the event contained in the\n                              notification.\n\n  event              Event    Event that triggered the notification.\n\n  deliveryTarget     String   The delivery target for the notification.\n\n  created            String   When the notification was created.\n  ------------------------------------------------------------------------------"}
{"filename": "pgrest.md", "prompt": "Overview", "completion": "There are two primary collections in the PgREST API. The Management API,\nprovided at the URL `/v3/pgrest/manage`, includes endpoints for managing\nthe collection of tables, views, stored procedures, and other objects\ndefined in the hosted Postgres database server. Each Tapis tenant has\ntheir own schema within PgREST\\'s managed Postrgres database in which\nthe tables and other objects are housed. When a table is created,\nendpoints are generated that allow users to interact with the data\nwithin a table. These endpoints comprise the Data API, available at the\nURL `/v3/pgrest/data`. Each collection, `/v3/pgrest/data/{collection}
`,\nwithin the Data API corresponds to a table defined in the Management\nAPI. The Data API is used to create, update, and read the rows within\nthe corresponding tables."
{"filename": "pgrest.md", "prompt": "Authentication and Tooling", "completion": "PgREST currently recognizes Tapis v2 and v3 authentication tokens and\nuses these for determining access levels. A valid Tapis v2 OAuth token\nshould be passec to all requests to PgREST using the header\n`Tapis-v2-token`. For example, using curl:\n\n\n\nTapis v3 OAuth authentication tokens should be passed to all requests to\nPgREST using the header `X-Tapis-Token`. For example, using curl:\n\n\n\nAdditionally, PgREST should be accessible from the Tapis v3 Python SDK\n(tapipy) now with the addition of v3 authentication."}
{"filename": "pgrest.md", "prompt": "Permissions and Roles", "completion": "PgREST currently implements a handful of basic, role-based permissions\nthat leverage the Tapis v3 Security Kernel (SK)."}
{"filename": "pgrest.md", "prompt": "Universal Roles", "completion": "For now PgREST establishes the following five universal roles:\n\n-   `PGREST_ADMIN` \\-- Grants user read and write access to all objects\n    (e.g. tables, views, roles) in the `/manage` API as well as read and\n    write access to all associated data in the `/data` API.\n-   `PGREST_ROLE_ADMIN` \\-- Grants user role creation and management\n    access to roles in the `/manage/roles` API.\n-   `PGREST_WRITE` \\-- Grants user read and write access to all\n    associated data in the `/data` API.\n-   `PGREST_READ` \\-- Grants user read access to all associated data in\n    the `/data` API.\n-   `PGREST_USER` \\-- Grants permission to user `/views` API. Each view\n    has additional permission rules though.\n\nWithout any of the above roles, a user will not have access to any\nPgREST endpoints."}
{"filename": "pgrest.md", "prompt": "Fine-Tuned Role Access", "completion": "Along with the general access to endpoints when a user has a role of\n`PGREST_READ` or above, we have fine-tuned role access to our get\n[views]{.title-ref}
 endpoint.\n\nOur get [views]{.title-ref
 endpoint requires only the `PGREST_USER`\nrole. However each view itself when created (or modified) has a\n[permission_rules]{.title-ref
 field. This field is a list of roles that\nthe user must have in order to have access to that view. Thus it is\npossible to divy out what information a user in the `PGREST_USER` role\ncan get from views by restricting views with roles created by the\n`/manage/roles` endpoint as a `PGREST_ADMIN` or `PGREST_ROLE_ADMIN`."
{"filename": "pgrest.md", "prompt": "Tenant Awareness", "completion": "Note that these roles are granted at the *tenant* level, so a user may\nbe authorized at one level in one tenant and at a different level (or\nnot at all) in another tenant. In PgREST, the base URLs for a given\ntenant follow the pattern `<tenant_id>.tapis.io`, just as they do for\nall other Tapis v3 services. Hence, this request:\n\n\n\nwould list tables in the TACC tenant, while\n\n\n\nwould list tables in the CII tenant."}
{"filename": "pgrest.md", "prompt": "Table Manage API", "completion": "Table management is accomplished with the `/v3/pgrest/manage/tables`\nendpoint. Creating a table amounts to specifying the table name, the\ncolumns on the table, including the type of each column, and any\nadditional validation to be performed when storing data in the column,\nthe root URL where the associated collection will be available within\nthe Data API, and, optionally, which HTTP verbs should not be available\non the collection."}
{"filename": "pgrest.md", "prompt": "Table Creation Example", "completion": "For example, suppose we wanted to manage a table of \\\"widgets\\\" with\nfour columns. We could create a table by POSTing the following JSON\ndocument to the `/v3/pgrest/manage/tables` endpoint:\n\n\n\nThe JSON describes a table with 4 columns, `name`, `widget_type`,\n`count`, and `is_private`. The fields within the JSON object describing\neach column include its type, defined in the `data_type` attribute (and\nsupporting fields such as `char_len` for `varchar` columns), as well as\noptional constraints, such as the NOT NULL and UNIQUE constraint, an\noptional `default` value, and an optional `primary_key` value. Only the\n`data_type` attribute is required.\n\nTo create this table and the corresponding `/data` API, we can use curl\nlike so:\n\n\n\nIf all works, the response should look something like this:\n\n\n\nSince the `root_url` attribute has value `widgets`, an associated\ncollection at URL `/v3/pgrest/data/widgets` is automatically made\navailable for managing and retrieving the data (rows) on the table. See\nthe `Data API <target data api>`{.interpreted-text role=\"ref\"}
 section\nbelow for more details."
{"filename": "pgrest.md", "prompt": "Table Definition Rules", "completion": "This is a complete list of constraints and properties a table can have\nin it\\'s table definition. Each table definition has a host of fields,\nwith the column field have a host of options to delegate how to create\nthe postgres column.\n\n-   `table_name` - **required**\n    -   The name of the table in question.\n-   `root_url`\n    -   The root_url for PgRESTs /data endpoint.\n    -   Ex: root_url \\\"table25\\\" would be accessible via\n        \\\"<http://pgrestURL/data/table25>\\\".\n-   `enums`\n    -   Enum generation is done in table definitions.\n    -   Provide a dict of enums where the key is enum name and the value\n        is the possible values for the enum.\n    -   Ex: `{\"accountrole\": [\"ADMIN\", \"USER\"]}
`\n        -   Creates an \\\"accountrole\\\" enum that can have values of\n            \\\"ADMIN\\\" or \\\"USER\\\"\n    -   Deletion/Updates are not currently supported. Speak to developer\n        if you\\'re interested in a delete/update endpoint.\n-   `comments`\n    -   Field to allow for better readability of table json. Table\n        comments are saved and outputted on /manage/tables/ endpoints.\n-   `constraints`\n    -   Specification of Postgres table constraints. Currently only\n        allows multi-column unique constraints\n    -   Constraints available:\n        -   `unique`\n            -   multi-column unique constraint that requires sets of\n                column values to be unique.\n            -   Note: Unique constraint must have unique naming between\n                tables. Constraints cannot be used more than once.\n            -   Ex:\n                `\"constraints\": {\"unique\": {\"two_col_pair\": [\"col_one\", \"col_two\"]
`\n                -   This means that col_one and col_two cannot have\n                    pairs of values that are identical.\n                -   The constraint name can be specified as well\n-   `columns` - **required**\n    -   Column definitions in the form of a dict. Dict key would be\n        column, value would be column definition.\n    -   Ex:\n        `{\"username\": {\"unique\": true, \"data_type\": \"varchar\", \"char_len\": 255
`\n    -   Columns arguments are as follows.\n        -   `data_type` - **required**\n            -   Specifies the data type for values in this column.\n            -   Case insensitive.\n            -   Can be varchar, datetime, {enumName
, text, timestamp,\n                serial, varchar\\[\\], boolean, integer, integer\\[\\].\n                -   Note: varchar requires the char_len column\n                    definition.\n                -   Note: Setting a timestamp data_type column default\n                    to `UPDATETIME` or `CREATETIME` has special\n                    properties.\n                    -   `CREATETIME` sets the field to the UTC time at\n                        creation. It is then not changed later.\n                    -   `UPDATETIME` sets the filed to the UTC time at\n                        creation. It is updated to the update time when\n                        it is updated.\n            -   Serial Data type\n                -   Not actually a serial data type. On the postgres\n                    side this is\n                    `INTEGER GENERATED BY DEFAULT AS IDENTITY (START WITH {serial_start
 INCREMENT BY {serial_increment
)`\n                -   This is to get around the fact that the serial data\n                    type isn\\'t that well received by Postgres\n                -   This also means that users can specify\n                    `serial_start` and `serial_increment` to adjust how\n                    it works.\n        -   `char_len`\n            -   Additional argument for varchar data_types. Required to\n                set max value size.\n            -   Can be any value from 1 to 255.\n        -   `serial_start`\n            -   Additional argument for serial data_types. Defaults to\n                1.\n            -   Integer that specifies the start of a SQL sequence.\n        -   `serial_increment`\n            -   Additional argument for serial data_types. Defaults to\n                1.\n            -   Integer that specifies the increment to use for serial\n                data_types. Integer.\n        -   `unique`\n            -   Determines whether or not each value in this column is\n                unique.\n            -   Can be true or false.\n        -   `null`\n            -   States whether or not a value can be \\\"null\\\".\n            -   Can be true or false.\n        -   `comments`\n            -   Field to allow for better readability of table and\n                column json. Column comments are not saved or used. They\n                are for json readability only.\n        -   `default`\n            -   Sets default value for column to fallback on if no value\n                is given.\n            -   Case insensitive.\n            -   Must follow the data_type for the column.\n            -   Note: Setting a timestamp data_type column default to\n                `UPDATETIME` or `CREATETIME` has special properties.\n                -   `CREATETIME` sets the field to the UTC time at\n                    creation. It is then not changed later.\n                -   `UPDATETIME` sets the filed to the UTC time at\n                    creation. It is updated to the update time when it\n                    is updated.\n        -   `primary_key`\n            -   Specifies primary_key for the table.\n            -   This can only be used for one column in the table.\n            -   This primary_key column will be the value users can use\n                to get a row in the table, `/v3/pgrest/data/my_pk`.\n            -   If this is not specified in a table, primary_key\n                defaults to \\\"{table_name
\\_id\\\".\n                -   Note that this default cannot be modified and is of\n                    data_type=serial.\n        -   `foreign_key`\n            -   Weather or not this key should reference a key in\n                another table, a \\\"foreign key\\\".\n            -   Can be true or false.\n            -   If foreign_key is set to true, columns arguments\n                `reference_table`, `reference_column`, and `on_delete`\n                must also be set.\n                -   `reference_table`\n                    -   Only needed in the case that foreign_key is set\n                        to true.\n                    -   Specifies the foreign table that the foreign_key\n                        is in.\n                    -   Can be set to the table_name of any table.\n                -   `reference_column`\n                    -   Only needed in the case that foreign_key is set\n                        to true.\n                    -   Specifies the foreign column that the\n                        foreign_key is in.\n                    -   Can be set to the key for any column in the\n                        reference_table.\n                -   `on_event`\n                    -   Only needed in the case that foreign_key is set\n                        to true.\n                    -   Case insensitive.\n                    -   Specifies the event strategy when referencing a\n                        foreign key.\n                    -   Can be set to an event of `ON DELETE` or\n                        `ON UPDATE`, if the key gets deleted or updated,\n                        the `event_action` will be completed by postgres\n                        -   `ON DELETE` Sets off event_action if\n                            reference is deleted.\n                        -   `ON UPDATE` Sets off event_action if\n                            reference is updated.\n                -   `event_action`\n                    -   Only needed in the case that foreign_key is set\n                        to true.\n                    -   Case insensitive.\n                    -   Specifies the event action to complete when a\n                        reference gets the `on_event` event.\n                        -   `CASCADE` deletes or updates this column\n                            when `on_event` occurs to reference.\n                        -   `SET NULL` set this column to null when\n                            `on_event` occurs to reference.\n                        -   `SET DEFAULT` set this column to column\n                            default when `on_event` occurs to reference.\n                        -   `RESTRICT` prevents deletion/update of a row\n                            when `on_event` occurs to reference.\n                        -   `NO ACTION` does nothing, raises error when\n                            referenced, when `on_event` occurs to\n                            reference."
{"filename": "pgrest.md", "prompt": "Retrieving Table Descriptions", "completion": "You can list all tables you have access to by making a GET request to\n`/v3/pgrest/manage/tables`. For example\n\n\n\nreturns a result like\n\n\n\nWe can also retrieve a single table by `id`. For example\n\n\n\nWe can also pass `details=true` query parameter to see the column\ndefinitions and validation schema for a particular table. This can be\nuseful to understand exactly what\\'s happening. The call would be as\nfollows:"}
{"filename": "pgrest.md", "prompt": "Example of Complex Table", "completion": "The following is a working complex table definition using all parameters\nfor user reference."}
{"filename": "pgrest.md", "prompt": "Editing tables", "completion": "Tenant admins are able to modify tables with a few select operations by\nmaking a PUT request to `/v3/pgrest/manage/tables/{table_id}
`. This\nfeature is only available to admins, please contact a service admin in\norder to see if you qualify for the role.\n\n::: important\n::: title\nImportant\n:::\n\nSupport for editing tables for tenant admins was added in version 1.1.0.\n:::\n\nThis PUT endpoint effectively uses `ALTER TABLE` commands with postgres\nand also updates the Django database backend to match these changes. Due\nto the complexity in this workflow, admins are only allowed a fixed set\nof operations to use. Additionally, only one operation is allowed to\ntake place per PUT request. For example, admins can not edit a table\\'s\nname, and drop a column in the same request.\n\nThe endpoint\\'s error messages are generally verbose, so errors should\nbe pointed out poignantly. A list of all PUT operations available and\nexamples is as follows:\n\n-   `root_url`\n    -   Operation to change the root_url currently associated with the\n        table.\n    -   Ex. Change table with root_url `all_people` to `some_people`.\n        -   Payload is `{\"root_url\": \"some_people\"
`\n-   `table_name`\n    -   Operation to change the table_name currently associated with the\n        table.\n    -   Ex: Change table with table_name `all_people` to `some_people`.\n        -   Payload is `{\"table_name\": \"some_people\"
`\n-   `comments`\n    -   Operation to change the comments currently associated with the\n        table (Overwrites existing).\n    -   Ex: Adding \\\"This column is not case-sensitive\\\" comment to\n        table with table_id `84`.\n        -   Payload is\n            `{\"comments\": \"This column is not case-sensitive\"
`\n-   `endpoints`\n    -   Operation to change the endpoints a table currently has\n        available.\n    -   Endpoints available are \\\"ALL\\\" (alias to set all endpoints to\n        available), \\\"NONE\\\" (alias to set no endpoints), \\\"GET_ALL\\\",\n        \\\"GET_ONE\\\", \\\"CREATE\\\", \\\"UPDATE\\\", and \\\"DELETE\\\".\n    -   Ex: Changing table so all endpoints are available.\n        -   Payload is `{\"endpoints\": [\"ALL\"]
`\n-   `column_type`\n    -   Operation to change the column_type of a particular column in\n        table.\n    -   column_types available are \\\"varchar\\\", \\\"boolean\\\",\n        \\\"integer\\\", \\\"text\\\", \\\"timestamp\\\", \\\"serial\\\", and\n        \\\"datetime\\\".\n    -   Ex: Changing column `names` from type `integer` to `varchar`.\n        -   Payload is `{\"column_type\": \"names, varchar\"
`\n-   `add_column`\n    -   Operation to add a column to a table.\n    -   Uses the same column definition formatting as when creating a\n        table. All data types and parameters apply.\n    -   Ex: Adding column `names`.\n        -   Payload is\n            `{\"add_column\": {\"names\": {\"data_type\": \"varchar\", \"char_len\": 255, \"comments\": \"first name\"
`\n-   `drop_column`\n    -   Operation to drop a column in a table (Not reversible).\n    -   Ex: Dropping column `useless_archive_column`.\n        -   Payload is `{\"drop_column\": \"useless_archive_column\"
`\n-   `drop_default`\n    -   Operation to drop a default currently set on a column in a\n        table.\n    -   Ex: Dropping default for column `i_no_longer_need_a_default`.\n        -   Payload is `{\"drop_default\": \"i_no_longer_need_a_default\"
`\n-   `set_default`\n    -   Operation to set a new default on a column in a table.\n    -   Ex: Setting default on column `names` to `no_name`.\n        -   Payload is `{\"set_default\": \"names,no_name\"
`\n\nFor example, to change the table_name of table with table_id `3` to\n`my_new_name`:\n\n\n\nFor example, to change the column_type of column `names` from `integer`\nto `varchar` in the same table:\n\n\n\nFor example, to change the endpoints of table `3` to only `CREATE` and\n`DELETE`:\n\n\n\nAll operations will result in either a verbose error message from the\nAPI regarding issues, the postgres error message returned to the API in\ncases of a postgres error, or a `Table put successfully` message."
{"filename": "pgrest.md", "prompt": "Table User API {#target data api}
", "completion": "The Data API provides endpoints for managing and retrieving data (rows)\nstored on tables defined through the Management API. For each table\ndefined through the Management API, there is a corresponding endpoint\nwithin the Data API with URL `/v3/pgrest/data/{root_url
`, where\n`{root_url
` is the associated attribute on the table.\n\nContinuing with our widgets table from above, the associated endpoint\nwithin the Data API would have URL `/v3/pgrest/data/widgets` because the\n`root_url` property of the widgets table was defined to be `widgets`.\nMoreover, all 5 default endpoints on the `widgets` collection are\navailable (none were explicitly restricted when registering the table).\nThe endpoints within the `widgets` can be described as follows:\n\n+---+---+---+----+-----------------------------+--------------------+\n| G | P | P | DE | Endpoint                    | > Description      |\n| E | O | U | LE |                             |                    |\n| T | S | T | TE |                             |                    |\n|   | T |   |    |                             |                    |\n+===+===+===+====+=============================+====================+\n| > | > | > |    | /v3/pgrest/data/widgets     | List/create        |\n|   |   |   |    |                             | widgets; bulk      |\n| X | X | X |    |                             | update multiple    |\n|   |   |   |    |                             | widgets.           |\n+---+---+---+----+-----------------------------+--------------------+\n| > |   | > | >  | /                           | Get/update/delete  |\n|   |   |   |  X | v3/pgrest/data/widgets/{id
 | a widget by id.    |\n| X |   | X |    |                             |                    |\n+---+---+---+----+-----------------------------+--------------------+\n\nNote that the `id` column is used for referencing a specific row.\nCurrently, PgREST generates this column automatically for each table and\ncalls it [{table_name
\\_id]{.title-ref
. It is a sql serial data type.\nTo override this generic `id` column, you may assign a key of your\nchoice the `primary_key` constraint. We\\'ll then use the values in this\nfield to get a specified rows. `primary_key` columns, must be integers\nor varchars which are not null and unique.\n\nAdditionally, to find the `id` to use for your row, the data endpoints\nreturn a `_pkid` field in the results for each row for ease of use.\n`_pkid` is not currently kept in the database, but is added to the\nresult object between retrieving the database result and returning the\nresult to the user. As such, `where` queries will NOT work on the\n`_pkid` field."
{"filename": "pgrest.md", "prompt": "Creating a Row", "completion": "Sending a POST request to the `/v3/pgrest/data/{root_url}
` URL will\ncreate a new row on the corresponding table. The POST message body\nshould be a JSON document providing values for each of the columns\ninside a single `data` object. The values will first be validated with\nthe json schema generated from the columns data sent in on table\ncreation. This will enforce data types, max lengths, and required\nfields. The row is then added to the table using pure SQL format and is\nfully ATOMIC.\n\nFor example, the following JSON body could be used to create a new row\non the widgets example table:\n\nnew_row.json:\n\n\n\nThe following curl command would create a row defined by the JSON\ndocument above\n\n\n\nif all goes well, the response should look like\n\n\n\nNote that an `id` of `1` was generated for the new record."
{"filename": "pgrest.md", "prompt": "Creating multiple rows at once", "completion": "The same as creating a single row, POST requests to\nthe`/v3/pgrest/data/{root_url}
` URL containing lists of row definitions\nwill create rows in bulk. POST message body should be a JSON list\ncontain JSON documents providing values for each of the columns inside a\nsingle `data` object. This works exactly the same way as single row\ncreation, but with a list input. The rows are then added to the table\nusing pure SQL format and is fully ATOMIC.\n\nFor example, the following JSON body could be used to create three rows\non the widgets example table:\n\nnew_rows.json:\n\n\n\nThe following curl command would create a row defined by the JSON\ndocument above\n\n\n\nif all goes well, the response should return all the rows just created."
{"filename": "pgrest.md", "prompt": "Updating a Row", "completion": "Sending a PUT request to the `/v3/pgrest/data/{root_url}
/{id
` URL will\nupdate an existing row on the corresponding table. The request message\nbody should be a JSON document providing the columns to be updates and\nthe new values. For example, the following would update the\n`example-widget` created above:\n\nupdate_row.json\n\n\n\nThe following curl command would update the `example-widget` row (with\n`id` of `i`) using the JSON document above\n\n\n\nNote that since only the `count` field is provided in the PUT request\nbody, that is the only column that will be modified."
{"filename": "pgrest.md", "prompt": "Updating Multiple Rows", "completion": "Update multiple rows with a single HTTP request is possible using a\n`where` filter (for more details, see the section [Where\nStanzas](#where-stanzas) below), provided in the PUT request body. For\nexample, we could update the `count` column on all rows with a negative\ncount to 0 using the following\n\nupdate_rows.json\n\n\n\nThis update_rows.json would be used in a PUT request to the root\n`widgets` collection, as follows:"}
{"filename": "pgrest.md", "prompt": "Where Stanzas", "completion": "In PgREST, `where` stanzas are used in various endpoints throughout the\nAPI to filter the collection of results (i.e. rows) that an action (such\nas retrieving or updating) is applied to. The `where` stanza should be a\nJSON object with each key being the name of a column on the table and\nthe value under each key being a JSON object with two properties:\n\n> -   `operator` \\-- a valid operator for the comparison. See the [Valid\n>     Operators](#valid-operators) table below.\n> -   `value` \\-- the value to compare the row\\'s column to (using the\n>     operator).\n\nNaturally, the type (string, integer, boolean, etc.) of the `value`\nproperty should correspond to the type of the column specified by the\nkey. Note that multiple keys corresponding to the same column or\ndifferent columns can be included in a single `where` stanza. For\nexample, the following where stanza would pick out rows whose `count`\nwas between `0` and `100` and whose `is_private` property was `true`:"}
{"filename": "pgrest.md", "prompt": "Valid Operators", "completion": "PgREST recognizes the following operators for use in `where` stanzas.\n\n+-----------+---------------------+---------------------------------+\n| Operator  | Postgres Equivalent | Description                     |\n+===========+=====================+=================================+\n| > \\<      | > \\<                | > Less than                     |\n+-----------+---------------------+---------------------------------+\n| > \\>      | > \\>                | > Greater than                  |\n+-----------+---------------------+---------------------------------+\n| > =       | > =                 | > Equal                         |\n+-----------+---------------------+---------------------------------+\n| > \\...    | > \\...              | > \\...                          |\n+-----------+---------------------+---------------------------------+\n\n*Todo\\... Full table coming soon*"}
{"filename": "pgrest.md", "prompt": "Retrieving Rows", "completion": "To retrieve data from the `/data` API, make an HTTP GET request to the\nassociated URL; an HTTP GET to `/v3/pgrest/data/{root_url}
` will\nretrieve all rows on the associated table, while an HTTP GET to\n`/v3/pgrest/data/{root_url
/{id
` will retrieve the individual row.\n\nFor example,\n\n\n\nretrieves all rows of the table \\\"init\\\":\n\n\n\nwhile the following curl:\n\n\n\nretrieves just the row with id \\\"3\\\":"
{"filename": "pgrest.md", "prompt": "Retrieving rows with search parameters", "completion": "PgREST allows users to retrieve table rows from either the `/data`\nendpoint or the `/view` endpoint using search parameters follow the\nTapis V3 search specification. All search operations available and\nexamples are detailed below.\n\n::: important\n::: title\nImportant\n:::\n\nSupport of retrieving rows with search parameters is available in\nversion 1.1.0. Previous implementation disregarded due to being out of\ndate with Tapis V3 search specifications.\n:::\n\n::: important\n::: title\nImportant\n:::\n\nTwo additional query parameters are limit and offset. Used like\n`?limit=20` or/and `?offset=5`. This caps the amount of results you get\nback or gives you results past a set number, respectively.\n:::\n\n  ----------- --------------------- -----------------------\n  Operator    Postgres Equivalent   Description\n\n  .eq         =                     Equal\n\n  .neq        !=                    Not equal\n\n  .lt         \\<                    Less than\n\n  .lte        \\<=                   Less than or equal\n\n  .gt         \\>                    Greater than\n\n  .gte        \\>=                   Greater than or equal\n\n  .in         IN                    In set of\n\n  .nin        NOT IN                Not in set of\n\n  .like       LIKE                  Like value\n\n  .nlike      NOT LIKE              Not like value\n\n  .between    BETWEEN               Between set\n\n  .nbetween   NOT BETWEEN           Not between set\n\n  .null       IS NULL               Set to TRUE/FALSE\n  ----------- --------------------- -----------------------\n\nThese operators are used in an endpoint\\'s query parameters. For example\nif I had a table with root_url `my_table` and I wanted to get all rows\nwith column `age` greater than `15` I could do the following:\n\n\n\nAnother example using .between this time would be finding all rows with\ncolumn `age` between `18-30`:\n\n\n\nIf you only wanted all rows with column `age` that match `20`, `30`, or\n`40`:"}
{"filename": "pgrest.md", "prompt": "Views Manage API", "completion": "Views allow admins to create postgres views to cordone off data from\nusers and give users exactly what they need. These views allow for\npermission_rules which cross reference a users roles, if a user has all\nroles in the permission_rules they have access to view the view.\n\nAdmins are able to create views with a post to the\n`/v3/pgrest/manage/views` endpoint. A get to `/v3/pgrest/manage/views`\nreturns information regarding the views."}
{"filename": "pgrest.md", "prompt": "View Definition Rules", "completion": "A post to the `/v3/pgrest/manage/views` endpoint to create a view\nexpects a json formatted view definition. Each view definition can have\nthe following rules.\n\n-   `view_name` - **required**\n    -   The name of the view in question.\n-   `select_query` - **required**\n    -   Query to select from the table specified with from_table\n-   `from_table` - **required**\n    -   Table to read data from\n-   `root_url`\n    -   The root_url for PgRESTs /views endpoint.\n    -   Ex: root_url \\\"view25\\\" would be accessible via\n        \\\"<http://pgrestURL/views/table25>\\\".\n-   `where_query`\n    -   Optional field that allows you to specify a postgres where\n        clause for the view\n-   `comments`\n    -   Field to allow for better readability of view json. Table\n        comments are saved and outputted on `/v3/pgrest/manage/views/`\n        endpoints.\n-   `permission_rules`\n    -   List of roles required to view this view.\n    -   If nothing is given, view is open to all.\n-   `raw_sql` - **admins only**\n    -   To allow for better use of postgres\\'s facilities there is a\n        raw_sql view creation parameter.\n\n    -   To use this parameter you must be an admin (talk to service\n        admins)\n\n    -   When using this paramter, select_query, where_query, and\n        from_table are no longer allowed, other parameters are fine.\n\n    -   The query follows\n        `CREATE OR REPLACE VIEW {tenant}
.{view_name
 {raw_sql
` format.\n\n    -   Example data body:\n\n        \n-   `materialized_view_raw_sql` - **admins only**\n    -   To allow for better use of postgres\\'s facilities there is a\n        materialized_view_raw_sql view creation parameter.\n\n    -   To use this parameter you must be an admin (talk to service\n        admins)\n\n    -   When using this paramter, select_query, raw_sql, where_query,\n        and from_table are no longer allowed, other parameters are fine.\n\n    -   The query follows\n        `CREATE MATERIALIZED VIEW {tenant
.{view_name
 {materialized_view_raw_sql
`\n        format.\n\n    -   Refresh the materialized view data by GET\n        `https://tenant.tapis.io/v3/pgrest/manage/views/{view_id
/refresh`.\n        Detailed later in these docs.\n\n    -   Data is only updated at creation and during manual refreshes\n        from users.\n\n    -   Example data body:"
{"filename": "pgrest.md", "prompt": "View Creation Example", "completion": "``` bash"}
{"filename": "pgrest.md", "prompt": "new_view.json", "completion": "{'view_name': 'test_view',\n 'root_url': 'just_a_cool_url',\n 'select_query': '*',\n 'from_table': 'initial_table_2',\n 'where_query': 'col_one >= 90',\n 'permission_rules': ['lab_6_admin', 'cii_rep'],\n 'comments': 'This is a cool test_view to view all of\n              initial_table_2. Only users with the\n              lab_6_admin and cii_rep role can view this.'}
\n bash\n$ curl -H \"tapis-v2-token: $TOKEN\" -H \"Content-type: application/json\" \\\n  -d \"@new_view.json\" https://<tenant>.tapis.io/v3/pgrest/manage/views\n bash\n$ curl -H \"tapis-v2-token: $TOKEN\" \\\n  https://<tenant>.tapis.io/v3/pgrest/manage/views/just_a_cool_url\n```"
{"filename": "pgrest.md", "prompt": "Materialized View Refresh Example", "completion": "If you had earlier created a view using the `materialized_view_raw_sql`\nattribute, you\\'d have a materialized view that you can refresh the data\nof manually. Data for materialized views is updated at creation, and at\nrefreshes, there is not other automatic refreshing of the data."}
{"filename": "pgrest.md", "prompt": "Views User API", "completion": "Users have no way to change views or to modify anything dealing with\nthem, but they are able to get views that they have sufficient\npermissions to view. The `/v3/pgrest/views/{view_id}
` requires the user\nto have at least a PGREST_USER role, this is to force all users to be in\nsome way identified or managed. The PGREST_USER role grants no\npermissions except the permission to call a get on the\n`/v3/pgrest/views/{view_id
` endpoint. This means that admins can assume\nPGREST_USER users can only view [views]{.title-ref
 in which the user\nsatisfies the permissions declared in a [views]{.title-ref
\n`permission_rules`. This gives admins fine-tuned controls on postgres\ndata by using solely views.\n\nUsers can make a get to `/v3/pgrest/views/{view_id
` with the following\ncurl."
{"filename": "pgrest.md", "prompt": "Retrieving rows with search parameters", "completion": "PgREST allows users to retrieve table rows from either the `/data`\nendpoint or the `/view` endpoint using search parameters follow the\nTapis V3 search specification. All search operations available and\nexamples are detailed below.\n\n::: important\n::: title\nImportant\n:::\n\nSupport of retrieving rows with search parameters is available in\nversion 1.1.0. Previous implementation disregarded due to being out of\ndate with Tapis V3 search specifications.\n:::\n\n::: important\n::: title\nImportant\n:::\n\nTwo additional query parameters are limit and offset. Used like\n`?limit=20` or/and `?offset=5`. This caps the amount of results you get\nback or gives you results past a set number, respectively.\n:::\n\n  ----------- --------------------- -----------------------\n  Operator    Postgres Equivalent   Description\n\n  .eq         =                     Equal\n\n  .neq        !=                    Not equal\n\n  .lt         \\<                    Less than\n\n  .lte        \\<=                   Less than or equal\n\n  .gt         \\>                    Greater than\n\n  .gte        \\>=                   Greater than or equal\n\n  .in         IN                    In set of\n\n  .nin        NOT IN                Not in set of\n\n  .like       LIKE                  Like value\n\n  .nlike      NOT LIKE              Not like value\n\n  .between    BETWEEN               Between set\n\n  .nbetween   NOT BETWEEN           Not between set\n\n  .null       IS NULL               Set to TRUE/FALSE\n  ----------- --------------------- -----------------------\n\nThese operators are used in an endpoint\\'s query parameters. For example\nif I had a view with root_url `my_view` and I wanted to get all rows\nwith column `age` greater than `15` I could do the following:\n\n\n\nAnother example using .between this time would be finding all rows with\ncolumn `age` between `18-30`:\n\n\n\nIf you only wanted all rows with column `age` that match `20`, `30`, or\n`40`:"}
{"filename": "pgrest.md", "prompt": "Role Manage API", "completion": "Role management is solely allowed for users in the PGREST_ROLE_ADMIN\nrole, or PGREST_ADMIN. These endpoints allow users to create, grant, and\nrevoke SK roles to users to match [view]{.title-ref}
 `permission_rules`.\nModifiable roles all must start with `PGREST_`, this ensures users\ncan\\'t change roles that might matter to other services. Along with\nthat, users cannot manage the `PGREST_ADMIN`, `PGREST_ROLE_ADMIN`,\n`PGREST_WRITE`, or `PGREST_READ` roles. This might be changed, but for\nnow, contact an admin to be given these roles. Note, users can grant or\nrevoke the `PGREST_USER` role, so that role_admins can manage who can\nsee views without unnecessary intervention.\n\nUsers have no access to these endpoints or anything regarding roles."
{"filename": "pgrest.md", "prompt": "Role Creation", "completion": "To create a new role users make a post to `/v3/pgrest/roles/` with a\nbody such as:\n\n``` bash"}
{"filename": "pgrest.md", "prompt": "new_role.json", "completion": "{\"role_name\": \"PGREST_My_New_Role\", \"description\": \"A new role!\"}
\n bash\n$ curl -H \"tapis-v2-token: $TOKEN\" -H \"Content-type: application/json\" \\\n  -d \"@new_role.json\" https://<tenant>.tapis.io/v3/pgrest/roles\n```\n\nThis would result in the creation of the `PGREST_My_New_Role` role."
{"filename": "pgrest.md", "prompt": "Role Creation Definition", "completion": "-   `role_name` - **required**\n    -   Name of the role to create, must start with `PGREST_`.\n-   `description` - **required**\n    -   A description of the role to be used for future reference in SK."}
{"filename": "pgrest.md", "prompt": "Role Management", "completion": "To grant or revoke a role to a specific username, users can make a post\nto `/v3/pgrest/roles/{role_name}
` with a body such as:\n\n``` bash"
{"filename": "pgrest.md", "prompt": "grant_role.json", "completion": "{\"method\": \"grant\", \"username\": \"user3234\"}
\n bash\n$ curl -H \"tapis-v2-token: $TOKEN\" -H \"Content-type: application/json\" \\\n  -d \"@grant_role.json\" https://<tenant>.tapis.io/v3/pgrest/roles/PGREST_My_New_Role\n```\n\nThis would result in the user, `user3234` being granted the role,\n`PGREST_My_New_Role`."
{"filename": "pgrest.md", "prompt": "Role Management Parameters", "completion": "-   `method` - **required**\n    -   String of either \\\"grant\\\" or \\\"revoke\\\", specifying whether to\n        revoke or grant the role to a user.\n-   `username` - **required**\n    -   Username to grant role to, or to revoke role from."}
{"filename": "pgrest.md", "prompt": "Stored Procedures", "completion": "*Coming soon*"}
{"filename": "pgrest.md", "prompt": "API Reference", "completion": "+---+---+---+---+-----------------------+------------------------------+\n| G | P | P | D | Endpoint              | Description                  |\n| E | O | U | E |                       |                              |\n| T | S | T | L |                       |                              |\n|   | T |   |   |                       |                              |\n+===+===+===+===+=======================+==============================+\n| > | > |   |   | /v3                   | Get/Create tables for the    |\n|   |   |   |   | /pgrest/manage/tables | tenant.                      |\n| X | X |   |   |                       |                              |\n+---+---+---+---+-----------------------+------------------------------+\n| > |   | > | > | /v3/pgrest/man        | Get/Update/Delete a          |\n|   |   |   |   | age/tables/{table_id}
 | specified table.             |\n| X |   | X | X |                       |                              |\n+---+---+---+---+-----------------------+------------------------------+\n| > | > | > |   | /v3/p                 | Get/Create/Update rows for a |\n|   |   |   |   | grest/data/{table_id
 | specified table.             |\n| X | X | X |   |                       |                              |\n+---+---+---+---+-----------------------+------------------------------+\n| > |   | > | > | /v3/pgrest/dat        | Get/Update/Delete specific   |\n|   |   |   |   | a/{table_id
/{row_id
 | row for a specified table.   |\n| X |   | X | X |                       |                              |\n+---+---+---+---+-----------------------+------------------------------+\n| > | > |   |   | /v                    | Get/Create view for the      |\n|   |   |   |   | 3/pgrest/manage/views | tenant.                      |\n| X | X |   |   |                       |                              |\n+---+---+---+---+-----------------------+------------------------------+\n| > |   |   | > | /v3/pgrest/m          | Get/Delete view specified.   |\n|   |   |   |   | anage/views/{view_id
 |                              |\n| X |   |   | X |                       |                              |\n+---+---+---+---+-----------------------+------------------------------+\n| > |   |   |   | /v3/p                 | Get results from view        |\n|   |   |   |   | grest/views/{view_id
 | specified.                   |\n| X |   |   |   |                       |                              |\n+---+---+---+---+-----------------------+------------------------------+\n| > | > |   |   | /v                    | Get/Create roles in SK for   |\n|   |   |   |   | 3/pgrest/manage/roles | the tenant.                  |\n| X | X |   |   |                       |                              |\n+---+---+---+---+-----------------------+------------------------------+\n| > | > |   |   | /v3/pgrest/man        | Get/Manage role specified.   |\n|   |   |   |   | age/roles/{role_name
 |                              |\n| X | X |   |   |                       |                              |\n+---+---+---+---+-----------------------+------------------------------+"
{"filename": "pods.md", "prompt": "Introduction to the Pods Service", "completion": ""}
{"filename": "pods.md", "prompt": "What is the Pods Service", "completion": "The Pods Service is a web service and distributed computing platform\nproviding pods-as-a-service (PaaS). The service implements a message\nbroker and processor model that requests pods, alongside a health module\nto poll for pod data, including logs, status, and health. The primary\nuse of this service is to have quick to deploy long-lived services based\non Docker images that are exposed via HTTP or TCP endpoints listed by\nthe API.\n\nThe Pods service provides functionality for two types of pod solutions:\n\n:   -   *Templated Pods* for run-as-is popular images. Neo4J is one\n        example, the template manages TCP ports, user creation, and\n        permissions.\n    -   *Custom Pods* for arbitrary docker images with less\n        functionality. In this case we will expose port 5000 and do\n        nothing else."}
{"filename": "pods.md", "prompt": "Using the Pods Service", "completion": "Please create issues on our [github\nrepo](https://github.com/tapis-project/pods_service) and report problems\nto Christian R. Garcia. The service is available to researchers and\nstudents. To learn more about the the system, including getting access,\nfollow the instructions in `/getting-started/index`{.interpreted-text\nrole=\"doc\"}
.\n\n------------------------------------------------------------------------"
{"filename": "pods.md", "prompt": "Getting Started", "completion": "This Getting Started guide will walk you through the initial steps of\nsetting up the necessary accounts and installing the required software\nbefore moving to the Pods Quickstart, where you will create your first\nPods service pod. If you are already using Docker Hub and the TACC TAPIS\nAPIs, feel free to jump right to the [Pods Quickstart](#pods-quickstart)\nor check out the [Pods Live\nDocs](https://tapis-project.github.io/live-docs/?service=Pods)."}
{"filename": "pods.md", "prompt": "Account Creation and Software Installation", "completion": ""}
{"filename": "pods.md", "prompt": "Create a TACC account", "completion": "The main instance of the Pods platform is hosted at the Texas Advanced\nComputing Center ([TACC](https://tacc.utexas.edu)). TACC designs and\ndeploys some of the world\\'s most powerful advanced computing\ntechnologies and innovative software solutions to enable researchers to\nanswer complex questions. To use the TACC-hosted Pods service, please\ncreate a [TACC account](https://portal.tacc.utexas.edu/account-request)."}
{"filename": "pods.md", "prompt": "Create a Docker account", "completion": "[Docker](https://www.docker.com/) is an open-source container runtime\nproviding operating-system-level virtualization. The Pods service pulls\nimages for its pods from the public Docker Hub. To register pods you\nwill need to publish images on Docker Hub, which requires a [Docker\naccount](https://hub.docker.com/) ."}
{"filename": "pods.md", "prompt": "Install the Tapis Python SDK", "completion": "To interact with the TACC-hosted Abaco platform in Python, we will\nleverage the Tapis Python SDK, tapipy. To install it, simply run:\n\n\n\n::: attention\n::: title\nAttention\n:::\n\n`tapipy` works with Python 3.\n:::"}
{"filename": "pods.md", "prompt": "Working with TACC OAuth", "completion": "Authentication and authorization to the Tapis APIs uses\n[OAuth2](https://oauth.net/2/), a widely-adopted web standard. Our\nimplementation of OAuth2 is designed to give you the flexibility you\nneed to script and automate use of Tapis while keeping your access\ncredentials and digital assets secure. This is covered in great detail\nin our Tenancy and Authentication section, but some key concepts will be\nhighlighted here, interleaved with Python code."}
{"filename": "pods.md", "prompt": "Create an Tapis Client Object", "completion": "The first step in using the Tapis Python SDK, tapipy, is to create a\nTapis Client object. First, import the `Tapis` class and create python\nobject called `t` that points to the Tapis server using your TACC\nusername and password. Do so by typing the following in a Python shell:\n\n::: important\n::: title\nImportant\n:::\n\nSupport for Pods service in Tapipy was added in version 1.2.3.\n:::\n\n``` python"}
{"filename": "pods.md", "prompt": "Import the Tapis object", "completion": "from tapipy.tapis import Tapis"}
{"filename": "pods.md", "prompt": "Log into you the Tapis service by providing user/pass and url.", "completion": "t = Tapis(base_url='https://tacc.tapis.io',\n          username='your username',\n          password='your password')\n```"}
{"filename": "pods.md", "prompt": "Generate a Token", "completion": "With the `t` object instantiated, we can exchange our credentials for an\naccess token. In Tapis, you never send your username and password\ndirectly to the services; instead, you pass an access token which is\ncryptographically signed by the OAuth server and includes information\nabout your identity. The Tapis services use this token to determine who\nyou are and what you can do.\n\n``` python"}
{"filename": "pods.md", "prompt": "Get tokens that will be used for authenticated function calls", "completion": "t.get_tokens()\nprint(t.access_token.access_token)\n\nOut[1]: eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9...\n```\n\nNote that the tapipy `t` object will store and pass your access token\nfor you, so you don\\'t have to manually provide the token when using the\ntapipy operations. You are now ready to check your access to the Tapis\nAPIs. It will expire though, after 4 hours, at which time you will need\nto generate a new token. If you are interested, you can create an OAuth\nclient (a one-time setup step, like creating a TACC account) that can be\nused to generate access and refresh tokens. For simplicity, we are\nskipping that but if you are interested, check out the Tenancy and\nAuthentication section."}
{"filename": "pods.md", "prompt": "Check Access to the Tapis APIs", "completion": "The tapipy `t` object should now be configured to talk to all Tapis APIs\non your behalf. We can check that the client is configured properly by\nmaking any API call. For example, we can use the authenticator service\nto retrieve the full TACC profile of our user. To do so, use the\n`get_profile()` function associated with the `authenticator` object on\nthe `t` object, passing the username of the profile to retrieve, as\nfollows.\n\n\n\n------------------------------------------------------------------------"}
{"filename": "pods.md", "prompt": "Pods Quickstart", "completion": "In this Quickstart, we will create an Pods service pod from a basic\nPython function. Then we will get pod credentials and logs."}
{"filename": "pods.md", "prompt": "Registering a templated Pod", "completion": "To get started we\\'re going to create a templated Pod. To do this, we\nwill use the `Tapis` client object we created above (see [Working with\nTACC OAuth](#working-with-tacc-oauth)).\n\nTo register an pod using the tapipy library, we use the\n`pods.create_pod()` method and pass the arguments describing the pod we\nwant to register through the function parameters. For example:\n\n\n\nAs you can see, we\\'re using pod_template equal to\n[template/neo4j]{.title-ref}
, that is one of our templated pods. You\nshould see a response like this:\n\n\n\nNotes:\n\n-   The [pod_id]{.title-ref
 given will be the id used by you to access\n    the pod at all times. It must be lowercase and alphanumeric. It also\n    must be unique within the tenant.\n-   Pods returned a status of `REQUESTED` for the pod; behind the\n    scenes, the Pods service has sent a message requesting the pod\n    described to our backend [spawner]{.title-ref
 infrastructure. The\n    pod\\'s image must be pulled, a pod service must be created (for\n    networking), and the networking changes must propagate to the Pod\\'s\n    proxy before the Pod is ready for use.\n-   When the pod itself has began running, the status will change to\n    `RUNNING`. Networking must change before use though (1-2 minutes).\n    Additionally, a `RUNNING` pod only means the pod itself has started,\n    check pod logs to see what your container is actually doing (if it\n    writes to stdout, else wait).\n\nAt any point we can check the details of our pods, including its status,\nwith the following:\n\n\n\nThe response format is identical to that returned from the\n`t.pods.create_pod()` method."
{"filename": "pods.md", "prompt": "Accessing a Pod", "completion": "Once your pod is in the `RUNNING` state, and after the networking\nchanges proliferate, you should have access to your pod via the\ninternet. In the case of templated pods, networking might be through a\nTCP or HTTP connection. In the case of custom docker image pods,\nnetworking always works by exposing port 5000 through HTTP.\n\nTo access your pod through the network though we can use the url\nprovided in the pod\\'s description when creating the pod or when getting\nthe pod description as we did above. In the pod response object, the\n`url` attribute gives you the url your service is being hosted on.\n\n**Place cool example here of something we can call**"}
{"filename": "pods.md", "prompt": "Retrieving the Logs", "completion": "The Pods service collects the latest 10 MB of logs (subject to change)\nfrom the pod when running and makes them available via the `logs`\nendpoint. Let\\'s retrieve the logs from the pod we just made. We use the\n`get_pod_logs()` method, passing in `pod_id`:\n\n\n\nThe response should be similar to the following:\n\n\n\nAs you can see, because this is a Neo4J database template, we have the\nlogs from the Neo4J database initializes and getting to it\\'s `Started`\nstate."}
{"filename": "pods.md", "prompt": "Conclusion", "completion": "Congratulations! You\\'ve now created, registered, and accessed your\nfirst pod. There is a lot more you can do with the Pods service though.\nTo learn more about the additional capabilities, please continue on to\nthe Technical Guide.\n\n------------------------------------------------------------------------"}
{"filename": "pods.md", "prompt": "Future work. Only quickstart is currently complete.", "completion": "Please view our API Reference to see what additional functionality is\ncurrently available.\n\n------------------------------------------------------------------------"}
{"filename": "pods.md", "prompt": "API Reference", "completion": "The following link is to our live-documentation that takes our OpenAPI\nv3 specification that is automatically generated and gives users the\npublic endpoints available within the Pods API along with request body\nexpected and descriptions for each field.\n\n<https://tapis-project.github.io/live-docs/?service=Pods>"}
{"filename": "pythondev.md", "prompt": "Tapipy User Guide", "completion": ""}
{"filename": "pythondev.md", "prompt": "What is Tapipy", "completion": "Tapipy is a Python library for interacting with an instance of the Tapis\nAPI framework. The package is hosted on [PyPi -\ntapipy](https://pypi.org/project/tapipy/). You can install it with:\n\n\n\nThe library is automatically generated by referencing the OpenAPI spec\nfiles which a `Tapis` client built from the OpenAPI spec files from\nTACC\\'s Tapis services. With this functionality a user is able to\nauthorize itself with the `Tapis` client and have a \\'live\\' library in\norder to interact with all Tapis services. Usage of `tapipy` allows for\nautomatic token refreshing, verbose error messages regarding endpoint\ninputs, and up to date syntax for all services."}
{"filename": "pythondev.md", "prompt": "Links", "completion": "| PyPi Package: <https://pypi.org/project/tapipy>\n| Github Repo: <https://github.com/tapis-project/tapipy>"}
{"filename": "pythondev.md", "prompt": "Initializing Tapipy", "completion": "To initialize a `Tapis` client object, `t`, you will replace\n`your_tacc_username` and `your_tacc_password` with your TACC username\nand password, preserving the quotation marks shown in the command below.\nThis command will authenticate you with Tapis for your respective\n`base_url` and store your retrieved token for later `tapipy` calls.\n\n``` python\nfrom tapipy.tapis import Tapis"}
{"filename": "pythondev.md", "prompt": "Create python Tapis client for user", "completion": "t = Tapis(base_url= \"https://tacc.tapis.io\",\n          username=\"your_tacc_username\",\n          password=\"your_tacc_password\")"}
{"filename": "pythondev.md", "prompt": "Get tokens now that you're initialized", "completion": "t.get_tokens()\n python\nt.access_token\n text\naccess_token: *very long string of alphanumeric characters*\nclaims: {\n    'jti': '007fa9e6-f044-4817-a812-12292b2bdbe3',\n    'iss': 'https://tacc.tapis.io/v3/tokens',\n    'sub': 'your_tacc_username',\n    'tapis/tenant_id': 'tacc',\n    'tapis/token_type': 'access',\n    'tapis/delegation': False,\n    'tapis/delegation_sub': None,\n    'tapis/username': 'your_tacc_username',\n    'tapis/account_type': 'user', 'exp': 1657686889,\n    'tapis/client_id': None,\n    'tapis/grant_type': 'password'\n}
\nexpires_at: 2022-07-13 04:34:49+00:00\nexpires_in: <function Tapis.add_claims_to_token.<locals>._expires_in at 0x10a070280>\njti: 007fa9e6-f044-4817-a812-12292b2bdbe3\noriginal_ttl: 14400\n```\n\nWhere you will have your own access token and the placeholder\n`your_tacc_username` will be replaced with the username you used."
{"filename": "pythondev.md", "prompt": "Using Tapipy", "completion": "Now in order to use the `Tapis` client you can reference the [Tapis Live\nDocs](https://tapis-project.github.io/live-docs) to browse all service\nspecifications. Tapipy is generated off of these specific\nspecifications, so this is your most up-to-date reference on `tapipy`\nfunctionality (if `tapipy` is fully updated).\n\nFor example, if you wanted to create a system using the Systems service\nyou would navigate to the proper service in the top bar, browse the\nsidebar for functions and view the specification in the main pane. An\nexample is displayed below:\n\n![image](images/livedocssystems.png)\n\nFor this example, creating a system, I can see I need to run the\nfollowing:\n\n\n\n::: important\n::: title\nImportant\n:::\n\nTapipy 1.3.2 introduces native support for `t.files.insert()`. Previous\na workaround, `t.upload(filePath, systemId, path)` was required. Read\nmore `here<tapipyuploadanchor>`{.interpreted-text role=\"ref\"}
.\n:::\n\nYou can specify any parameter identified by the spec. Tapipy will detail\nmissing required parameters if they\\'re missing.\n\nThe live-docs are a helpful GUI on top of OpenAPI v3 specifications that\neach service writes to detail endpoints, attributes, proper\ncontent-types, return objects, etc. Depending on the verbosity of the\nspecifications the site could detail what each function and attribute\nis, along with defaults and type restrictions."
{"filename": "pythondev.md", "prompt": "Special User Query Parameters and Headers", "completion": "For the most part, arguments that can or should be passed to a Tapis\nendpoint are described in the OpenAPI definition files and recognized\nautomatically by `tapipy`. However, due to limitations in what can be\nexpressed in OpenAPI, there are some paramaters that are not defined in\nthe definition files; for example, the search parameters for various\nendpoints.\n\nTo accommodate these cases, `tapipy` recognizes two special keyword\narguments to all of its methods that correspond to Tapis API calls\n(i.e., all of its \\\"operations\\\"). They are:\n\n> -   `_tapis_headers` - dictionary-like object of header names (keys)\n>     and vales.\n> -   `_tapis_query_parameters` - dictionary-like object of query\n>     parameter names (keys) and values.\n\nUse the above two special arguments for passing headers (respectively,\nquery parameters) that are not specified in the OpenAPI definition of an\nendpoint.\n\nFor example, I can issue a search using the following syntax:"}
{"filename": "pythondev.md", "prompt": "Additional Tapipy Parameters", "completion": "The tapipy package allows for spec file customization in `Tapis` client\ninitialization:\n\n-   \n\n    `resource_set`: str in \\[\\\"tapipy\\\", \\\"prod\\\", \\\"staging\\\", \\\"dev\\\"\\]\n\n    :   -   Determines which set of resource to use, defaults to\n            `tapipy`.\n\n        -   \n\n            Important to note that if a `custom_spec_dictionary` is used, it is appended to this resource_set.\n\n            :   -   For example, you would set `prod` and then specify a\n                    custom specs that will be added on.\n\n-   \n\n    `custom_spec_dict`: {resource_name: str, resource_url: str}
\n\n    :   -   \n\n            Allows users to modify the base resource set urls.\n\n            :   -   i.e. I can specify actor as a resource name and\n                    change the url.\n\n        -   \n\n            Also allows users to add new resources to the set.\n\n            :   -   i.e. I can add a new resource named \\\"test\\\" with a\n                    custom url.\n                -   Any new specs will be downloaded and added cached\n                    without an explicit recache.\n\n        -   \n\n            Allows local resources.\n\n            :   -   Specify an absolute path in the dict with local:\n                    prefixing it and tapipy will load in a local OpenAPI\n                    v3 yml spec file.\n                -   `custom_spec_dict={'cactus': 'local: /home/tapis/myfolder/cactusSpec.yml'
`\n\n-   \n\n    `download_latest_specs`: bool\n\n    :   -   Allows users to re-download all specs regardless on if they\n            already exist in the cache. Defaulted to False\n\n        -   \n\n            This will happen every time the `Tapis` client is initialized, it\\'s a tad slower, and can cause live updates to specs.\n\n            :   -   As such, be warned. There are functions to update\n                    spec files below.\n\n-   \n\n    `spec_dir`: str\n\n    :   -   Allows users to specify folder to save specs to. Defaults to\n            none which uses Tapipy\\'s install folder.\n        -   If you are updating specs it\\'s wise to use a different\n            folder in order to not modify the base specs.\n\n::: warning\n::: title\nWarning\n:::\n\nIf your user account does not have permission to write to Tapipy\\'s\ninstall directory you will encounter an error due to file system\npermissions. You can use `spec_dir` as highlighted above and use a\ndirectory you have permission to write to. (Important for Docker\ncontainers)\n:::"
{"filename": "pythondev.md", "prompt": "Example", "completion": "The following is an example of some custom parameter setting. As you can\nsee, the abaco resource will now use the spec at `URL#1`, overwriting\nthe resource definition in the `tapipy` resource set, it\\'ll download it\nif it doesn\\'t exist. The same for the longhorn resource. This means\nthat the `Tapis` client will now have access to all specs in `tapipy`\nlike normal, but with a modified abaco and with a new longhorn resource.\nAll of these are stored at the new spec_dir because you don\\'t want to\naccidentally overwrite any packaged specs when call update_spec_cache()\nlater (talked about in the next section)."}
{"filename": "pythondev.md", "prompt": "Tapipy Instantiation with only a Tapis access token", "completion": "Tapipy can be instantiated with only a Tapis access token. This is\nuseful when you have a created token and don\\'t want to provide username\nand password. The minimal instantiation requires the base_url and the\ntoken as follows:\n\n\n\nAs the token is already created users do not have to run\n`t.get_tokens()`. You can use the client as normal at this point."}
{"filename": "pythondev.md", "prompt": "Refresh Tapipy\\'s Cached Specs", "completion": "Tapipy uses a cache to organize spec dictionaries as pickled files and\nhas the ability to accept custom spec files. By default Tapipy keeps a\nset of base spec files in the `{tapipyInstallDir}
/specs` folder. These\nspecs are pre-pickled at package build time. To update specs,\n\n::: warning\n::: title\nWarning\n:::\n\nIf your user account does not have permission to write to Tapipy\\'s\ninstall directory you will encounter an error due to file system\npermissions. You can use `spec_dir` as highlighted above and use a\ndirectory you have permission to write to. (Important for Docker\ncontainers)\n:::"
{"filename": "pythondev.md", "prompt": "Two Spec Refresh Methods", "completion": "1.  \n\n    `t.update_spec_cache()` - No inputs\n\n    :   -   Will refresh client\\'s `t.resource_set` resources and save\n            to `t.spec_dir`.\n\n        \n\n2.  \n\n    `tapipy.tapis.update_spec_cache()` - Takes `resources` and `spec_dir`\n\n    :   -   Will refresh specified resources and cache to inputted\n            `spec_dir`\n        -   Default to refreshing all resources from `tapipy` and saving\n            to package install dir."}
{"filename": "pythondev.md", "prompt": "Creating and Using Tapipy Refresh tokens", "completion": "Tapipy will authenticate you to Tapis and generate a token, however to\nactivate automatic token refreshing via refresh token you must also have\na `Tapis` client created in order to provide `client_id` and\n`client_key` at Tapis initialization.\n\nThe following code will show you how to create a client if you don\\'t\nalready have one and then initialize a `Tapis` client with refresh\ncapabilities by also providing `client_id` and `client_key`. If you\nalready have a client created then you can jump to the `Fourth` step.\nFeel free to run just the code you need.\n\n``` python\nfrom tapipy.tapis import Tapis"}
{"filename": "pythondev.md", "prompt": "First - Create your normal Tapis client so we can run create_client.", "completion": "t = Tapis(base_url = 'https://tacc.tapis.io',\n          username = 'cgarcia',\n          password = 'password')\nt.get_tokens()"}
{"filename": "pythondev.md", "prompt": "Second - List your clients (check if you already have a client created)", "completion": "t.authenticator.list_clients()"}
{"filename": "pythondev.md", "prompt": "Third - If you don't have a client, create one.", "completion": "t.authenticator.create_client()"}
{"filename": "pythondev.md", "prompt": "Should result in:", "completion": "'''\n[\nactive: True\ncallback_url: \nclient_id: my_client_id\nclient_key: my_client_key\ncreate_time: Wed, 02 Nov 2022 20:18:01 GMT\ndescription: \ndisplay_name: my_display_name\nlast_update_time: Wed, 02 Nov 2022 20:18:01 GMT\nowner: cgarcia\ntenant_id: icicle]\n'''"}
{"filename": "pythondev.md", "prompt": "Fourth - Grab the client_id and client_key, add it to your new Tapis client initialization", "completion": "t2 = Tapis(base_url='https://tapis.io',\n        username='cgarcia',\n        password='pass',\n        client_id='client_id',\n        client_key='client_key')\nt2.get_tokens()"}
{"filename": "pythondev.md", "prompt": "You should now be able to run refresh_tokens with no errors. The t2 object will refresh when needed.", "completion": "t2.refresh_tokens()\n```"}
{"filename": "pythondev.md", "prompt": "Using `t.files.insert` with Tapipy for file upload. {#tapipyuploadanchor}
", "completion": "Introduced in Tapipy 1.3.2, the library now has native support for\n`multipart/form-data` input data and headers. This allows us to natively\nsupport the `t.files.insert` operation. The following is an example of\nuploading a `filepath.ext` file to a specific `path` in a users Tapis\n`system_id` system.\n\n\n\n::: important\n::: title\nImportant\n:::\n\nThe previous static workaround,\n`t.upload(filePath, systemId, local_file_path)`, is now deprecated and\nwill be removed in a future release.\n:::\n\n------------------------------------------------------------------------"
{"filename": "pythondev.md", "prompt": "TapisService User Guide", "completion": ""}
{"filename": "pythondev.md", "prompt": "What is TapisService", "completion": "TapisService is a Python plugin for Tapipy which extends Tapipy with\nadditional service oriented Python modules. This plugin is useful when\ncreating a Tapis service or using a Tapis service account. The package\nis hosted on [PyPi -\ntapisservice](https://pypi.org/project/tapisservice/). You can install\nit with:\n\n\n\nThe plugin adds systems for authentication, configs, logging, Tapis\nerrors, TenantCache, alongside helper code for working in `Flask`,\n`FastAPI`, and `Django`, three popular Python web frameworks. Python\nbased Tapis services should make use of `tapisservice` for at least the\nauthentication in order to not unneccessary rereplicate code."}
{"filename": "pythondev.md", "prompt": "Links", "completion": "| PyPi Package: <https://pypi.org/project/tapisservice>\n| Github Repo: <https://github.com/tapis-project/tapipy-tapisservice>"}
{"filename": "pythondev.md", "prompt": "Writing a New Service", "completion": ""}
{"filename": "pythondev.md", "prompt": "Getting Started", "completion": "When creating a new Tapis v3 service, use the following guidelines:"}
{"filename": "pythondev.md", "prompt": "Create a git repository with the following:", "completion": ""}
{"filename": "pythondev.md", "prompt": "Service configuration", "completion": "A description of all possible service configurations should be provided\nin the `configschema.json` file or example, if your service requires a\nSQL database, you may have a property, `sql_db_url`, with the following\ndefinition in your `configschema.json`:\n\n\n\nThe fields you define in your service\\'s `configschema.json` file will\ncomplement those defined in the `configschema.json` file included in\nthis repository. Any configs that will be used by all services, such as\n`service_password`, should be defined in the common `configschema.json`.\nIt is currently possible to override the definition of a config provided\nin this repository with a new definition provided in your service\\'s\n`configschema.json` file, but this is not recommended.\n\nProvide values for the configs in the `config-local.json` file. When\ndeploying to a remote environment, such as the Tapis develop\nenvironment, the `config-local.json` can be replaced with file mounted\nfrom a ConfigMap with different values.\n\n-   Add any packages required by your service to the `requirements.txt`\n    file. Keep in mind that a number of packages are installed by this\n    repository (such as jsonschema, pyjwt, etc.), so it is possible you\n    will need to add few additional packages.\n-   Create a Dockerfile to build your service. The image name for your\n    service should be `tapis/<service_name>-api`; for example,\n    `tapis/tokens-api` or `tapis/tenants-api`. Here is a general\n    template for the Dockerfile for your service:\n\n``` python"}
{"filename": "pythondev.md", "prompt": "inherit from the flaskbase image:", "completion": "FROM: tapis/flaskbase"}
{"filename": "pythondev.md", "prompt": "set the name of the api, for use by some of the tapisservice modules.", "completion": "ENV TAPIS_API <api_name>"}
{"filename": "pythondev.md", "prompt": "install additional requirements for the service", "completion": "COPY requirements.txt /home/tapis/requirements.txt\nRUN pip install -r /home/tapis/requirements.txt"}
{"filename": "pythondev.md", "prompt": "copy service source code", "completion": "COPY configschema.json /home/tapis/configschema.json\nCOPY config-local.json /home/tapis/config.json\nCOPY service /home/tapis/service"}
{"filename": "pythondev.md", "prompt": "run service as non-root tapis user", "completion": "RUN chown -R tapis:tapis /home/tapis\nUSER tapis\n```"}
{"filename": "pythondev.md", "prompt": "Migration skeleton", "completion": "For services using Postgres, create migration skeleton. Migrations are\nbased on the alembic package and must be initialized. Run the following\ncommands from a terminal:\n\n``` bash\n$ docker run -it --entrypoint=bash --network=<service>-api_<service> -v $(pwd):/home/tapis/mig tapis/tenants-api"}
{"filename": "pythondev.md", "prompt": "inside the container:", "completion": "$ cd mig; flask db init\n$ flask db migrate\n$ flask db upgrade\n$ exit\n```"}
{"filename": "pythondev.md", "prompt": "Migrations Dockerfile", "completion": "Create Dockerfile-migrations to containerize your migrations code. For\nsimple cases, you may be able to just use the following after change\n\\<service\\> to the name of your service.\n\n``` dockerfile"}
{"filename": "pythondev.md", "prompt": "image: tapis/<service>-api-migrations", "completion": "from tapis/<service>-api\n\nCOPY migrations /home/tapis/migrations\n\nENTRYPOINT [\"flask\", \"db\"]\n```"}
{"filename": "pythondev.md", "prompt": "Service Dockerfile", "completion": "Write a docker-compose.yml file to facilitate local development."}
{"filename": "pythondev.md", "prompt": "TapisService Modules", "completion": "Code for a number of common tasks has been packaged into `tapisservice`\nmodules. In this section, we give an overview of how to use some of the\nfunctionality in your service."}
{"filename": "pythondev.md", "prompt": "Accessing TapisService Modules", "completion": "You can install `tapisservice` with `pip install tapisservice`. Services\ncan import modules directly from this package; for example:"}
{"filename": "pythondev.md", "prompt": "Service Configuration and Initialization", "completion": "> Most services will need to do some initialization tasks before they\n> are ready to respond to requests. For example, they may need to\n> connect to a database or make some requests to some other Tapis\n> services. Initialization also usually involves reading configuration\n> data. `tapisservice` audits the supplied config file and makes\n> configration data available through a singelton object, `conf`,\n> available from the `tapisservice.config` package. By importing the\n> object, for instance, in the API\\'s `__init__.py` module, the config\n> file will be read and validated, and the resulting configurations\n> transformed to Python objects. For example, if a service requires a\n> configuration, `max_number_retries`, then it could use the following\n> entry in its `configschema.json` file:\n\n\n\nand then, place the following code in its `__init__.py` file:\n\n\n\nBy the time the import has completed, the service is guaranteed that\n`conf` contains all required fields and that they conform to the\nrequirements specified in the `configschema.json` file. In particular,\nthe types of the attributes are the same as that specified in the\n`configchema.json` file."}
{"filename": "pythondev.md", "prompt": "Making Service Requests", "completion": "The `tapisservice.auth` package provides a function,\n`get_service_tapis_client` which can be used to get a pre-configured\n`Tapis` client for making service requests. A common technique is to\nfetch the service client in the `__init__.py` module so that it is\ncreated at service initialization and available via import throughout\nthe rest of the service code.\n\nWithin `__init__.py`:\n\n\n\nFrom within any other service modules:\n\n``` python\nfrom __init__ import t\n. . ."}
{"filename": "pythondev.md", "prompt": "use the client within some method or function:", "completion": "t.sk.getUsersWithRole(...)\n```"}
{"filename": "pythondev.md", "prompt": "Authentication", "completion": "The `tapisservice.auth` module provides functions for resolving the\nauthentication data contained in a reqeust to your service."}
{"filename": "pythondev.md", "prompt": "JWT Authentication", "completion": "The most common and straight-forward case is when an endpoint in your\nservice requires a JWT. For this use case, the\n`tapisservice.auth.authentication()` function can be used. This function\ndoes the following:\n\n> 1.  Checks for a JWT in the `X-Tapis-Token` header, and checks the\n>     other `X-Tapis-` headers.\n>\n> 2.  Validates the JWT, including checking the signature and\n>     expiration, and sets the following on the global thread-local,\n>     `g`:\n>\n>     > a.  `g.x_tapis_token` - the raw JWT.\n>     > b.  `g.token_claims` - the claims object associated with the\n>     >     JWT, as a python dictionary.\n>     > c.  `g.username` - the username from the JWT `username` claim.\n>     > d.  `g.tenant_id` - the tenant id from the JWT `tenant_id`\n>     >     claim.\n>     > e.  `g.account_type` - the account type (either `user` or\n>     >     `service`) from the JWT.\n>     > f.  `g.delegation` - whether the token was a delegation token\n>     >     (True or False).\n>     > g.  `g.x_tapis_tenant` - the value of the `X-Tapis-Tenant`\n>     >     header.\n>     > h.  `g.x_tapis_user` - the value of the `X-Tapis-User` header.\n>     > i.  `g.request_tenant_id` - **USE THIS** JWT `tenant_id` or, if\n>     >     service, `X-Tapis-Tenant`.\n>     > j.  `g.request_username` - **USE THIS** JWT `username` or, if\n>     >     service, `X-Tapis-User`.\n\n::: important\n::: title\nImportant\n:::\n\nUse `g.request_tenant_id` and `g.request_username`. These both resolve\nto their respective values, obtained from the incoming request JWT Tapis\ntoken. However, the values change when service accounts send \\\"on behalf\nof\\\" (obo) requests. Only services within the same site may complete\nthese request, this is checked by tapisservice.\n:::\n\nThis function raises the following exceptions:\n\n:   1.  `tapisservice.errors.NoTokenError` - if no token was found.\n    2.  `tapisservice.errors.AuthenticationError` - the token was\n        invalid."}
{"filename": "pythondev.md", "prompt": "Other Types of Authentication", "completion": "Some services, such as the Authenticator, use other types of\nauthentication, including HTTP Basic Auth. Several `tapisservice.auth`\nfunctions are provided to facilitate tasks related to alternative\nauthentication methods. TODO"}
{"filename": "pythondev.md", "prompt": "Logging", "completion": "The `tapisservice.logs` module provides basic logger with a file and\nstdout handler. To use it, create an instance of the logger in each\nmodule where you want to add logs by calling the `get_logger` function\nwith the module name; for example:\n\n\n\nThen add log statements using the logger:"}
{"filename": "pythondev.md", "prompt": "Error Handling", "completion": "For REST APIs, the `tapisservice.util` module provides the `TapisApi`\nclass and the `flask_errors_dict` dict and `handle_error()` function.\nUse them by adding the following to your `api.py` module:\n\n``` python\nfrom flask import Flask\nfrom tapisservice.utils import TapisApi, handle_error, flask_errors_dict"}
{"filename": "pythondev.md", "prompt": "basic flask \"app\" object:", "completion": "app = Flask(__name__)"}
{"filename": "pythondev.md", "prompt": "TapisApi object; created with the app object and the flask_errors_dict to establish the 4 stanza structure of error respones:", "completion": "api = TapisApi(app, errors=flask_errors_dict)"}
{"filename": "pythondev.md", "prompt": "Set up error handling to use the handle_error() function -", "completion": "api.handle_error = handle_error\napi.handle_exception = handle_error\napi.handle_user_exception = handle_error\n python\nraise errors.MyServiceError(msg='Invalid sprocket; too many widgets.', code=400)\n json\n{\n    \"message\": \"Invalid sprocket; too many widgets.\",\n    \"status\": \"error\",\n    \"version\": \"conf.version\",\n    \"result\": null\n}
\n python\ntry:\n    app_id = post_data['app_id']\nexcept KeyError:\n    raise errors.BadInputError(msg='The app_id parameter was missing. Please be sure to pass app_id.')   # <- decends from BaseTapisError\n```"
{"filename": "pythondev.md", "prompt": "Working with TapisService, Need to Knows", "completion": "If you\\'re developing with TapisService then please ensure you\nunderstand the following topics."}
{"filename": "pythondev.md", "prompt": "Framework Talk", "completion": "Flaskbase is the original implementation of a `tapisservice` like scheme\nwhere we provide neccessary helper functions for dealing with Tapis. It\nwas a library packaged in a Docker container which services based (and\nstill can) their containers off of for a working coding environment. As\nthe name says, this library was originally intended for `Flask` and\novertime it was generalized for more frameworks and to be used even\nwithout any specified framework. Alongside `Flask`, `FastAPI` has full\nsupport of all `tapisservice` features. `Django` is also a supported\nframework, however all functionality is not yet at 100%.\n\nThese three frameworks already have Tapis services already built off of\nthem. These can be great as example implementations to base your own\nservices off of. Information is as follows:\n\n-   \n\n    Flask:\n\n    :   -   [Abaco](https://github.com/TACC/abaco/tree/dev-v3). The\n            repository\\'s `/actors` folder contains `__init__.py` along\n            with `auth.py`.\n        -   Abaco was the original user of Flaskbase and fully\n            implements all `tapisservice` functionality.\n\n-   \n\n    FastAPI:\n\n    :   -   [Pod\n            Service](https://github.com/tapis-project/pods_service). The\n            repository\\'s `/service` folder contains `__init__.py` along\n            with `auth.py`.\n        -   Pods was the first implementation of `FastAPI` with\n            `tapisservice`, it fully implements all `tapisservice`\n            functionality.\n\n-   \n\n    Django:\n\n    :   -   [PgREST](https://github.com/tapis-project/paas). The\n            repository\\'s `/pgrest` folder contains `__init__.py` along\n            with `views.py`.\n        -   PgREST implemented most features, however it\\'s missing a\n            full use of auth capabilities. Making use of only token\n            resolution in the aforementioned `views.py`."}
{"filename": "pythondev.md", "prompt": "Framework Initialization", "completion": ""}
{"filename": "pythondev.md", "prompt": "Flask", "completion": "``` python\nfrom __init__ import Tenants ## Expects \"init.py\" as described in \"Making Service Requests\"\nfrom flask import Flask\nfrom tapisservice.tapisflask.utils import TapisApi\nfrom tapisservice.auth import authn_and_authz\n\ndef authentication():\n    ## You can write your own auth code here\n    ## This will run after tapisservice validates token and sets all valuable g object fields\n    user = g.request_username\n    tenant_id = g.request_tenant_id\n\n\ndef authorization():\n    ## You can write your own auth code here\n    ## This will run after tapisservice validates token and sets all valuable g object fields\n\napp = Flask(__name__)\napi = TapisApi(app)  ## Empty wrapper around \"flask.Flask.api\" for future use."}
{"filename": "pythondev.md", "prompt": "tapisservice authn/authz", "completion": "@app.before_request\ndef auth():\n    # Provide callbacks for authn and authz which will have access to g object fields.\n    # authentication runs, followed by authorization\n    authn_and_authz(tenant_cache=Tenants,\n                    authn_callback=authentication,\n                    authz_callback=authorization)\n```"}
{"filename": "pythondev.md", "prompt": "FastAPI", "completion": ""}
{"filename": "pythondev.md", "prompt": "Example of Usual API file imports", "completion": "This applies most for FastAPI and Flask. Django can use some of this on\na case by case basis.\n\n``` python\nfrom __init__ import t ## Expects \"init.py\" as described in \"Making Service Requests\"\nfrom tapisservice.config import conf\nfrom tapisservice.errors import BaseTapisError\nfrom tapisservice.logs import get_logger\nlogger = get_logger(__name__)"}
{"filename": "pythondev.md", "prompt": "Use precreated t object with service", "completion": "t.systems.get_systems(_tapis_set_x_headers_from_service = True)"}
{"filename": "pythondev.md", "prompt": "Get config.json site_id key value", "completion": "site_id_to_use = conf.site_id"}
{"filename": "pythondev.md", "prompt": "Use logger and Tapis error", "completion": "try:\n    logger.debug(\"Writing logs to file described in config.json and configschema.json.\")\nexcept BaseTapisError:\n    logger.debug(\"Error found, will be given 400 status code and traditional Tapis 5 stanza output.\")\n```"}
{"filename": "pythondev.md", "prompt": "Global `g` object - Flask and FastAPI", "completion": "`tapisservice` uses request thread local `g` objects to expose data\nglobally. This object is created per request and lasts the lifespan of\nsaid request. This allows `tapisservice` to write valuable information\nto it for reference by code. Key variables to use are: -\n`g.request_tenant_id` - Either incoming token `tenant_id` claim or \\\"on\nbehalf of\\\" headers set by Tapis services. - `g.request_username` -\nEither incoming token `username` claim or \\\"on behalf of\\\" headers set\nby Tapis services. - `g.token_claims` - the claims object associated\nwith the JWT, as a python dictionary.\n\nFlask natively support a `g` object. `tapisservice` simply intercepts\nthe object and writes to it during the `authn_and_authz` phase. FastAPI\ndoes not natively use a `g` object, but `tapisservice` introduces the\nvariable. You can think of both versions of `g` to be the same in these\ndocs, use it in both frameworks."}
{"filename": "pythondev.md", "prompt": "Use `g.request_tenant_id` and `g.request_username`", "completion": "Hammering home the point, `tapisservice` exposes two sets of similar\nvariables:\n\n-   `g.request_username` - `g.username` if not service and\n    `g.x_tapis_user` otherwise\n-   `g.username` - incoming request token\\'s `username` claim\n-   `g.x_tapis_user` - the value of the `X-Tapis-Tenant` header\n\nand\n\n-   `g.request_tenant_id` - `g.tenant_id` if not service and\n    `g.x_tapis_tenant` otherwise\n-   `g.tenant_id` - incoming request token\\'s `tenant_id` claim\n-   `g.x_tapis_tenant` - the value of the `X-Tapis-User` header\n\nUse `g.request_tenant_id` and `g.request_username`. These both resolve\nto the values obtained from the incoming request JWT Tapis token.\nHowever, the values change (to the `x_tapis_*` variants) when service\naccounts send \\\"on behalf of\\\" (obo) requests. This allows services to\nreference only one variable versus choosing to use `g.username` or\n`g.x_tapis_user` at runtime. To note, only services within the same site\nmay complete these OBO requests, this is checked by tapisservice.\n\nThe remaining variables are useful for debugging or defining exactly who\nsent a request and exactly how."}
{"filename": "pythondev.md", "prompt": "Use Tapipy as a Service Account", "completion": "A service account can still make use of the `tapipy` client, some\nchanges will apply though.\n\n``` python\nfrom __init__ import t ## Expects \"init.py\" as described in \"Making Service Requests\""}
{"filename": "pythondev.md", "prompt": "Uses tenant and username from client, meaning service's tenant and username.", "completion": "t.systems.get_systems(_tapis_set_x_headers_from_service = True)"}
{"filename": "pythondev.md", "prompt": "Use specified _x_tapis_user and _x_tapis_tenant_id rather than service's values.", "completion": "t.systems.get_systems(_x_tapis_user = \"cgarcia\",\n                      _x_tapis_tenant_id = \"tacc\")\n```"}
{"filename": "security.md", "prompt": "Introduction to the Security Kernel", "completion": "The Security Kernel (SK) microservice provides role-based authorization\nand secrets management for Tapis. Authentication is based on JSON Web\nTokens (JWTs) managed by the\n[Authentication](https://tapis.readthedocs.io/en/latest/technical/authentication.html)\nsubsystem.\n\nSK uses a PostgreSQL database to store its authorization data and the\nopen source version of HashiCorp\n[Vault](https://www.hashicorp.com/products/vault) as its secrets\nbackend. In the sections that follow, we discuss SK\\'s authorization and\nsecrets model, interfaces and capabilities. The actual SK REST APIs can\nbe found [here](https://tapis-project.github.io/live-docs/?service=SK)."}
{"filename": "security.md", "prompt": "Authorization", "completion": "SK authorization is based on an extended version of the Apache\n[Shiro](https://shiro.apache.org/) authorization model, which defines\nboth [roles](#roles) and [permissions](#permissions)."}
{"filename": "security.md", "prompt": "Roles", "completion": "In a tenant, each role has a unique name and is assigned an owner. SK\nprovides a set of *role endpoints* to create, update, query and delete\nroles. It also provides *user endpoints* to grant users roles, revoke\nroles from users, and query the roles assigned to a user. With these\nprimitives, Tapis implements fairly typical role-base access control\nusing a distributed architecture. Among the most called endpoints is\n[/user/hasRole](https://tapis-project.github.io/live-docs/?service=SK#operation/hasRole),\nwhich checks whether a user has a certain role.\n\nAll role-based authorizations go through SK, which provides site-wide,\nnetwork access to authorization checking. A possible downside of this\napproach is the extra network cost incurred on Tapis calls that\nauthorize users (many service to service calls avoid this overhead). So\nfar this overhead has had minimal impact."}
{"filename": "security.md", "prompt": "Built-In Roles", "completion": "Each tenant has at least one tenant administrator, which is nothing more\nthan a user assigned a distinguished, tightly controlled role\n(*\\$!tenant_admin*). The initial tenant administrator is assigned during\ntenant creation; see\n[Tenants](https://tapis.readthedocs.io/en/latest/technical/authentication.html#tenants)\nfor details. There are special endpoints for granting and revoking the\ntenant administrator role, and for validating and listing administrators\nin a tenant. Only a tenant administrator can grant or revoke the\nadministrator role to another user.\n\nEach user is implicitly given a default role. These roles have names\nthat begin with \\\"\\$\\$\\\" and end with the user\\'s ID. Default roles are\nmost commonly used in conjunction permissions, as discussed below."}
{"filename": "security.md", "prompt": "Hierarchical Roles", "completion": "One feature that distinguishes SK\\'s implementation of role-based\nauthorization is that roles can be arranged in directed acyclic graphs\n(DAGs) based on parent/child relationships. We say a parent role\n*contains* a child role, and a child can have zero or more parents. The\nset of roles defined in a tenant can be thought of as a forest of DAGs.\n\nThis contains relation allows SK users to define roles with fine\ngranularity and then compose them in flexible ways. For example, let\nDirA be the root of a directory subtree that is shared among users. We\ncould define *DirA_Owner* as a parent role with *DirA_Reader* and\n*DirA_Writer* as child roles (assume the typical semantics implied by\ntheir names). Users assigned *DirA_Owner* would implicitly also be\nassigned *DirA_Reader* and *DirA_Writer*. A user only assigned\n*DirA_Reader* would not have write privileges.\n\nTaking the example one step further, assume for DirB we define\n*DirB_Owner* as a parent role with *DirB_Reader* and *DirB_Writer*\nchildren. We could then define another role, *AllDir_Reader*, with\n*DirA_Reader* and *DirB_Reader* children. Users assigned *AllDir_Reader*\nwould have read access to both DirA and DirB."}
{"filename": "security.md", "prompt": "Permissions", "completion": "In addition to checking whether a user has been granted a certain role,\nSK authorization can also be based on permissions. SK roles can contain\nzero or more permission strings. The syntax and semantics of these\npermissions are explained in the [Shiro\ndocumentation](http://shiro.apache.org/permissions.html). The\n[/usr/isPermitted](https://tapis-project.github.io/live-docs/?service=SK#operation/isPermitted)\nand related SK endpoints are called to determine if a user has a\npermission matching a required permission string.\n\nPermissions only exist inside roles. For convenience, SK implements\npermission endpoints that automatically apply to a user\\'s default role.\nSee the\n[/user/grantUserPermission](https://tapis-project.github.io/live-docs/?service=SK#operation/grantUserPermission)\nendpoint for details."}
{"filename": "security.md", "prompt": "Extended Permissions", "completion": "SK implements the full Shiro permission model and extends it to\naccommodate hierarchical resources such as file systems. For certain\nregistered permission schemas, the last component of a specification can\nbe treated as *extended path attribute*. Extended path attributes\nenhance the standard Shiro matching algorithm with one that treats\ndesignated components as hierarchical names, such as Posix file or\ndirectory path names. Consider, for example, permissions that conform to\nthe registered *files* schema:\n\n    SCHEMA:    files:<tenant>:<operation>:<systemId>:<path>\n    Examples:  files:tacc:read:mysystem:/home/bud/data\n               files:mytenant:read,write:mysystem:/home/mary/images\n\nWhen a user is assigned a role that contains the first example\npermission string, then that user is authorized to read files in the\n*/home/bud/data* directory subtree. A user assigned the second\npermission is authorized to read and write files in the\n*/home/mary/images* directory subtree.\n\nSK\\'s extended attribute permissions are used to maintain authorization\nto hierarchical resources *outside of those resources*. The need for\nexternalized authorization control arises, for instance, when a single\nservice account is used to access data on a system for multiple actual\nusers. In this case, the host system is always accessed using the same\naccount, but authorization needs to be carried out for different actual\nTapis users."}
{"filename": "security.md", "prompt": "Secrets", "completion": "SK uses HashiCorp [Vault](https://www.hashicorp.com/products/vault) as\nis backend database for storing and managing secrets. There is no direct\naccess to Vault for users or services\\--all access comes through SK. SK\nallows secrets to be created, read, versioned, deleted and destroyed by\nreflecting in its API the capabilities of Vault\\'s version 2\n[Key/Value](https://www.vaultproject.io/docs/secrets/kv/kv-v2) secrets\nengine.\n\nSK overlays Vault\\'s native capabilities with its own *typed secrets\nmodel*. The basic idea is that SK requires users to provide a\n*secretType* and *secretName* on most of its calls. Using this\ninformation, SK calculates the virtual paths (i.e., locations) in Vault\nbeing referenced. Users do not need to understand Vault\\'s naming scheme\nand SK has complete control of where secrets reside inside of Vault. The\nfollowing table lists the secret types supported by SK.\n\n>   Secret Type          Description\n>   -------------------- --------------------------------------------------------\n>   Service Password     Password used by services to acquire their JWTs\n>   JWT Signing Key      Tenant-specific JWT signing key used by Tokens service\n>   DB Credentials       Credentials used by services to access their databases\n>   System Credentials   Credentials for accessing Tapis systems\n>   User                 User secrets\n\nOnly the User secret type can be used by Tapis users; the rest are\nreserved for Tapis services only. Currently, SK only allows a single\nsecret to be referenced by each secretType/secretName combination.\nOtherwise, the full capabilities of the underlying Vault secrets engine\nis reflected in the SK [secrets\nAPI](https://tapis-project.github.io/live-docs/?service=SK#tag/vault)."}
{"filename": "sharing.md", "prompt": "Introduction to Tapis Access Controls", "completion": "Tapis users define resources such as systems, applications, files,\nstreams, functions and workflows. These resources can be used to\ngenerate other resources, such as job, workflow and function outputs.\nThe ability to share resources greatly extends their utility and, in\ngeneral, the usefulness of Tapis. On the other hand, users need to\nsafeguard their data by controlling access to resources. To facilitate\nboth sharing and access control, Tapis provides two built-in\nauthorization mechanisms.\n\nThe first facility is implemented using *roles and permissions*. These\ncontrols operate at a low level of abstraction and apply to the\nindividual Tapis resources to which they are associated. The second\nfacility, *Tapis shared resources* (or simply *shared resources*),\noperate at a higher level of abstraction by authorizing complex actions\nin Tapis. Such actions include running jobs, which authorizes access to\nan application, execution and archive system. In this case, the granting\nof a single share affects multiple Tapis resources. We explore both\nroles/permissions and shared resources in the following sections."}
{"filename": "sharing.md", "prompt": "Roles and Permissions", "completion": "The Security Kernel (SK) implements a distributed, role-based access\ncontrol (RBAC) [facility]() in which users are assigned roles that limit\nor allow access to resources. At its most basic, a role is simply a\nnamed entity with an owner. Tenant administrators can manage roles in\ntheir tenant and Tapis services can manage roles in any tenant.\nTypically, services only create roles in the administrative tenant at a\n[site]() (each site defines a restricted administrative tenant).\n\nAccess to specific resources is controlled by assigning roles to users.\nTapis supports [user-based]() APIs that check if a user has a certain\nrole. Services and other software that perform these checks are free to\ndetermine what membership in a role means.\n\nTo make managing user authorizations more flexible and convenient, a\nrole can contain zero or more other roles. A contained role is the\n*child* of the containing role. A child role can have any number of\n*parents*, that is, be contained in any number of other roles. Roles\nform a forest of directed acyclic graphs.\n\nWhen checking whether a user has been assigned a particular role, SK\nAPIs check whether the user has been assigned that role or any of its\nparent roles. When a user is assigned a role, the user is also\nimplicitly assigned all the children of that role.\n\nIn addition, roles can contain *permissions*, which are case-sensitive\nstrings that follow the format defined by Apache [Shiro](). The\n[permission-creation]() API adds a permission to a role. The\n[permission-checking]() API takes a required permission and determines\nif a user has a matching permission in any of its roles.\n\nBelow are examples of permissions enforced by the Tapis Systems service.\nThe first permission allows read/write access to *system1* in the\n*MyTenant* tenant. A user assigned a role that contains this permission\nwould have access to *system1*. Similarly, the second permission allows\nits assignees to create, read, write and delete any system in the\n*MyTenant* tenant.\n\n    system:MyTenant:read,write:system1\n    system:MyTenant:create,read,write,delete:*\n\nFor convenience, each user is automatically assigned a default role that\nis implicitly created by Tapis. Assigning a permission to a user really\nmeans adding the permission to the user\\'s default role."}
{"filename": "sharing.md", "prompt": "Implementations of Roles and Permissions", "completion": "Below are links to the roles and permissions APIs for each service. Also\nsee the roles and permissions discussions in each service\\'s\ndocumentation.\n\n-   [Systems-rbac]()\n-   [Applications-rbac]()\n-   [Files-rbac]()\n-   [Streams-rbac]()\n-   [Actors-rbac]()"}
{"filename": "sharing.md", "prompt": "Tapis Shared Resources", "completion": "The roles and permissions discussed in the last section allow\nfine-grained authorization to Tapis resources, such as systems or\napplications. Although the semantics of a role or permission can\nauthorize access to multiple resources of the same type, they cannot\neasily authorize access to all the resources encountered in a complex\nworkflow, such as job execution. To get a deeper understanding of the\nchallenge, consider the authorizations needed to run a job:\n\n1.  Read access to the application definition.\n2.  Read access to the execution system definition.\n3.  Read access to each job input\\'s storage system.\n4.  Read access to each job input\\'s file path or object ID.\n5.  Read/write access to the execution system\\'s input, output and\n    application staging directories.\n6.  Read access to the job\\'s archive system definition.\n7.  Write access to the job\\'s archive system\\'s archive path.\n\nIf a user simply wants to share an application with another user so that\nthe latter can execute it, many individual role or permission grants\nwould have to be put in place across multiple services.\n\nTo solve this problem, *Tapis sharing* grants some implicit access to\nall resources needed to perform an *action*. Tapis sharing defines a\nruntime *context* in which limited Tapis authorization is implicitly\ngranted to users performing an action. This context can span multiple\nservices. For example, if a file path is shared with a user, then access\nto the Tapis system on which the file resides is implicitly granted when\nthe user attempts access through the Files service.\n\nAn important aspect of sharing is that implicit access applies within a\ncertain context and does not apply outside of that context. In the case\nof a shared file path, for instance, read access to the required system\ndefinition is only valid when accessing that file path through the Files\nAPI. If the user tries to access the system definition directly through\nthe Systems API, the request will be rejected as unauthorized (assuming\nno other authorizations apply).\n\nAnother characteristic of sharing is that implicit authorizations apply\nonly to Tapis resources. In the file path sharing scenario, the system\ndefinition that is part of the shared context is an artifact defined\nwithin and under the control of Tapis. Tapis is the controlling agent.\nAccess to the path itself, however, is ultimately under the control of\nthe file system on which the path resides, such as a Posix file system.\nTapis sharing has no effect on authorizations enforced by external\nsystems. For example, a user could share a path in their home directory,\nbut unless the grantee already has Posix access to that path, requests\nto access it will be denied by the operating system.\n\nIn addition to sharing resources with individual users, Tapis sharing\nalso supports granting public access to resources. The *public-grantee*\naccess designation allows all users in a tenant access to the shared\nresource. Where supported, the *public-grantee-no-authn* access\ndesignation grants access to all users, even those that have not\nauthenticated with Tapis. See individual service documentation for\ndetails on public access support."}
{"filename": "sharing.md", "prompt": "Shared Application Contexts (SACs)", "completion": "The concept of a *Shared Application Context (SAC)* recognizes that\napplications run in the context of a Tapis job. This context is\nleveraged by multiple, cooperating services to allow limited implicit\naccess to all the resources needed to run a job. In this case, the term\n*limited implicit access* means that for certain resources, the user\nrunning the job will have the application owner\\'s authorizations in\naddition to their own. Specifically, users are able to access systems\nand file paths which they cannot normally access but the application\nowner can access.\n\nWhen a job runs in a SAC, services grant this *limited implicit access*\nfor **resources explicitly specified in the application definition**.\nImportant characteristics of a SAC are:\n\n1.  \n\n    The SAC-aware services are Systems, Applications, Jobs and Files.\n\n    :   a)  These services know when they are running in a SAC and how\n            to alter their behavior.\n\n2.  \n\n    SAC-aware services grant implicit access only during Job execution of a shared application.\n\n    :   a)  Users are not conferred any special privileges on\n            application-specified resources outside of job execution.\n        b)  Relaxed authorization checking applies only to systems and\n            file paths referenced in the application definition.\n\n3.  \n\n    SSH authentication to a host is not affected by SAC processing.\n\n    :   a)  The Tapis system definition still determines the credentials\n            used to login to a host.\n        b)  The host operating system still authorizes access to host\n            resources.\n\n4.  \n\n    File system and object store authorization is not affected by SAC processing.\n\n    :   a)  The authenticated user must still be authorized by the\n            persistent storage systems.\n\nIn summary, a user can share an application with another user and the\nTapis file and system resources referenced in the application definition\nare also implicitly shared. This implicit sharing is implemented by\nsupplementing the requesting user\\'s authorizations with the application\nowner\\'s authorizations for these resources (and only these resources).\nThe underlying operating systems\\' and persistent storage systems\\'\nauthentication and authorization mechanisms are unchanged, so users have\nno more low-level access than they would otherwise. Tapis simply relaxes\nits access constraints *during job execution*, but all host\nauthorizations are still enforced."}
{"filename": "sharing.md", "prompt": "SAC-Eligible Attributes", "completion": "The following attributes of application definitions are SAC-eligible,\nmeaning that implicit access to the resources they designate can be\ngranted to jobs running in a SAC.\n\n1.  execSystemId\n2.  execSystemExecDir\n3.  execSystemInputDir\n4.  execSystemOutputDir\n5.  archiveSystemId\n6.  archiveSystemDir\n7.  fileInputs sourceUrl\n8.  fileInputs targetPath\n\nIf an execution system, for instance, is specified in a shared\napplication definition, *and that system is not overridden in the job\nsubmission request*, then jobs running in a SAC will be granted implicit\naccess to the system\\'s definition. The same is true for the other\nSAC-eligible attributes: If their values are specified in the\napplication and those values are not overridden when a job is submitted,\nTapis implicitly grants access to the designated Tapis resource."}
{"filename": "sharing.md", "prompt": "Implementations of Tapis Sharing", "completion": "Below are links to the sharing APIs for each service. Also see the\nsharing discussions in each serivce\\'s documentation.\n\n-   [Systems-Sharing]()\n-   [Applications-Sharing]()\n-   [Files-Sharing]()\n-   [Jobs-Sharing]()"}
{"filename": "streams.md", "prompt": "Projects", "completion": "Projects are defined at a top level in the hierarchy of Streams\nresources. A user registers a project by providing metadata information\nsuch as the principal Investigator, project URL, funding resource, etc.\nA list of authorized users can be added to various project roles to have\na controlled access over the project resources. When a project is first\nregistered, a collection is created in the back-end MongoDB. User\npermissions to access this collection are then set up in the security\nkernel. Every request to access the project resource or documents within\n(i.e sites, instruments, variables) goes through a security kernel check\nand only the authorized user requests are allowed to be processed."}
{"filename": "streams.md", "prompt": "**Create Project**", "completion": "With PySDK:\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::\n\n|"
{"filename": "streams.md", "prompt": "**List Projects**", "completion": "With PySDK:\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::\n\n|"
{"filename": "streams.md", "prompt": "**Get Project Details**", "completion": "With PySDK:\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::\n\n|"
{"filename": "streams.md", "prompt": "**Update Project**", "completion": "With PySDK:\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::"
{"filename": "streams.md", "prompt": "**Delete Project**", "completion": "With PySDK:\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::\n\n|"
{"filename": "streams.md", "prompt": "Sites", "completion": "Site is a geographical location that may hold one or more instruments.\nSites are next in the streams hierarchy and they inherit permissions\nfrom the projects. Project owners can create sites by providing the\ngeographical information such as latitude, longitude and elevation of\nthe site or GeoJSON encoded spatial information. This spatial\ninformation is useful when searching sites or data based on location. In\nthe back-end database a site is represented as a JSON document within\nthe project collection. Site permissions are inherited from the project."}
{"filename": "streams.md", "prompt": "**Create Site**", "completion": "With PySDK:\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::\n\n|"
{"filename": "streams.md", "prompt": "**List Sites**", "completion": "With PySDK:\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::\n\n|"
{"filename": "streams.md", "prompt": "**Get Site Details**", "completion": "With PySDK:\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\\$ t.streams.get_site(project_id=\\'tapis_demo_project_testuser6\\',\nsite_id=\\'tapis_demo_site\\')\n\n\n:::\n\n|"
{"filename": "streams.md", "prompt": "**Update Site**", "completion": "With CURL:\n\n\n\nWith PySDK\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::"
{"filename": "streams.md", "prompt": "**Delete Site**", "completion": "With CURL:\n\n\n\nWith PySDK\n\n\n\n|"}
{"filename": "streams.md", "prompt": "Instruments", "completion": "Instruments are physical entities that may have one or more embedded\nsensors to sense various parameters such as temperature, relative\nhumidity, specific conductivity, etc. These sensors referred to as\nvariables in Streams API generate measurements, which are stored in the\ninfluxDB along with a ISO8601 timestamp. Instruments are associated with\nspecific sites and projects. Information about the instruments such as\nsite and project ids, name and description of the instrument, etc. are\nstored in the mongoDB sites JSON document."}
{"filename": "streams.md", "prompt": "**Create Instrument**", "completion": "With PySDK\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::\n\n|"
{"filename": "streams.md", "prompt": "**List Instruments**", "completion": "With PySDK\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::\n\n|"
{"filename": "streams.md", "prompt": "**Get instrument Details**", "completion": "With PySDK\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::\n\n|"
{"filename": "streams.md", "prompt": "**Update Instrument**", "completion": "With PySDK\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::"
{"filename": "streams.md", "prompt": "**Delete Instrument**", "completion": "With PySDK\n\n\n\nWith CURL:\n\n\n\n|"}
{"filename": "streams.md", "prompt": "Variables", "completion": ""}
{"filename": "streams.md", "prompt": "**Create Variables**", "completion": "With PySDK\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::\n\n|"
{"filename": "streams.md", "prompt": "**List Variables**", "completion": "With PySDK\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::\n\n|"
{"filename": "streams.md", "prompt": "**Get Variable Details**", "completion": "With PySDK\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::\n\n|"
{"filename": "streams.md", "prompt": "**Update Variable**", "completion": "With PySDK\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::"
{"filename": "streams.md", "prompt": "**Delete Variable**", "completion": "With PySDK\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::\n\n|"
{"filename": "streams.md", "prompt": "Measurements", "completion": ""}
{"filename": "streams.md", "prompt": "**Create Measurements**", "completion": "With PySDK\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::\n\n|"
{"filename": "streams.md", "prompt": "**List Measurements**", "completion": "With PySDK\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::\n\n|"
{"filename": "streams.md", "prompt": "Channels", "completion": ""}
{"filename": "streams.md", "prompt": "**Channel Alert Types**", "completion": "::: tabs\n.. tab:: Threshold Check\n\n+--------+----------+------------------+-----------------------------+\n| Att    | Type     | Example          | Note                        |\n| ribute |          |                  |                             |\n+========+==========+==================+=============================+\n| key    | String   | \\\"m              | should be in the format of  |\n|        |          | y_instrument_id. | \\<inst                      |\n|        |          | my_variable_id\\\" | rument_id\\>.\\<varaible_id\\> |\n+--------+----------+------------------+-----------------------------+\n| op     | String   | \\\"\\>\\\", \\\"\\<\\\"   | (default is \\\"\\<\\\")         |\n| erator |          |                  |                             |\n+--------+----------+------------------+-----------------------------+\n| val    | Integer  | > 3              |                             |\n+--------+----------+------------------+-----------------------------+\n\nRefer to\n<https://docs.influxdata.com/influxdb/v2.0/api/#operation/CreateCheck>\nfor more infomation of each attribute parameter\n\n::: tab\nDeadman Check\n:::\n\n  --------------------------------------------------------------------------------------------------\n  Attribute     Type      Example                               \n  ------------- --------- ------------------------------------- ------------------------------------\n  key           String    \\\"my_instrument_id.my_variable_id\\\"   should be in the format of\n                                                                \\<instrument_id\\>.\\<varaible_id\\>\n\n  time_since    String    \\\"3s\\\"                                \\*refer to [InfluxDB]() for more\n                                                                info on valid duration type\n\n  stale_time    String    \\\"1m\\\"                                \\*(Optional) If value is not\n                                                                provided, stale_time will be\n                                                                calculated to produce a max of 20\n                                                                action\n\n  report_zero   Boolean   True, False                           \\*(Optional) If only zero values\n                                                                reported since time, trigger an\n                                                                alert\n\n  every         String    \\\"1m\\\"                                \\*(Optional) If value is not\n                                                                provided, default will be \\\"10s\\\"\n\n  offset        String    \\\"1m\\\"                                \\*(Optional) If value is not\n                                                                provided, default will be \\\"0s\\\"\n  --------------------------------------------------------------------------------------------------\n\nRefer to\n<https://docs.influxdata.com/influxdb/v2.0/api/#operation/CreateCheck>\nfor more infomation of each attribute parameter\n:::"}
{"filename": "streams.md", "prompt": "**Channel Actions**", "completion": "The type of actions that a channel should be performed can be defined\nunder the action parameter within a triggers_with_actions object.\n\n::: tabs\n.. tab:: Actor\n\n  -----------------------------------------------------------------------\n  Attribute    Type           Example\n  ------------ -------------- -------------------------------------------\n  method       String         \\\"ACTOR\\\"\n\n  actor_id     String         \\\"my_actor\\\"\n\n  message      String         \\\"X instrument have exceeded the threshold\n                              of Y\\\"\n  -----------------------------------------------------------------------\n\n::: tab\nJob\n:::\n\n  -----------------------------------------------------------------------\n  Attribute    Type           Example\n  ------------ -------------- -------------------------------------------\n  method       String         \\\"JOB\\\"\n\n  job_param    Object         see [Jobs]() section for more information\n  -----------------------------------------------------------------------\n\n::: tab\nSlack\n:::\n\n  --------------------------------------------------------------------------\n  Attribute     Type           Example\n  ------------- -------------- ---------------------------------------------\n  method        String         \\\"SLACK\\\"\n\n  webhook_url   String         \\\"<https://hooks.slack.com/services/XXXX>\\\"\n\n  message       String         \\\"X instrument have exceeded the threshold of\n                               Y\\\"\n  --------------------------------------------------------------------------\n\n::: tab\nDiscord\n:::\n\n  --------------------------------------------------------------------------\n  Attribute     Type           Example\n  ------------- -------------- ---------------------------------------------\n  method        String         \\\"DISCORD\\\"\n\n  webhook_url   String         \\\"<https://discord.com/api/webhooks/XXXX>\\\"\n\n  message       String         \\\"X instrument have exceeded the threshold of\n                               Y\\\"\n  --------------------------------------------------------------------------\n\n::: tab\nHTTP Webhook\n:::\n\n  ---------------------------------------------------------------------------\n  Attribute     Type           Example\n  ------------- -------------- ----------------------------------------------\n  method        String         \\\"WEBHOOK\\\"\n\n  webhook_url   String         \\\"<https://api.services.com/webhooks/XXXX>\\\"\n\n  data_field    String         \\\"content\\\", \\\"text\\\", \\\"message\\\" (field name\n                               for webhook message)\n\n  message       String         \\\"X instrument have exceeded the threshold of\n                               Y\\\"\n  ---------------------------------------------------------------------------\n:::"}
{"filename": "streams.md", "prompt": "**Create Channels**", "completion": "With PySDK\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::"
{"filename": "streams.md", "prompt": "**List Channels**", "completion": "With PySDK\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::\n\n|"
{"filename": "streams.md", "prompt": "**Get Channel Details**", "completion": "With PySDK\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::\n\n|"
{"filename": "streams.md", "prompt": "**Update Channels**:", "completion": "With PySDK\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::\n\n|"
{"filename": "streams.md", "prompt": "**Update Channels Status**", "completion": "With PySDK\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::\n\n|"
{"filename": "streams.md", "prompt": "Templates", "completion": ""}
{"filename": "streams.md", "prompt": "**Create Template**", "completion": "With PySDK\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::\n\n|"
{"filename": "streams.md", "prompt": "**List Templates**", "completion": "With PySDK\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::\n\n|"
{"filename": "streams.md", "prompt": "**Get Template Details**", "completion": "With PySDK\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::\n\n|"
{"filename": "streams.md", "prompt": "**Update Template**", "completion": "With PySDK\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::\n\n|"
{"filename": "streams.md", "prompt": "Alerts", "completion": ""}
{"filename": "streams.md", "prompt": "**List Alerts**", "completion": "With PySDK\n\n\n\nWith CURL:\n\n\n\nThe response will look something like the following:\n\n::: {.container .foldable}
\n\n:::\n\n|"
{"filename": "streams.md", "prompt": "Roles", "completion": "Streams service uses **roles** to manage permissions on the streams\nresources. CRUD operations on Streams resources such as Sites,\nInstruments and Variables can be performed by authorized users having a\nspecific role on the Project. Similarly CRUD operations on Channels and\nTemplates can be done by authorized users having specific roles. Streams\nservice supports three types of roles: *admin*, *manager* and *user*.\n\n**Admin** has elevated privileges. An *admin* can create, update, or\ndelete any of the Streams resources.\n\n**Manager** can perform all read and write operations on Streams\nresources, with an exception of deleting them.\n\n**User** can only perform read operations on the resources and are not\nauthorized to write or delete the resources.\n\nTable 1 below summarizes the authorized actions with respect to user\nroles.\n\n+---------------------+----------------+--------------------+\n| Role                | > Request      |                    |\n|                     | > permitted    |                    |\n+=====================+================+====================+\n| admin               | > GET, PUT,    |                    |\n|                     | > POST, DELETE |                    |\n+---------------------+----------------+--------------------+\n| manager             | > GET, PUT,    |                    |\n|                     | > POST         |                    |\n+---------------------+----------------+--------------------+\n| user                | > GET          |                    |\n+---------------------+----------------+--------------------+\n\nWhen a user creates project, channel or template, an admin role of the\nform: **streams_projects\\_\\$project-oid_admin**,\n**streams_channels\\_\\$channel-oid_admin** or\n**streams_templates\\_\\$template-oid_admin**, respectively is created in\nthe Security Kernel and is assigned to the requesting (JWT) user. Oid is\nthe unique object id generated by the backend MongoDB for each of the\nProject, Channel or Template.\n\nAdmins can further grant roles such as **admin**, **manager** or\n**user** for other users listed on the project. To perform CRUD\noperations on Projects, Sites, Instruments and Variables, users must\nhave appropriate role on the Project. To perform CRUD operation on\neither Channels and Templates, users must have role associated with each\nof the resources."}
{"filename": "streams.md", "prompt": "**List Roles**", "completion": "To get the list of user roles on a project, channel or template, the\nrequesting(JWT) user must provide following three parameters:\n\n> 1)  **resource_type** : project/channel/template\n> 2)  **resource_id**: project_id/channel_id/template_id\n> 3)  **user**: username for whom roles are to be checked\n\nIn order to list the user roles on a resource (Project, Channel,\nTemplate) the requesting(JWT) user must have one of the three roles\n(admin, manager, user) on it.\n\nWith PySDK\n\n\n\nWith CURL:\n\n\n\nThe response will look like the following with the Python Client:\n\n::: {.container .foldable}
\n\n:::\n\n| \n\nThere are three possible responses depending on if the requesting(JWT)\nuser and user specified in query parameters are same or different.\n\nCase I: When requesting(JWT) user and user specified in the query\nparameters are same and both have role on the project/channel/template\nThe result will include all the roles for the user in query parameters\nfor the given resource_id\n\n::: {.container .foldable
\n\n:::\n\nCase II: When requesting(JWT) user and user specified in the query\nparameters are different and JWT user does not have any role on the\nproject/channel/template\n\n::: {.container .foldable
\n\n:::\n\nCase III: When requesting(JWT) user and user specified in the query\nparameters are different. JWT user has role on the project and user in\nquery parameter does not have role on the project/channel/template\n\n::: {.container .foldable
\n\n:::"
{"filename": "streams.md", "prompt": "**Grant Roles**", "completion": "Roles can be granted by Project/Channel/Template *admins* or *managers*\nso that users can perform CRUD operations on the Streams resources.\n\nTable 2 below shows that *admin* can grant any of the three roles to\nother users. Same or lower level permissions can be granted by *admins*\nand *managers*. Self role granting is not permitted.\n\nManagers can only grant *manager* and *user* to other users.\n\nUsers do **not** have privileges to grant roles.\n\n-   Roles of the requesting(JWT) user are first checked by querying SK.\n-   If the username provided in the request body is the same as the JWT\n    user, then self role granting is not permitted.\n-   If the JWT user and user provided in the request body are different,\n    then existing roles for the username provided in the request body\n    are retrieved and if the user already has the role, JWT user user is\n    asking for no action is taken.\n-   If the role does not exist then JWT user roles are retrieved and\n    compared with the rolename provided in the request body. Role is\n    granted only if the JWT user has **same** or **higher** roles than\n    the role name specified in the request body (*admin* role has\n    highest rank, followed by *manager* and then *user*). Otherwise an\n    error message saying, *User not authorized to grant role* is given\n    in the response.\n\n+---------------------+------------------------+\n| Role                | Grant                  |\n+=====================+========================+\n| admin               | > admin, manager, user |\n+---------------------+------------------------+\n| manager             | > manager, user        |\n+---------------------+------------------------+\n| user                | > cannot grant roles   |\n+---------------------+------------------------+\n\nWith PySDK\n\n\n\nWith CURL:\n\n\n\nThe response will vary based on following cases:\n\nCase I: If the username provided in the request body is the same as the\nJWT user, then self role granting is not permitted.\n\nWith PySDK\n\n\n\n::: {.container .foldable}
\n\n:::\n\nCase II: If the JWT user and username provided in the request body are\ndifferent, then existing roles for the username provided in the request\nbody are retrieved and if the user already has the role JWT user user is\nasking for, no action is taken.\n\nWith PySDK\n\n\n\n::: {.container .foldable
\n\n:::\n\nCase III: If the role does not exist then JWT user roles are retrieved\nand compared with the rolename provided in the request body. Role is\ngranted only if the JWT user has **same** or **higher**\\* roles than the\nrole name specified in the request body. Otherwise an error message\nsaying, \"User not authorized to grant role\" is given in the response.\n\nFor example testuser4 has **\\*manager** role on the project and the\nrequest is to grant testuser5 **admin**\\* role, then the request will\nnot be fulfilled.\n\n\n\n::: {.container .foldable
\n\n:::\n\nIf the requesting (JWT) user only has a **user**\\* role, then no role\ncan be granted to other users, and the response will be following\n\n::: {.container .foldable
\n\n:::\n\nCase IV: If the requesting (JWT) user has no role on the\nproject/channel/template, then the user is not authorized to grant any\nroles\n\n::: {.container .foldable
\n\n:::"
{"filename": "streams.md", "prompt": "**Revoke Roles**", "completion": "Users in **admin** role are capable of revoking any of the three roles:\n**admin**, **manager** and **user** for other users. Self role revoking\nis not permitted. Users in *manager* and *user* role are not capable of\nrevoking roles.\n\n+---------------------+------------------------+\n| Role                | Revoke                 |\n+=====================+========================+\n| admin               | > admin, manager, user |\n+---------------------+------------------------+\n| manager             | > Cannot revoke role   |\n+---------------------+------------------------+\n| user                | > Cannot revoke role   |\n+---------------------+------------------------+\n\nWith PySDK\n\n\n\nWith CURL:\n\n\n\nThe response will be following:\n\n::: {.container .foldable}
\n\n:::\n\nResponses will vary based on following cases:\n\nCase I: If the requesting (JWT) user has a **manager** or **user** role\n\n::: {.container .foldable
\n\n:::\n\nCase II: If the JWT user is trying to revoke self role\n\n::: {.container .foldable
\n\n:::"
{"filename": "systems.md", "prompt": "Overview", "completion": "A Tapis system represents a server or collection of servers exposed\nthrough a single host name or IP address. Each system is associated with\na specific tenant. A system can be used for the following purposes:\n\n-   Running a job, including:\n    -   Staging files to a system in preparation for running a job.\n    -   Executing a job on a system.\n    -   Archiving files and data on a remote system after job execution.\n-   Storing and retrieving files and data.\n\nEach system is of a specific type (such as LINUX or S3) and owned by a\nspecific user who has special privileges for the system. The system\ndefinition also includes the user that is used to access the system,\nreferred to as *effectiveUserId*. This access user can be a specific\nuser (such as a service account) or dynamically specified as\n`${apiUserId}
`. For the case of `${apiUserId
`, the username is\nextracted from the identity associated with the request to the service."
{"filename": "systems.md", "prompt": "Model", "completion": "At a high level a system represents the following information:\n\n*id*\n\n:   A short descriptive name for the system that is unique within the\n    tenant.\n\n*description*\n\n:   An optional more verbose description for the system.\n\n*systemType* - Type of system\n\n:   LINUX, S3, IRODS or GLOBUS\n\n*owner*\n\n:   A specific user set at system creation. By default this is the\n    resolved value for *\\${apiUserId}
*, the user making the request to\n    create the system.\n\n*host* - Host name, IP address or Globus ID\n\n:   FQDN, IP address, Globus endpoint ID or Globus collection ID.\n\n*enabled* - Enabled flag\n\n:   Indicates if system is currently considered active and available for\n    use. By default this is *true*.\n\n*effectiveUserId* - Effective User\n\n:   The username to use when accessing the system. A specific user (such\n    as a service account) or the dynamic user `${apiUserId
`. By default\n    this is `${apiUserId
`.\n\n*defaultAuthnMethod* - Default authentication method\n\n:   How access authentication is handled by default. Authentication\n    method can also be specified as part of a request. Supported\n    methods: PASSWORD, PKI_KEYS, ACCESS_KEY, TOKEN.\n\n*bucketName* - Bucket name\n\n:   For an S3 system this is the name of the bucket.\n\n*rootDir* - Effective root directory\n\n:   Directory to be used when listing files or moving files to and from\n    the system. For LINUX and IRODS this is required and must begin with\n    `/`. For S3 this is optional and typically will not begin with `/`.\n    May not be updated. Contact support to request a change.\n\n*dtnSystemId* - DTN system Id\n\n:   An alternate system to use as a Data Transfer Node (DTN) during job\n    execution. The execution system and the DTN system must have shared\n    storage.\n\n*dtnMountPoint* - DTN mount point\n\n:   Mount point (aka target) used when running the mount command on this\n    system. During job execution this is the path on this system for\n    files transferred to *rootDir* on *dtnSystemId*.\n\n*dtnMountSourcePath* - DTN mount source path\n\n:   The path exported by *dtnSystemId* that matches the *dtnMountPoint*\n    on this system. This will be relative to *rootDir* on *dtnSystemId*.\n    Used during job execution.\n\n*isDtn*\n\n:   Indicates if system will be used as a data transfer node (DTN). If\n    this is *true* then *rootDir* is required, *canExec* must be false\n    and following may not be specified: *dtnSystemId*,\n    *dtnMountSourcePath*, *dtnMountPoint* and all job execution related\n    attributes. By default this is *false*.\n\n*canExec*\n\n:   Indicates if system can be used to execute jobs.\n\n*canRunBatch*\n\n:   Indicates if system supports running jobs using a batch scheduler.\n    By default this is *false*.\n\n*enableCmdPrefix*\n\n:   Indicates if system allows a job submission request to specify a\n    *cmdPrefix*. Since *cmdPrefix* is a free form command it is a\n    security concern. By default this is *false*.\n\nJob related attributes\n\n:   Various attributes related to job execution such as *jobRuntimes*,\n    *jobWorkingDir*, *batchScheduler*, *batchLogicalQueues*\n\nWhen creating a system the required attributes are: *id*, *systemType*,\n*host*, *defaultAuthnMethod* and *canExec*. Depending on the type of\nsystem and specific values for certain attributes there are other\nrequirements.\n\nNote that a system may be created as a storage-only resource\n(*canExec=false*) or as a system that can be used for both execution and\nstorage (*canExec=true*)."
{"filename": "systems.md", "prompt": "Getting Started", "completion": "Before going into further details about Systems, here we give some\nexamples of how to create and view systems. In the examples below we\nassume you are using the TACC tenant with a base URL of `tacc.tapis.io`\nand that you have authenticated using PySDK or obtained an authorization\ntoken and stored it in the environment variable JWT, or perhaps both."}
{"filename": "systems.md", "prompt": "Creating a System", "completion": "Create a local file named `system_example.json` with json similar to the\nfollowing:\n\n    {\n      \"id\":\"tacc-sample-<userid>\",\n      \"description\":\"My storage system\",\n      \"host\":\"tapis-vm.tacc.utexas.edu\",\n      \"systemType\":\"LINUX\",\n      \"defaultAuthnMethod\":\"PKI_KEYS\",\n      \"effectiveUserId\":\"${apiUserId}
\",\n      \"rootDir\":\"/\",\n      \"canExec\": false\n    
\n\nwhere *\\<userid\\>* is replaced with your username, and your host name is\nupdated appropriately. Note that although credentials may be included in\nthe definition we have not done so here. For security reasons, it is\nrecommended that login credentials be updated using a separate API call\nas discussed below.\n\nUsing PySDK:\n\n\n\nUsing CURL:\n\n    $ curl -X POST -H \"content-type: application/json\" -H \"X-Tapis-Token: $JWT\" https://tacc.tapis.io/v3/systems -d @system_example.json"
{"filename": "systems.md", "prompt": "Registering Credentials for a System", "completion": "Now that you have registered a system you will need to register\ncredentials so you can use Tapis to access the host. Various\nauthentication methods can be used to access a system, such as PASSWORD,\nPKI_KEYS and TOKEN. Note that the TOKEN authentication method is for\nsystems of type GLOBUS. Registering credentials for a GLOBUS type system\nis a special case that involves steps different from those described in\nthis section. Please see the section below on [Registering Credentials\nfor a Globus System](#registering-credentials-for-a-globus-system) for\nmore information.\n\nHere we will cover registering PKI_KEYS (i.e. ssh keys) as an example.\n\nCreate a local file named `cred_tmp.json` with json similar to the\nfollowing:\n\n    {\n      \"publicKey\": \"<ssh_public_key>\",\n      \"privateKey\": \"<ssh_private_key>\"\n    }
\n\nwhere *\\<ssh_public_key\\>* and *\\<ssh_private_key\\>* are replaced with\nyour keys. The keys must be encoded on a single line with embedded\nnewline characters. You may find the following linux command useful in\nconverting a multi-line private key into a single line:\n\n    cat $privateKeyFile | awk -v ORS='\\\\n' '1'\n\nUsing PySDK:\n\n\n\nUsing CURL:\n\n    $ curl -X POST -H \"content-type: application/json\" -H \"X-Tapis-Token: $JWT\" https://tacc.tapis.io/v3/systems/credential/tacc-sample-<userid>/user/<userid> -d @cred_tmp.json\n\nAn optional attribute *loginUser* may be included in the request body in\norder to map the Tapis user to a username to be used when accessing the\nsystem. If the login user is not provided then there is no mapping and\nthe Tapis user is always used when accessing the system. When a\n*loginUser* is provided the json would be similar to the following:\n\n    {\n      \"publicKey\": \"<ssh_public_key>\",\n      \"privateKey\": \"<ssh_private_key>\",\n      \"loginUser\": \"<linux_host_username>\"\n    
\n\nNote that credentials are stored in the Security Kernel. Only specific\nTapis services are authorized to retrieve credentials."
{"filename": "systems.md", "prompt": "Use of PKI_KEYS as credentials", "completion": "When using an ssh keypair as credentials there are several important\npoints to keep in mind. As discussed above, the public key and private\nkey must be encoded on a single line. This can sometimes be challenging.\nFor example, copying and pasting may convert newline characters in a way\nthat is not compatible with processing in Tapis. You may find the\nfollowing linux command useful in converting a multi-line private key\ninto a single line:\n\n    cat $privateKeyFile | awk -v ORS='\\\\n' '1'\n\nAlso, Tapis does not currently support OPENSSH type keys. After\ngenerating the keypair, please inspect the first few lines of the\nprivate key file and confirm that it is not of type OPENSSH. Typically,\na valid private key file will start with the line\n`-----BEGIN RSA PRIVATE KEY-----`. If your private key is of type\nOPENSSH please use a command similar to the following to generate your\nkeypair:\n\n    ssh-keygen -t rsa -b 4096 -m PEM\n\nWhen generating the keypair, do not use a passphrase. This can interfere\nwith non-interactive use of the keypair.\n\nFinally, please be aware that if the host has multi-factor\nauthentication (MFA) enabled this may prevent Tapis from communicating\nwith the host. Tapis does not currently support MFA.\n\nWhen encountering problems here are some suggestions on what to check:\n\n-   Public and private keys are each on one line in the json file.\n    Newline characters in private key are properly encoded.\n-   Keypair is not of type OPENSSH\n-   Keypair does not have a passphrase\n-   Public key has been added to the authorized_keys file for the target\n    user. File \\~/.ssh/authorized_keys\n-   File \\~/.ssh/authorized_keys has proper permissions.\n-   MFA is not enabled for the target host.\n\nIf problems persist you can also attempt to manually validate the\nkeypair using a command similar to this:\n\n    ssh -i /tmp/my_private_key testuser@myhost.com\n\nwhere /tmp/my_private_key contains the original multi-line private key.\nIf everything is set up correctly and the keypair is valid you should be\nlogged into the host without being prompted for a password."}
{"filename": "systems.md", "prompt": "Registering Credentials for a Globus System", "completion": "Registering credentials for a GLOBUS type system is a special case that\ninvolves steps different from those described in the section above. For\na GLOBUS type system, the user will need to use the TOKEN authentication\nmethod and generate an `accessToken` and `refreshToken` using two\nspecial-purpose System service endpoints.\n\nPlease note that your Tapis site installation must have been configured\nby the site administrator to support Globus. Please see\n[Globus_Config](https://tapis.readthedocs.io/en/latest/deployment/deployer.html#configuring-support-for-globus)."}
{"filename": "systems.md", "prompt": "Obtain Globus Authorization Code", "completion": "The first step in generating Globus credentials is for the user to call\nthe systems *authUrl* credential endpoint to obtain a Globus\nauthorization code.\n\nUsing CURL, the request would look something like this:\n\n    $curl -H \"X-Tapis-Token: $JWT\" https://dev.tapis.io/v3/systems/credential/globus/authUrl\n\nThe response should look similar to the following. Note that for brevity\nand readability, only the result portion of the response is shown, the\nresponse has been split into multiple lines and various IDs are not\nfilled in:\n\n    {\n      \"url\": \"https://auth.globus.org/v2/oauth2/authorize?client_id=<client_id>\n          &redirect_uri=https%3A%2F%2Fauth.globus.org%2Fv2%2Fweb%2Fauth-code\n          &scope=openid+profile+email+urn%3Aglobus%3Aauth%3Ascope%3Atransfer.api.globus.org%3Aall\n          &state=_default&response_type=code&code_challenge=<challenge_id>\n          &code_challenge_method=S256&access_type=offline\",\n      \"sessionId\": \"<session_id>\"\n    }
\n\nThe user should copy the url (as a single string, no line breaks) and\nmake note of the session Id for later use. The user then visits the\nprovided URL and is presented with a Globus logon page that will allow\nthem to authenticate using one of thousands of supported identity\nproviders, including through their existing organization using CILogon.\n\nThe user must use the following flow to obtain an authorization code:\n\n1.  Visit the provided URL and authenticate through Globus. After\n    authentication, user is re-directed back to a Globus page showing\n    the access being requested by Tapis.\n2.  Fill in a label for future reference and click *Allow* to authorize\n    Tapis to access Globus on their behalf.\n3.  Copy the provided authorization code in preparation for the final\n    step. Note that the code is valid for a short time (as of this\n    writing it is valid for 10 minutes)."
{"filename": "systems.md", "prompt": "Exchange Authorization Code for Tokens", "completion": "The final step is for the user to call the systems credential endpoint\nto exchange the authorization code and session ID for tokens which are\nstored by the Systems service in a credentials record.\n\nUsing CURL, the request would look something like this:\n\n    $curl -X POST -H \"content-type: application/json\" -H \"X-Tapis-Token: $JWT\"\n           https://dev.tapis.io/v3/systems/credential/<system>/user/<user>/globus/tokens/<authCode>/<sessionId>\n\nThe response should look similar to the following:\n\n    {\n      \"result\": null,\n      \"status\": \"success\",\n      \"message\": \"SYSAPI_CRED_UPDATED Credential updated. ...\",\n      \"version\": \"1.3.1\",\n      \"commit\": \"619aa7ce\",\n      \"build\": \"2023-04-02T19:06:38Z\",\n      \"metadata\": null\n    }
\n\nAt this point the user will have registered credentials for a Tapis\nsystem that can be used as a source or destination for Globus\noperations."
{"filename": "systems.md", "prompt": "Viewing Systems", "completion": ""}
{"filename": "systems.md", "prompt": "Retrieving details for a system", "completion": "To retrieve details for a specific system, such as the one above:\n\n::: note\n::: title\nNote\n:::\n\nSee the section below on [Selecting](#selecting) to find out how to\ncontrol the amount of information returned.\n:::\n\nUsing PySDK:\n\n\n\nUsing CURL:\n\n    $ curl -H \"X-Tapis-Token: $JWT\" https://tacc.tapis.io/v3/systems/tacc-sample-<userid>\n\nThe response should look similar to the following:\n\n    {\n       \"result\": {\n           \"tenant\": \"dev\",\n           \"id\": \"tacc-sample-<userid>\",\n           \"description\": \"My storage system\",\n           \"systemType\": \"LINUX\",\n           \"owner\": \"<userid>\",\n           \"host\": \"tapis-vm.tacc.utexas.edu\",\n           \"enabled\": true,\n           \"effectiveUserId\": \"<userid>\",\n           \"defaultAuthnMethod\": \"PKI_KEYS\",\n           \"authnCredential\": null,\n           \"rootDir\": \"/\",\n           \"port\": 22,\n           \"useProxy\": false,\n           \"proxyHost\": \"\",\n           \"proxyPort\": -1,\n           \"dtnSystemId\": null,\n           \"dtnMountPoint\": null,\n           \"dtnMountSourcePath\": null,\n           \"isDtn\": false,\n           \"canExec\": false,\n           \"canRunBatch\": false,\n           \"enableCmdPrefix\": false,\n           \"jobRuntimes\": [],\n           \"jobWorkingDir\": null,\n           \"jobEnvVariables\": [],\n           \"jobMaxJobs\": 2147483647,\n           \"jobMaxJobsPerUser\": 2147483647,\n           \"batchScheduler\": null,\n           \"batchLogicalQueues\": [],\n           \"batchDefaultLogicalQueue\": null,\n           \"jobCapabilities\": [],\n           \"tags\": [],\n           \"notes\": {}
,\n           \"uuid\": \"f83606bf-7a1a-4ff0-9953-dd732cc07ac0\",\n           \"deleted\": false,\n           \"created\": \"2021-04-26T18:45:40.771Z\",\n           \"updated\": \"2021-04-26T18:45:40.771Z\"\n       
,\n       \"status\": \"success\",\n       \"message\": \"TAPIS_FOUND System found: tacc-sample-<userid>\",\n       \"version\": \"0.0.1\",\n       \"metadata\": null\n    
\n\nNote that authnCredential is *null*. Only specific Tapis services are\nauthorized to retrieve credentials."
{"filename": "systems.md", "prompt": "Retrieving details for all systems", "completion": "To see the list of systems that you own:\n\nUsing PySDK:\n\n\n\nUsing CURL:\n\n    $ curl -H \"X-Tapis-Token: $JWT\" https://tacc.tapis.io/v3/systems?select=allAttributes\n\nThe response should contain a list of items similar to the single\nlisting shown above.\n\n::: note\n::: title\nNote\n:::\n\nSee the sections below on [Searching](#searching),\n[Selecting](#selecting), [Sorting](#sorting) and [Limiting](#limiting)\nto find out how to control the amount of information returned.\n:::"}
{"filename": "systems.md", "prompt": "Child Systems", "completion": ""}
{"filename": "systems.md", "prompt": "Creating Child Systems", "completion": "The ability to create child systems provides a way to easily clone and\nmanage systems based on existing systems. Child systems allow a user to\nset only a few fields, and use all other values from an existing system.\nThis reduces the difficulty in creating a child system, but also allows\nthe child system to be updated when the parent is updated.\n\nTo create a child system, create a local file (for example\nchild_system_example.json) with the following:\n\n    {\n       \"id\": \"my-child-<userid>\",\n       \"effectiveUserId\": \"${apiUserId}
\",\n       \"rootDir\": \"/home/<userid>\"\n    
\n\nWhere *\\<userid\\>* is replaced with your username. Also ensure that the\nroot directory path is correct. Now use the create child system REST\nendpoint to create the child system. Let\\'s assume that the new child\nsystem will be a child of a parent system called *parent-system*.\n\nUsing PySDK:\n\n    import json\n    from tapipy.tapis import Tapis\n    t = Tapis(base_url='https://tacc.tapis.io', username='<userid>', password='************')\n    with open('child_system_example.json', 'r') as openfile:\n        child_system = json.load(openfile)\n    t.systems.createChildSystem(parentId=\"parent-system\", **child_system)\n\nUsing CURL:\n\n    $ curl -X POST -H \"content-type: application/json\" -H \"X-Tapis-Token: $JWT\" https://tacc.tapis.io/v3/systems/parent-system/createChildSystem -d @child_system_example.json\n\nThese fields are maintained independently for child systems:\n\n+---------+-------+---------+----------------------------------------+\n| At      | Type  | Example | Notes                                  |\n| tribute |       |         |                                        |\n+=========+=======+=========+========================================+\n| id      | S     | ds1.s   | -   Identifier for the system. URI     |\n|         | tring | torage. |     safe, see RFC 3986.                |\n|         |       | default | -   *tenant* + *id* must be unique.    |\n|         |       |         | -   Allowed characters: Alphanumeric   |\n|         |       |         |     \\[0-9a-zA-Z\\] and special          |\n|         |       |         |     characters \\[-.\\_\\~\\].             |\n+---------+-------+---------+----------------------------------------+\n| owner   | S     | jdoe    | -   username of *owner*.               |\n|         | tring |         | -   Variable references:               |\n|         |       |         |     *\\${apiUserId
*. Resolved at       |\n|         |       |         |     create time.                       |\n|         |       |         | -   By default this is the resolved    |\n|         |       |         |     value for *\\${apiUserId
*.         |\n+---------+-------+---------+----------------------------------------+\n| enabled | bo    | FALSE   | -   Indicates if system currently      |\n|         | olean |         |     enabled for use.                   |\n|         |       |         | -   May be updated using the           |\n|         |       |         |     enable/disable endpoints.          |\n|         |       |         | -   By default this is *true*.         |\n+---------+-------+---------+----------------------------------------+\n| e       | S     | t       | -   User to use when accessing the     |\n| ffectiv | tring | g869834 |     system.                            |\n| eUserId |       |         | -   May be a static string or a        |\n|         |       |         |     variable reference.                |\n|         |       |         | -   Variable references:               |\n|         |       |         |     *\\${apiUserId
*, *\\${owner
*       |\n|         |       |         | -   On output variable reference will  |\n|         |       |         |     be resolved.                       |\n+---------+-------+---------+----------------------------------------+\n| rootDir | S     | /home   | -   Required if *systemType* is LINUX  |\n|         | tring | /\\${api |     or IRODS or *isDtn* = true.        |\n|         |       | UserId
 | -   For LINUX or IRODS must begin with |\n|         |       |         |     `/`.                               |\n|         |       |         | -   Optional for S3 and will typically |\n|         |       |         |     not begin with `/`.                |\n|         |       |         | -   Variable references are resolved   |\n|         |       |         |     at create time.                    |\n|         |       |         | -   Serves as effective root directory |\n|         |       |         |     when listing or moving files.      |\n|         |       |         | -   May not be updated. Contact        |\n|         |       |         |     support to request a change.       |\n|         |       |         | -   Variable references:               |\n|         |       |         |     *\\${apiUserId
*, *\\${owner
*,      |\n|         |       |         |     *\\${tenant
*                       |\n+---------+-------+---------+----------------------------------------+\n| deleted | bo    | FALSE   | -   Indicates if system has been       |\n|         | olean |         |     deleted.                           |\n|         |       |         | -   May be updated using the           |\n|         |       |         |     delete/undelete endpoints.         |\n+---------+-------+---------+----------------------------------------+\n| created | Time  | 2020-0  | -   When the system was created.       |\n|         | stamp | 6-19T15 |     Maintained by service.             |\n|         |       | :10:43Z |                                        |\n+---------+-------+---------+----------------------------------------+\n| updated | Time  | 2020-0  | -   When the system was last updated.  |\n|         | stamp | 7-04T23 |     Maintained by service.             |\n|         |       | :21:22Z |                                        |\n+---------+-------+---------+----------------------------------------+\n\nDuring the creation of a child system, any of these fields may be\nspecified except for created, updated and deleted. All other fields are\ntaken from the parent system."
{"filename": "systems.md", "prompt": "Updating a Child System", "completion": "Updates are done just like any other system, however, only the following\nfields may be updated for a child system.\n\n+---------+-------+---------+----------------------------------------+\n| At      | Type  | Example | Notes                                  |\n| tribute |       |         |                                        |\n+=========+=======+=========+========================================+\n| e       | S     | t       | -   User to use when accessing the     |\n| ffectiv | tring | g869834 |     system.                            |\n| eUserId |       |         | -   May be a static string or a        |\n|         |       |         |     variable reference.                |\n|         |       |         | -   Variable references:               |\n|         |       |         |     *\\${apiUserId}
*, *\\${owner
*       |\n|         |       |         | -   On output variable reference will  |\n|         |       |         |     be resolved.                       |\n+---------+-------+---------+----------------------------------------+\n\nSome other fields can be updated through special endpoints. For example\ndeleted and enabled are updated through the endpoints for deleting,\nundeleting, enabling and disabling."
{"filename": "systems.md", "prompt": "Child System Operations", "completion": "Most operations other than update are the same for child systems as they\nare for parent systems. For more information see the appropriate section\nof the document for the operation.\n\n-   Delete - see [Deletion](#deletion)\n-   Undelete - see [Deletion](#deletion)\n-   Enable - see \\\"enabled\\\" in [System Attributes\n    Table](#system-attributes-table)\n-   Disable - see \\\"enabled\\\" in [System Attributes\n    Table](#system-attributes-table)"}
{"filename": "systems.md", "prompt": "Unlinking a Child System from it\\'s Parent System", "completion": "A child system may be unlinked from it\\'s parent. This is a permanent\noperation, and cannot be undone. This will make the child a standalone\nsystem with all of it\\'s current settings. When the unlink happens any\nfields that had previously been linked to the parent will be copied to\nthe child, and it will be as if the child was created as in independent\nsystem with those values.\n\nIf the owner of the child system wants to unlink the child from it\\'s\nparent, the owner may use the *unlinkFromParent* endpoint.\n\nUsing PySDK:\n\n    import json\n    from tapipy.tapis import Tapis\n    t = Tapis(base_url='https://tacc.tapis.io', username='<userid>', password='************')\n    t.systems.unlinkFromParent(childSystemId=\"<child-system-id>\")\n\nUsing CURL:\n\n    $ curl -X POST -H \"content-type: application/json\" -H \"X-Tapis-Token: $JWT\" https://tacc.tapis.io/v3/systems/<child-system-id>/unlinkFromParent\n\nReplace *\\<child-system-id\\>* with the id of the child system.\n\nThe owner of a parent system can also decide to unlink child systems\nfrom the parent. In that case the parent system owner would use the\n*unlinkChildren* endpoint. The child systems to unlink may be specified\nin the request body. First create a json file (for example\nchildren_to_unlink.json):\n\n    {\n       \"childSystemIds\":\n       [\n         \"<child-system-1-id>\",\n         \"<child-system-2-id>\"\n         ...\n       ]\n    }
\n\nUsing PySDK:\n\n    import json\n    from tapipy.tapis import Tapis\n    t = Tapis(base_url='https://tacc.tapis.io', username='<userid>', password='************')\n    with open('children_to_unlink.json', 'r') as openfile:\n        children_to_unlink = json.load(openfile)\n    t.systems.unlinkChildren(parentSystemId=\"<parent-system-id>\", **children_to_unlink)\n\nUsing CURL:\n\n    $curl -X POST -H \"content-type: application/json\" -H \"X-Tapis-Token: $JWT\" https://tacc.tapis.io/v3/systems/<parent-system-id>/unlinkChildren -d @./children_to_unlink.json\n\nOr all child systems using *all=True* (no json file required)\n\nUsing PySDK:\n\n    import json\n    from tapipy.tapis import Tapis\n    t = Tapis(base_url='https://tacc.tapis.io', username='<userid>', password='************')\n    t.systems.unlinkChildren(parentSystemId=\"<parent-system-id>\", all=True)\n\nUsing CURL:\n\n    $ curl -X POST -H \"content-type: application/json\" -H \"X-Tapis-Token: $JWT\" \"https://tacc.tapis.io/v3/systems/<parent-system-id>/unlinkChildren?all=true\""
{"filename": "systems.md", "prompt": "Minimal Definition and Restrictions", "completion": "When creating a system the required attributes are: *id*, *systemType*,\n*host*, *defaultAuthnMethod* and *canExec*. Depending on the type of\nsystem and specific values for certain attributes there are other\nrequirements. The restrictions are:\n\n-   If *systemType* is S3 then *bucketName* is required, *canExec* and\n    *isDtn* must be false.\n-   If *systemType* is LINUX or IRODS then *rootDir* is required and\n    must begin with `/`.\n-   If *effectiveUserId* is `${apiUserId}
` (i.e. it is not static) then\n    *authnCredential* may not be specified.\n-   If *isDtn* is true then *rootDir* is required, *canExec* must be\n    false and following may not be specified: *dtnSystemId*,\n    *dtnMountSourcePath*, *dtnMountPoint*, all job execution related\n    attributes.\n-   If *canExec* is true then *jobWorkingDir* is required and\n    *jobRuntimes* must have at least one entry.\n-   If *canRunBatch* is true then *batchScheduler* must be specified.\n-   If *canRunBatch* is true then *batchLogicalQueues* must have at\n    least one item.\n    -   If *batchLogicalQueues* has more than one item then\n        *batchLogicalDefaultQueue* must be specified.\n    -   If *batchLogicalQueues* has exactly one item then\n        *batchLogicalDefaultQueue* is set to that item."
{"filename": "systems.md", "prompt": "Permissions", "completion": "The permissions model allows for fine grained access control of Tapis\nsystems.\n\nAt system creation time the owner is given full access to the system.\nPermissions for other users may be granted and revoked through the\nsystems API. Please note that grants and revokes through this service\nonly impact the default role for the user. A user may still have access\nthrough permissions in another role. So even after revoking permissions\nthrough this service, when permissions are retrieved the access may\nstill be listed. This indicates access has been granted via another\nrole.\n\nPermissions are specified as either `*` for all permissions or some\ncombination of the following specific permissions:\n`(\"READ\",\"MODIFY\",\"EXECUTE\")`. Specifying permissions in all lower case\nis also allowed. Having `MODIFY` implies `READ`."}
{"filename": "systems.md", "prompt": "Sharing", "completion": "In addition to fine grained permissions support, Tapis also supports a\nhigher level approach to granting access. This approach is known simply\nas *sharing*. The sharing API allows you to share a system with a set of\nusers as well as share publicly with all users in a tenant. Sharing\nprovides `READ+EXECUTE` access. When the system has a dynamic\n*effectiveUserId*, sharing also allows for MODIFY access to all paths\nfor calls made through the Files service. Note that Tapis permissions\nand sharing are independent of native permissions enforced by the\nunderlying system host.\n\nThe most common use case for sharing a system is to publicly share the\nsystem with all users in the tenant. This would allow any user to use\nthe system for execution or storage when running an application.\n\nFor more information on sharing please see `sharing`{.interpreted-text\nrole=\"doc\"}
"
{"filename": "systems.md", "prompt": "Authentication Credentials", "completion": "At system creation time the authentication credentials may be specified\nif the effective access user *effectiveUserId* is a specific user (such\nas a service account) and not a dynamic user (i.e. not equal to\n`${apiUserId}
`).\n\nIf the effective access user is dynamic (i.e. equal to `${apiUserId
`)\nthen authentication credentials for any user allowed to access the\nsystem must be registered in separate API calls. In this case the\npayload provided may contain the optional attribute *loginUser* which\nwill be used to map the Tapis user to a username to be used when\naccessing the system. If the login user is not provided then there is no\nmapping and the Tapis user is always used when accessing the system.\n\nNote that the Systems service does not store credentials. Credentials\nare persisted by the Security Kernel service and only specific Tapis\nservices are authorized to retrieve credentials.\n\nBy default any credentials provided for LINUX and S3 type systems are\nverified. The query parameter *skipCredentialCheck=true* may be used to\nbypass the initial verification of credentials."
{"filename": "systems.md", "prompt": "Runtime", "completion": "Runtime environment supported by the system that may be used to run\napplications, such as docker or singularity. Consists of the runtime\ntype and version."}
{"filename": "systems.md", "prompt": "Logical Batch Queue", "completion": "A queue that maps to a single HPC queue. Logical batch queues provide a\nuniform front end abstraction for an HPC queue. They also provide more\nfeatures and flexibility than is typically provided by an HPC scheduler.\nMultiple logical queues may be defined for each HPC queue. If an HPC\nqueue does not have a corresponding logical queue defined then a user\nwill not be able use the Tapis system to directly submit a job via Tapis\nto that HPC queue."}
{"filename": "systems.md", "prompt": "Deletion", "completion": "A system may be deleted and undeleted. Deletion means the system is\nmarked as deleted and is no longer available for use. By default deleted\nsystems will not be included in searches and operations on deleted\nsystems will not be allowed. When listing systems the query parameter\n*showDeleted* may be used in order to include deleted systems in the\nresults."}
{"filename": "systems.md", "prompt": "System Attributes Table", "completion": "+---------+-------+---------+----------------------------------------+\n| At      | Type  | Example | Notes                                  |\n| tribute |       |         |                                        |\n+=========+=======+=========+========================================+\n| tenant  | S     | des     | -   Name of the tenant for which the   |\n|         | tring | ignsafe |     system is defined.                 |\n|         |       |         | -   *tenant* + *id* must be unique.    |\n|         |       |         | -   Determined by the service at       |\n|         |       |         |     system creation time.              |\n+---------+-------+---------+----------------------------------------+\n| id      | S     | ds1.s   | -   Identifier for the system. URI     |\n|         | tring | torage. |     safe, see RFC 3986.                |\n|         |       | default | -   *tenant* + *id* must be unique.    |\n|         |       |         | -   Allowed characters: Alphanumeric   |\n|         |       |         |     \\[0-9a-zA-Z\\] and special          |\n|         |       |         |     characters \\[-.\\_\\~\\].             |\n+---------+-------+---------+----------------------------------------+\n| desc    | S     | Default | -   Description                        |\n| ription | tring | storage |                                        |\n+---------+-------+---------+----------------------------------------+\n| sys     | enum  | LINUX   | -   Type of system.                    |\n| temType |       |         | -   Types: LINUX, S3, IRODS, GLOBUS    |\n+---------+-------+---------+----------------------------------------+\n| owner   | S     | jdoe    | -   username of *owner*.               |\n|         | tring |         | -   Variable references:               |\n|         |       |         |     *\\${apiUserId}
*. Resolved at       |\n|         |       |         |     create time.                       |\n|         |       |         | -   By default this is the resolved    |\n|         |       |         |     value for *\\${apiUserId
*.         |\n+---------+-------+---------+----------------------------------------+\n| host    | S     | data.t  | -   Host name, ip address, Globus      |\n|         | tring | acc.ute |     endpoint ID or Globus collection   |\n|         |       | xas.edu |     ID.                                |\n+---------+-------+---------+----------------------------------------+\n| enabled | bo    | FALSE   | -   Indicates if system currently      |\n|         | olean |         |     enabled for use.                   |\n|         |       |         | -   May be updated using the           |\n|         |       |         |     enable/disable endpoints.          |\n|         |       |         | -   By default this is *true*.         |\n+---------+-------+---------+----------------------------------------+\n| e       | S     | t       | -   User to use when accessing the     |\n| ffectiv | tring | g869834 |     system.                            |\n| eUserId |       |         | -   May be a static string or a        |\n|         |       |         |     variable reference.                |\n|         |       |         | -   Variable references:               |\n|         |       |         |     *\\${apiUserId
*, *\\${owner
*       |\n|         |       |         | -   On output variable reference will  |\n|         |       |         |     be resolved.                       |\n+---------+-------+---------+----------------------------------------+\n| defa    | enum  | P       | -   How access authentication is       |\n| ultAuth |       | KI_KEYS |     handled by default.                |\n| nMethod |       |         | -   Can be overridden as part of a     |\n|         |       |         |     request to get a system or         |\n|         |       |         |     credential.                        |\n|         |       |         | -   Methods: PASSWORD, PKI_KEYS,       |\n|         |       |         |     ACCESS_KEY, TOKEN                  |\n|         |       |         | -   See table *Credential Attributes*  |\n|         |       |         |     below for more information.        |\n+---------+-------+---------+----------------------------------------+\n| a       | Crede |         | -   On input credentials to be stored  |\n| uthnCre | ntial |         |     in Security Kernel.                |\n| dential |       |         | -   *effectiveUserId* must be static,  |\n|         |       |         |     either a string constant or        |\n|         |       |         |     \\${owner
.                         |\n|         |       |         | -   May not be specified if            |\n|         |       |         |     *effectiveUserId* is dynamic, i.e. |\n|         |       |         |     *\\${apiUserId
*.                   |\n|         |       |         | -   On output contains credential for  |\n|         |       |         |     *effectiveUserId* and requested    |\n|         |       |         |     *authnMethod*.                     |\n|         |       |         | -   Returned credential contains       |\n|         |       |         |     relevant information based on      |\n|         |       |         |     *authnMethod*.                     |\n|         |       |         | -   Credentials may be updated using   |\n|         |       |         |     the systems credentials endpoint.  |\n|         |       |         | -   By default for LINUX the           |\n|         |       |         |     credentials are verified during    |\n|         |       |         |     create or update.                  |\n|         |       |         | -   Use query parameter                |\n|         |       |         |     skipCredentialCheck=true to bypass |\n|         |       |         |     initial verification.              |\n|         |       |         | -   See table *Credential Attributes*  |\n|         |       |         |     below for more information.        |\n+---------+-------+---------+----------------------------------------+\n| buc     | S     | tapis-d | -   Name of bucket for an S3 system.   |\n| ketName | tring | s1-jdoe | -   Required if *systemType* is S3.    |\n|         |       |         | -   Variable references:               |\n|         |       |         |     *\\${apiUserId
*, *\\${owner
*,      |\n|         |       |         |     *\\${tenant
*                       |\n+---------+-------+---------+----------------------------------------+\n| rootDir | S     | /home   | -   Required if *systemType* is LINUX  |\n|         | tring | /\\${api |     or IRODS or *isDtn* = true.        |\n|         |       | UserId
 | -   For LINUX or IRODS must begin with |\n|         |       |         |     `/`.                               |\n|         |       |         | -   Optional for S3 and will typically |\n|         |       |         |     not begin with `/`.                |\n|         |       |         | -   Variable references are resolved   |\n|         |       |         |     at create time.                    |\n|         |       |         | -   Serves as effective root directory |\n|         |       |         |     when listing or moving files.      |\n|         |       |         | -   May not be updated. Contact        |\n|         |       |         |     support to request a change.       |\n|         |       |         | -   Variable references:               |\n|         |       |         |     *\\${apiUserId
*, *\\${owner
*,      |\n|         |       |         |     *\\${tenant
*                       |\n+---------+-------+---------+----------------------------------------+\n| port    | int   | 22      | -   Port number used to access the     |\n|         |       |         |     system                             |\n+---------+-------+---------+----------------------------------------+\n| u       | bo    | TRUE    | -   Indicates if system should be      |\n| seProxy | olean |         |     accessed through a proxy.          |\n+---------+-------+---------+----------------------------------------+\n| pr      | S     |         | -   Name of proxy host.                |\n| oxyHost | tring |         |                                        |\n+---------+-------+---------+----------------------------------------+\n| pr      | int   |         | -   Port number for *proxyHost*        |\n| oxyPort |       |         |                                        |\n+---------+-------+---------+----------------------------------------+\n| dtnS    | S     | defa    | -   An alternate system to use as a    |\n| ystemId | tring | ult.cor |     Data Transfer Node (DTN).          |\n|         |       | ral.dtn | -   This system and *dtnSystemId* must |\n|         |       |         |     have shared storage.               |\n+---------+-------+---------+----------------------------------------+\n| dtnMou  | S     | /gpf    | -   Mount point (aka target) used when |\n| ntPoint | tring | s/corra |     running the mount command on this  |\n|         |       | l3/repl |     system.                            |\n|         |       |         | -   Base location on this system for   |\n|         |       |         |     files transferred to *rootDir* on  |\n|         |       |         |     *dtnSystemId.*                     |\n+---------+-------+---------+----------------------------------------+\n| dtnM    | S     | /gpf    | -   Relative path defining DTN source  |\n| ountSou | tring | s/corra |     directory relative to rootDir on   |\n| rcePath |       | l3/repl |     *dtnSystemId.*                     |\n+---------+-------+---------+----------------------------------------+\n| isDtn   | bo    | FALSE   | -   Indicates if system will be used   |\n|         | olean |         |     as a data transfer node (DTN).     |\n|         |       |         | -   By default this is *false*.        |\n+---------+-------+---------+----------------------------------------+\n| canExec | bo    |         | -   Indicates if system will be used   |\n|         | olean |         |     to execute jobs.                   |\n+---------+-------+---------+----------------------------------------+\n| canR    | bo    |         | -   Indicates if system supports       |\n| unBatch | olean |         |     running jobs using a batch         |\n|         |       |         |     scheduler.                         |\n|         |       |         | -   By default this is *false*.        |\n+---------+-------+---------+----------------------------------------+\n| e       | bo    |         | -   Indicates if system allows a job   |\n| nableCm | olean |         |     submission request to specify a    |\n| dPrefix |       |         |     cmdPrefix.                         |\n|         |       |         | -   By default this is *false*.        |\n+---------+-------+---------+----------------------------------------+\n| jobR    | \\     |         | -   List of runtime environments       |\n| untimes | [Runt |         |     supported by the system.           |\n|         | ime\\] |         | -   At least one entry required if     |\n|         |       |         |     *canExec* is true.                 |\n|         |       |         | -   Each Runtime specifies the Runtime |\n|         |       |         |     type and version                   |\n|         |       |         | -   Runtime type is required and must  |\n|         |       |         |     be one of: DOCKER, SINGULARITY.    |\n|         |       |         | -   Runtime version is optional.       |\n+---------+-------+---------+----------------------------------------+\n| jobWor  | S     | HOST_E  | -   Parent directory from which a job  |\n| kingDir | tring | VAL(\\$S |     is run.                            |\n|         |       | CRATCH) | -   Relative to the effective root     |\n|         |       |         |     directory *rootDir*.               |\n|         |       |         | -   Required if *canExec* is true.     |\n|         |       |         | -   Variable references:               |\n|         |       |         |     *\\${apiUserId
*, *\\${owner
*,      |\n|         |       |         |     *\\${tenant
*                       |\n+---------+-------+---------+----------------------------------------+\n| j       | \\     |         | -   Environment variables added to the |\n| obEnvVa | [KeyV |         |     shell environment in which the job |\n| riables | alueP |         |     is running.                        |\n|         | air\\] |         | -   Added to environment variables     |\n|         |       |         |     specified in job and application   |\n|         |       |         |     definitions.                       |\n|         |       |         | -   Each entry has *key* (required)    |\n|         |       |         |     and *value* (optional) as well as  |\n|         |       |         |     other attributes.                  |\n|         |       |         | -   See table *KeyValuePair            |\n|         |       |         |     Attributes* below for more         |\n|         |       |         |     information.                       |\n+---------+-------+---------+----------------------------------------+\n| job     | int   |         | -   Max total number of jobs .         |\n| MaxJobs |       |         | -   Set to -1 for unlimited.           |\n+---------+-------+---------+----------------------------------------+\n| job     | int   |         | -   Max total number of jobs           |\n| MaxJobs |       |         |     associated with a specific user.   |\n| PerUser |       |         | -   Set to -1 for unlimited.           |\n+---------+-------+---------+----------------------------------------+\n| batchSc | S     | SLURM   | -   Type of scheduler used when        |\n| heduler | tring |         |     running batch jobs.                |\n|         |       |         | -   Schedulers: SLURM                  |\n+---------+-------+---------+----------------------------------------+\n| batc    | \\     |         | -   List of logical queues available   |\n| hLogica | [Logi |         |     on the system.                     |\n| lQueues | calQu |         | -   Each logical queue maps to a       |\n|         | eue\\] |         |     single HPC queue.                  |\n|         |       |         | -   Multiple logical queues may be     |\n|         |       |         |     defined for each HPC queue.        |\n|         |       |         | -   See table *LogicalQueue            |\n|         |       |         |     Attributes* below for more         |\n|         |       |         |     information.                       |\n+---------+-------+---------+----------------------------------------+\n| batch   | Lo    |         | -   Default logical batch queue for    |\n| Default | gical |         |     the system.                        |\n| Logical | Queue |         |                                        |\n| Queue   |       |         |                                        |\n+---------+-------+---------+----------------------------------------+\n| tags    | \\[Str |         | -   List of tags as simple strings.    |\n|         | ing\\] |         |                                        |\n+---------+-------+---------+----------------------------------------+\n| notes   | S     | \\\"{
\\\"  | -   Simple metadata in the form of a   |\n|         | tring |         |     Json object.                       |\n|         |       |         | -   Not used by Tapis.                 |\n+---------+-------+---------+----------------------------------------+\n| uuid    | UUID  |         | -   Auto-generated by service.         |\n+---------+-------+---------+----------------------------------------+\n| deleted | bo    | FALSE   | -   Indicates if system has been       |\n|         | olean |         |     deleted.                           |\n|         |       |         | -   May be updated using the           |\n|         |       |         |     delete/undelete endpoints.         |\n+---------+-------+---------+----------------------------------------+\n| created | Time  | 2020-0  | -   When the system was created.       |\n|         | stamp | 6-19T15 |     Maintained by service.             |\n|         |       | :10:43Z |                                        |\n+---------+-------+---------+----------------------------------------+\n| updated | Time  | 2020-0  | -   When the system was last updated.  |\n|         | stamp | 7-04T23 |     Maintained by service.             |\n|         |       | :21:22Z |                                        |\n+---------+-------+---------+----------------------------------------+"
{"filename": "systems.md", "prompt": "Credential Attributes Table", "completion": "+---------+-------+---------+----------------------------------------+\n| At      | Type  | Example | Notes                                  |\n| tribute |       |         |                                        |\n+=========+=======+=========+========================================+\n| user    | S     | jsmith  | -   Username associated with the       |\n|         | tring |         |     credential.                        |\n+---------+-------+---------+----------------------------------------+\n| auth    | S     | P       | -   Indicates the authentication       |\n| nMethod | tring | KI_KEYS |     method associated with a retrieved |\n|         |       |         |     credential.                        |\n|         |       |         | -   When a credential is retrieved it  |\n|         |       |         |     is for a specific authentication   |\n|         |       |         |     method.                            |\n|         |       |         | -   Methods: PASSWORD, PKI_KEYS,       |\n|         |       |         |     ACCESS_KEY, TOKEN                  |\n+---------+-------+---------+----------------------------------------+\n| lo      | S     |         | -   Optional native username valid on  |\n| ginUser | tring |         |     the system.                        |\n|         |       |         | -   May be used to map a Tapis user to |\n|         |       |         |     a native login user.               |\n+---------+-------+---------+----------------------------------------+\n| p       | S     |         | -   Password for when authnMethod is   |\n| assword | tring |         |     PASSWORD. For LINUX and IRODS      |\n|         |       |         |     systems.                           |\n+---------+-------+---------+----------------------------------------+\n| pri     | S     |         | -   Private key for when authnMethod   |\n| vateKey | tring |         |     is PKI_KEYS. For LINUX systems.    |\n+---------+-------+---------+----------------------------------------+\n| pu      | S     |         | -   Public key for when authnMethod is |\n| blicKey | tring |         |     PKI_KEYS. For LINUX systems.       |\n+---------+-------+---------+----------------------------------------+\n| ac      | S     |         | -   Access key for when authnMethod is |\n| cessKey | tring |         |     ACCESS_KEY. For S3 systems.        |\n+---------+-------+---------+----------------------------------------+\n| acces   | S     |         | -   Access secret for when authnMethod |\n| sSecret | tring |         |     is ACCESS_KEY. For S3 systems.     |\n+---------+-------+---------+----------------------------------------+\n| acce    | S     |         | -   Access token for when authnMethod  |\n| ssToken | tring |         |     is TOKEN. For GLOBUS systems.      |\n+---------+-------+---------+----------------------------------------+\n| refre   | S     |         | -   Refresh token for when authnMethod |\n| shToken | tring |         |     is TOKEN. For GLOBUS systems.      |\n+---------+-------+---------+----------------------------------------+"}
{"filename": "systems.md", "prompt": "KeyValuePair Attributes Table", "completion": "+----------+---+----------+------------------------------------------+\n| A        | T | Example  | Notes                                    |\n| ttribute | y |          |                                          |\n|          | p |          |                                          |\n|          | e |          |                                          |\n+==========+===+==========+==========================================+\n| key      | S | > \\\"INPU | -   Environment variable name. Required. |\n|          | t | T_FILE\\\" |                                          |\n|          | r |          |                                          |\n|          | i |          |                                          |\n|          | n |          |                                          |\n|          | g |          |                                          |\n+----------+---+----------+------------------------------------------+\n| value    | S | > \\\"/    | -   Environment variable value           |\n|          | t | tmp/file |                                          |\n|          | r | .input\\\" |                                          |\n|          | i |          |                                          |\n|          | n |          |                                          |\n|          | g |          |                                          |\n+----------+---+----------+------------------------------------------+\n| des      | S |          | -   Description                          |\n| cription | t |          |                                          |\n|          | r |          |                                          |\n|          | i |          |                                          |\n|          | n |          |                                          |\n|          | g |          |                                          |\n+----------+---+----------+------------------------------------------+\n| i        | e | >        | -   Indicates how argument is to be      |\n| nputMode | n | REQUIRED |     treated when processing individual   |\n|          | u |          |     job requests.                        |\n|          | m |          | -   Modes: REQUIRED, FIXED,              |\n|          |   |          |     INCLUDE_ON_DEMAND,                   |\n|          |   |          |     INCLUDE_BY_DEFAULT                   |\n|          |   |          | -   Default is INCLUDE_BY_DEFAULT.       |\n|          |   |          | -   REQUIRED: Must be provided in a job  |\n|          |   |          |     request or application definition.   |\n|          |   |          | -   FIXED: Not overridable in            |\n|          |   |          |     application or job request.          |\n|          |   |          | -   INCLUDE_ON_DEMAND: Included if       |\n|          |   |          |     referenced in a job request.         |\n|          |   |          | -   INCLUDE_BY_DEFAULT: Included unless  |\n|          |   |          |     *include=false* in a job request.    |\n+----------+---+----------+------------------------------------------+\n| notes    | S | > \\\"{}
\\\" | -   Simple metadata in the form of a     |\n|          | t |          |     Json object.                         |\n|          | r |          | -   Not used by Tapis.                   |\n|          | i |          |                                          |\n|          | n |          |                                          |\n|          | g |          |                                          |\n+----------+---+----------+------------------------------------------+"
{"filename": "systems.md", "prompt": "LogicalQueue Attributes Table", "completion": "+---------+-------+---------+----------------------------------------+\n| At      | Type  | Example | Notes                                  |\n| tribute |       |         |                                        |\n+=========+=======+=========+========================================+\n| name    | S     | > tapi  | -   Name for logical queue. Typically  |\n|         | tring | sNormal |     will match or be a variant of HPC  |\n|         |       |         |     queue name.                        |\n+---------+-------+---------+----------------------------------------+\n| hpcQu   | S     | >       | -   Name of the HPC queue for which    |\n| eueName | tring |  normal |     this logical queue is a front end. |\n+---------+-------+---------+----------------------------------------+\n| maxJobs | int   |         | -   Maximum total number of jobs that  |\n|         |       |         |     can be queued or running in this   |\n|         |       |         |     queue.                             |\n+---------+-------+---------+----------------------------------------+\n| maxJobs | int   |         | -   Maximum number of jobs associated  |\n| PerUser |       |         |     with a specific user that can be   |\n|         |       |         |     queued.                            |\n+---------+-------+---------+----------------------------------------+\n| minNo   | int   |         | -   Minimum number of nodes that can   |\n| deCount |       |         |     be requested when submitting a job |\n|         |       |         |     to the queue.                      |\n+---------+-------+---------+----------------------------------------+\n| maxNo   | int   |         | -   Maximum number of nodes that can   |\n| deCount |       |         |     be requested when submitting a job |\n|         |       |         |     to the queue.                      |\n+---------+-------+---------+----------------------------------------+\n| m       | int   |         | -   Minimum number of cores per node   |\n| inCores |       |         |     that can be requested when         |\n| PerNode |       |         |     submitting a job.                  |\n|         |       |         | -   Default is 1                       |\n+---------+-------+---------+----------------------------------------+\n| m       | int   |         | -   Maximum number of cores per node   |\n| axCores |       |         |     that can be requested when         |\n| PerNode |       |         |     submitting a job.                  |\n+---------+-------+---------+----------------------------------------+\n| minM    | int   |         | -   Minimum memory in megabytes that   |\n| emoryMB |       |         |     can be requested when submitting a |\n|         |       |         |     job.                               |\n|         |       |         | -   Default is 0                       |\n+---------+-------+---------+----------------------------------------+\n| maxM    | int   |         | -   Maximum memory in megabytes that   |\n| emoryMB |       |         |     can be requested when submitting a |\n|         |       |         |     job.                               |\n|         |       |         | -   Default is unlimited               |\n+---------+-------+---------+----------------------------------------+\n| min     | int   |         | -   Minimum run time in minutes that   |\n| Minutes |       |         |     can be requested when submitting a |\n|         |       |         |     job.                               |\n|         |       |         | -   Default is 0                       |\n+---------+-------+---------+----------------------------------------+\n| max     | int   |         | -   Maximum run time in minutes that   |\n| Minutes |       |         |     can be requested when submitting a |\n|         |       |         |     job.                               |\n|         |       |         | -   Default is unlimited               |\n+---------+-------+---------+----------------------------------------+"}
{"filename": "systems.md", "prompt": "Searching", "completion": "The service provides a way for users to search for systems based on a\nlist of search conditions provided either as query parameters for a GET\ncall or a list of conditions in a request body for a POST call to a\ndedicated search endpoint."}
{"filename": "systems.md", "prompt": "Search using GET", "completion": "To search when using a GET request to the `systems` endpoint a list of\nsearch conditions may be specified using a query parameter named\n`search`. Each search condition must be surrounded with parentheses,\nhave three parts separated by the character `.` and be joined using the\ncharacter `~`. All conditions are combined using logical AND. The\ngeneral form for specifying the query parameter is as follows:\n\n    ?search=(<attribute_1>.<op_1>.<value_1>)~(<attribute_2>.<op_2>.<value_2>)~ ... ~(<attribute_N>.<op_N>.<value_N>)\n\nAttribute names are given in the table above and may be specified using\nCamel Case or Snake Case.\n\nSupported operators: `eq` `neq` `gt` `gte` `lt` `lte` `in` `nin` `like`\n`nlike` `between` `nbetween`\n\nExample CURL command to search for systems that have `Test` in the id,\nare of type LINUX, are using a port less than `1024` and have a default\nauthentication method of either `PKI_KEYS` or `PASSWORD`:\n\n    $ curl -H \"X-Tapis-Token: $JWT\" https://tacc.tapis.io/v3/systems?search=\"(id.like.*Test*)~(system_type.eq.LINUX)~(port.lt.1024)~(DefaultAuthnMethod.in.PKI_KEYS,PASSWORD)\"\n\nNotes:\n\n-   For the `like` and `nlike` operators the wildcard character `*`\n    matches zero or more characters and `!` matches exactly one\n    character.\n\n-   For the `between` and `nbetween` operators the value must be a two\n    item comma separated list of unquoted values.\n\n-   If there is only one condition the surrounding parentheses are\n    optional.\n\n-   In a shell environment the character `&` separating query parameters\n    must be escaped with a backslash.\n\n-   In a shell environment the query value must be surrounded by double\n    quotes and the following characters must be escaped with a backslash\n    in order to be properly interpreted by the shell:\n\n    -   `\"` `\\` ``\\`\n\n-   Attribute names may be specified using Camel Case or Snake Case.\n\n-   Following complex attributes not supported when searching:\n\n    > -   `authnCredential` `jobRuntimes` `jobEnvVariables`\n    >     `batchLogicalQueues` `notes`"}
{"filename": "systems.md", "prompt": "Dedicated Search Endpoint", "completion": "The service provides the dedicated search endpoint\n`systems/search/systems` for specifying complex queries. Using a GET\nrequest to this endpoint provides functionality similar to above but\nwith a different syntax. For more complex queries a POST request may be\nused with a request body specifying the search conditions using an\nSQL-like syntax."}
{"filename": "systems.md", "prompt": "Search using GET on Dedicated Endpoint", "completion": "Sending a GET request to the search endpoint provides functionality very\nsimilar to that provided for the endpoint `systems` described above. A\nlist of search conditions may be specified using a series of query\nparameters, one for each attribute. All conditions are combined using\nlogical AND. The general form for specifying the query parameters is as\nfollows:\n\n    ?<attribute_1>.<op_1>=<value_1>&<attribute_2>.<op_2>=<value_2>)& ... &<attribute_N>.<op_N>=<value_N>\n\nAttribute names are given in the table above and may be specified using\nCamel Case or Snake Case.\n\nSupported operators: `eq` `neq` `gt` `gte` `lt` `lte` `in` `nin` `like`\n`nlike` `between` `nbetween`\n\nExample CURL command to search for systems that have `Test` in the name,\nare of type `LINUX`, are using a port less than `1024` and have a\ndefault authentication method of either `PKI_KEYS` or `PASSWORD`:\n\n    $ curl -H \"X-Tapis-Token: $JWT\" https://tacc.tapis.io/v3/systems/search/systems?name.like=*Test*\\&enabled.eq=true\\&system_type.eq=LINUX\\&DefaultAuthnMethod.in=PKI_KEYS,PASSWORD\n\nNotes:\n\n-   For the `like` and `nlike` operators the wildcard character `*`\n    matches zero or more characters and `!` matches exactly one\n    character.\n-   For the `between` and `nbetween` operators the value must be a two\n    item comma separated list of unquoted values.\n-   In a shell environment the character `&` separating query parameters\n    must be escaped with a backslash.\n-   Attribute names may be specified using Camel Case or Snake Case.\n-   Following complex attributes not supported when searching:\n    -   `authnCredential` `jobRuntimes` `jobEnvVariables`\n        `batchLogicalQueues` `tags` `notes`"}
{"filename": "systems.md", "prompt": "Search using POST on Dedicated Endpoint", "completion": "More complex search queries are supported when sending a POST request to\nthe endpoint `systems/search/systems`. For these requests the request\nbody must contain json with a top level property name of `search`. The\n`search` property must contain an array of strings specifying the search\ncriteria in an SQL-like syntax. The array of strings are concatenated to\nform the full search query. The full query must be in the form of an\nSQL-like `WHERE` clause. Note that not all SQL features are supported.\n\nFor example, to search for systems that are owned by `jdoe` and of type\n`LINUX` or owned by `jsmith` and using a port less than `1024` create a\nlocal file named `system_search.json` with following json:\n\n    {\n      \"search\":\n        [\n          \"(owner = 'jdoe' AND system_type = 'LINUX') OR\",\n          \"(owner = 'jsmith' AND port < 1024)\"\n        ]\n    }
\n\nTo execute the search use a CURL command similar to the following:\n\n    $ curl -X POST -H \"content-type: application/json\" -H \"X-Tapis-Token: $JWT\" https://tacc.tapis.io/v3/systems/search/systems -d @system_search.json\n\nNotes:\n\n-   String values must be surrounded by single quotes.\n-   Values for BETWEEN must be surrounded by single quotes.\n-   Search query parameters as described above may not be used in\n    conjunction with a POST request.\n-   SQL features not supported include:\n    -   `IS NULL` and `IS NOT NULL`\n    -   Arithmetic operations\n    -   Unary operators\n    -   Specifying escape character for `LIKE` operator"
{"filename": "systems.md", "prompt": "Map of SQL operators to Tapis operators", "completion": "---------------------------------\n  Sql Operator     Tapis Operator\n  ---------------- ----------------\n  =                eq\n\n  \\<\\>             neq\n\n  \\<               lt\n\n  \\<=              lte\n\n  \\>               gt\n\n  \\>=              gte\n\n  LIKE             like\n\n  NOT LIKE         nlike\n\n  BETWEEN          between\n\n  NOT BETWEEN      nbetween\n\n  IN               in\n\n  NOT IN           nin\n  ---------------------------------"}
{"filename": "systems.md", "prompt": "Sort, Limit, Select and ListType", "completion": "When a list of Systems is retrieved the service provides for sorting,\nfiltering and limiting the results. By default, only resources owned by\nyou will be included. The service provides a way for you to request that\nall resources accessible to you be included. This is determined by the\nquery parameter *listType*.\n\nWhen retrieving either a list of resources or a single resource the\nservice also provides a way to *select* which fields (i.e. attributes)\nare included in the results. Sorting, limiting and attribute selection\nare supported using query parameters."}
{"filename": "systems.md", "prompt": "Selecting", "completion": "When retrieving systems the fields (i.e. attributes) to be returned may\nbe specified as a comma separated list using a query parameter named\n`select`. Attribute names may be given using Camel Case or Snake Case.\n\nNotes:\n\n> -   Special select keywords are supported: `allAttributes` and\n>     `summaryAttributes`\n> -   Summary attributes include:\n>     -   `id`, `systemType`, `owner`, `host`, `effectiveUserId`,\n>         `defaultAuthnMethod`, `canExec`\n> -   By default all attributes are returned when retrieving a single\n>     resource via the endpoint *systems/\\<system_id\\>*.\n> -   By default summary attributes are returned when retrieving a list\n>     of systems.\n> -   Specifying nested attributes is not supported.\n> -   The attribute `id` is always returned.\n\nFor example, to return only the attributes `host` and `effectiveUserId`\nthe CURL command would look like this:\n\n    $ curl -H \"X-Tapis-Token: $JWT\" https://tacc.tapis.io/v3/systems?select=host,effectiveUserId\n\nThe response should look similar to the following:\n\n    {\n     \"result\": [\n           {\n               \"id\": \"CSys_CltSrchGet_011\",\n               \"host\": \"hostCltSrchGet_011\",\n               \"effectiveUserId\": \"effUserCltSrchGet_011\"\n           }
,\n           {\n               \"id\": \"CSys_CltSrchGet_012\",\n               \"host\": \"hostCltSrchGet_012\",\n               \"effectiveUserId\": \"effUserCltSrchGet_012\"\n           
,\n           {\n               \"id\": \"CSys_CltSrchGet_013\",\n               \"host\": \"hostCltSrchGet_013\",\n               \"effectiveUserId\": \"effUserCltSrchGet_013\"\n           
\n       ],\n       \"status\": \"success\",\n       \"message\": \"TAPIS_FOUND Systems found: 12 systems\",\n       \"version\": \"1.0.0\",\n       \"metadata\": {\n           \"recordCount\": 3,\n           \"recordLimit\": 100,\n           \"recordsSkipped\": 0,\n           \"orderBy\": null,\n           \"startAfter\": null,\n           \"totalCount\": -1\n       
\n    
"
{"filename": "systems.md", "prompt": "Sorting", "completion": "The query parameter for sorting is named `orderBy` and the value is the\nattribute name to sort on with an optional sort direction. The general\nformat is `<attribute_name>(<dir>)`. The direction may be `asc` for\nascending or `desc` for descending. The default direction is ascending.\n\nExamples:\n\n> -   orderBy=id\n> -   orderBy=id(asc)\n> -   orderBy=name(desc),created\n> -   orderBy=id(asc),created(desc)"}
{"filename": "systems.md", "prompt": "Limiting", "completion": "Additional query parameters may be used in order to limit the number and\nstarting point for results. This is useful for implementing paging. The\nquery parameters are:\n\n> -   `limit` - Limit number of items returned. For example limit=10.\n>     -   Use 0 or less for unlimited.\n>     -   Default is 100.\n> -   `skip` - Number of items to skip. For example skip=10.\n>     -   May not be used with startAfter.\n>     -   Default is 0.\n> -   `startAfter` - Where to start when sorting. For example\n>     limit=10&orderBy=id(asc),created(desc)&startAfter=101\n>     -   May not be used with `skip`.\n>     -   Must also specify `orderBy`.\n>     -   The value of `startAfter` applies to the major `orderBy`\n>         field.\n>     -   Condition is context dependent. For ascending the condition is\n>         value \\> `startAfter` and for descending the condition is\n>         value \\< `startAfter`.\n\nWhen implementing paging it is recommend to always use `orderBy` and\nwhen possible use `limit+startAfter` rather than `limit+skip`. Sorting\nshould always be included since returned results are not guaranteed to\nbe in the same order for each call. The combination of\n`limit+startAfter` is preferred because `limit+skip` is more likely to\nresult in inconsistent results as records are added and removed. Using\n`limit+startAfter` works best when the attribute has a natural\nsequential ordering such as when an attribute represents a timestamp or\na sequential ID."}
{"filename": "systems.md", "prompt": "ListType", "completion": "By default, you will only see the resources that you own. The query\nparameter *listType* allows you to see additional resources that are\navailable to you.\n\nOptions:\n\n*OWNED*\n\n:   Include only items owned by you (Default)\n\n*SHARED_PUBLIC*\n\n:   Include only items shared publicly\n\n*ALL*\n\n:   Include all items you are authorized to view."}
{"filename": "systems.md", "prompt": "Tapis Responses", "completion": "For requests that return a list of resources the response result object\nwill contain the list of resource records that match the user\\'s query\nand the response metadata object will contain information related to\nsorting and limiting.\n\nThe metadata object will contain the following information:\n\n> -   `recordCount` - Actual number of records returned.\n> -   `recordLimit` - The limit query parameter specified in the\n>     request. -1 if query parameter was not specified.\n> -   `recordsSkipped` - The skip query parameter specified in the\n>     request. -1 if query parameter was not specified.\n> -   `orderBy` - The orderBy query parameter specified in the request.\n>     Empty string if query parameter was not specified.\n> -   `startAfter` - The startAfter query parameter specified in the\n>     request. Empty string if query parameter was not specified.\n> -   `totalCount` - Total number of records that would have been\n>     returned without a limit query parameter being imposed. -1 if\n>     total count was not computed.\n\nFor performance reasons computation of `totalCount` is only determined\non demand. This is controlled by the boolean query parameter\n`computeTotal`. By default `computeTotal` is *false*.\n\nExample query and response:\n\nQuery:\n\n    $ curl -H \"X-Tapis-Token: $JWT\" https://tacc.tapis.io/v3/systems?limit=2&orderBy=id(desc)\n\nResponse:\n\n    {\n     \"result\": [\n       {\n         \"id\": \"testMin0\",\n         \"systemType\": \"S3\",\n         \"owner\": \"testuser\",\n         \"host\": \"my.example.host\",\n         \"defaultAccessMethod\": \"ACCESS_KEY\",\n         \"canExec\": false\n       }
,\n       {\n         \"id\": \"MinSystem1c\",\n         \"systemType\": \"LINUX\",\n         \"owner\": \"testuser\",\n         \"defaultAccessMethod\": \"PASSWORD\",\n         \"host\": \"data.tacc.utexas.edu\",\n         \"canExec\": true\n       
\n     ],\n     \"status\": \"success\",\n     \"message\": \"TAPIS_FOUND Systems found: 2 systems\",\n     \"version\": \"1.0.0\",\n     \"metadata\": {\n       \"recordCount\": 2,\n       \"recordLimit\": 2,\n       \"recordsSkipped\": 0,\n       \"orderBy\": \"id(desc)\",\n       \"startAfter\": null,\n       \"totalCount\": -1\n     
"
