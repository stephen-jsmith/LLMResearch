{"prompt": "What types of containers does the Tapis v3 Jobs service support?", "completion": "The Tapis v3 Jobs service supports Docker and Singularity containers."}
{"prompt": "What are the steps taken when a job request is received by the Tapis v3 Jobs service?", "completion": "When a job request is received, the following steps are taken: 1. Request authorization - The tenant, owner, and user values from the request and Tapis JWT are used to authorize access to the application, execution system and, if specified, archive system. 2. Request validation - Request values are checked for missing, conflicting or improper values; all paths are assigned; required paths are created on the execution system; and macro substitution is performed to finalize all job parameters. 3. Job creation - A Tapis job object is written to the database. 4. Job queuing - The Tapis job is queue on an internal queue serviced by one or more Job Worker processes. 5. Response - The initial Job object is sent back to the caller in the response. This ends the synchronous portion of job submission."}
{"prompt": "What is the status of the 'Submit' method in the Beta release of Jobs?", "completion": "The 'Submit' method in the Beta release of Jobs is implemented."}{"prompt": "What are the required parameters for a job submission request?", "completion": "A job submission request must contain the 'name', 'appId' and 'appVersion' values. These are required parameters for a job submission request."}

{"prompt": "What is the default value for the 'archiveOnAppError' parameter in a job submission request?", "completion": "The default value for the 'archiveOnAppError' parameter in a job submission request is 'true'."}

{"prompt": "How is the 'jobType' parameter determined in a job request?", "completion": "The 'jobType' parameter is determined as follows: 1. If the user specifies jobType in the job request, it is used. 2. Otherwise, if the app.jobType != null, it is used. 3. Otherwise, the execution system is queried and jobType=BATCH if execSys.canRunBatch==true. 4. Otherwise, jobType is set to FORK."}{"prompt": "What are the default directory assignments in a Tapis system?", "completion": "The default directory assignments in a Tapis system are: rootDir - the root of the file system that is accessible through this Tapis system, jobWorkingDir - the default directory for temporary files used or created during job execution, and dtnMountPoint - the path relative to the execution system's rootDir where the DTN file system is mounted."}

{"prompt": "How are the execution and archive system directories assigned during a job request submission?", "completion": "When a job request is submitted, each of the job's four execution and archive system directories are assigned as follows: 1. If the job submission request assigns the directory, that value is used. Otherwise, 2. If the application definition assigns the directory, that value is used. Otherwise, 3. The default values are assigned."}

{"prompt": "What are the rules governing how job inputs are calculated in Tapis?", "completion": "The rules governing how job inputs are calculated in Tapis are: 1. The effective inputs to a job are the combined inputs from the application and job request. 2. Only named inputs are allowed in application definitions. 3. Application defined inputs are either REQUIRED, OPTIONAL or FIXED. 4. Applications can restrict the number and definitions of inputs (strictFileInputs=true). 5. Anonymous (unnamed) inputs can be specified in the job request unless prohibited by the application definition (strictFileInputs=true). 6. Job request inputs override values set in the application except for FIXED inputs. 7. The tapislocal URL scheme specifies in-place inputs for which transfers are not performed."}{"prompt": "What is the purpose of the 'tapislocal' URL scheme in Tapis?", "completion": "The 'tapislocal' URL scheme is introduced by Tapis and is only recognized by the Applications and Jobs services. It is used to indicate that a filepath already exists on the execution system and, therefore, does not require data transfer during job execution. It is commonly used for large data sets that cannot reasonably be copied to be mounted directly onto execution systems."}

{"prompt": "What is the 'fileInputArrays' parameter and how is it used in Tapis?", "completion": "The 'fileInputArrays' parameter provides an alternative syntax for specifying inputs in Applications and job requests. It is convenient for specifying multiple inputs destined for the same target directory, an I/O pattern sometimes referred to as 'scatter-gather'. It is an array of JobFileInputArray objects, each of which contains an array of 'sourceUrls' and a single 'targetDir'. However, 'tapislocal' URLs cannot appear in 'sourceUrls' fields."}

{"prompt": "What is the 'parameterSet' argument in a Tapis job and what does it consist of?", "completion": "The job 'parameterSet' argument in Tapis is comprised of several objects including 'appArgs', 'containerArgs', 'schedulerOptions', 'envVariables', 'archiveFilter', and 'logConfig'. These objects can be specified in Tapis application definitions and/or in job submission requests. They are used to pass arguments to the user's application, container runtime, HPC batch scheduler, inject environment variables into the application container, select file archiving, and redirect stdout and stderr respectively."}{"prompt": "How do I specify command line arguments for the user application?", "completion": "You can specify one or more command line arguments for the user application using the *appArgs* parameter. Arguments specified in the application definition are appended to those in the submission request."}
{"prompt": "How can I specify key/value pairs that will be injected as environment variables into the application's container?", "completion": "You can specify key/value pairs that will be injected as environment variables into the application's container when it's launched using the *envVariables* parameter. Key/value pairs specified in the execution system definition, application definition, and job submission request are aggregated using precedence ordering (system < app < request) to resolve conflicts."}
{"prompt": "How can I use the *archiveFilter* to specify which files should be archived?", "completion": "An *archiveFilter* can be specified in the application definition and/or the job submission request. The *includes* and *excludes* arrays are merged by appending entries from the application definition to those in the submission request. The *excludes* filter is applied first, so it takes precedence over *includes*. If *excludes* is empty, then no output file or directory will be explicitly excluded from archiving. If *includes* is empty, then all files in *execSystemOutputDir* will be archived unless explicitly excluded. If *includes* is not empty, then only files and directories that match an entry and not explicitly excluded will be archived."}{"prompt": "How can users subscribe to job execution events?", "completion": "Users can subscribe to job execution events through the application definition and job request. These subscriptions are merged to produce a job's initial subscription list. New subscriptions can be added while a job is running, but not after the job has terminated. A job's subscriptions can be listed and deleted. Only job owners or tenant administrators can subscribe to a job. The 'ttlminutes' parameter can specify up to 4 weeks. If the parameter is not specified or if it's set to 0, a default value of 1 week is used."}

{"prompt": "What are the event types to which users can subscribe?", "completion": "Users can subscribe to the following event types: JOB_NEW_STATUS, JOB_INPUT_TRANSACTION_ID, JOB_ARCHIVE_TRANSACTION_ID, JOB_SUBSCRIPTION, JOB_SHARE_EVENT, JOB_ERROR_MESSAGE, JOB_USER_EVENT, and ALL. All event types other than JOB_USER_EVENT are generated by Tapis. A JOB_USER_EVENT contains a user-specified payload that can be sent to an active job using the job's UUID."}

{"prompt": "How are job arguments calculated?", "completion": "Job arguments are calculated based on several rules. All argument in application definitions must be named. Application arguments are either REQUIRED, FIXED or one of two optional types. Anonymous (unnamed) argument can be specified in job requests. Job request argument override values set in the application except for FIXED arguments. The final argument ordering is the same as the order specified in the definitions, with application arguments preceding those from the job request. Application arguments maintain their place even when overridden in the job request. The notes field can be any JSON object."}{"prompt": "What is the JSON schema for defining key/value pairs of strings in various ParameterSet components?", "completion": "The JSON schema for defining key/value pairs of strings in various ParameterSet components is as follows: \n\n\"KeyValuePair\": {\n    \"$comment\": \"A simple key/value pair\",\n    \"type\": \"object\",\n        \"properties\": {\n            \"key\":   {\"type\": \"string\", \"minLength\": 1},\n            \"value\": {\"type\": \"string\", \"minLength\": 0},\n            \"description\": {\"type\": \"string\", \"minLength\": 1, \"maxLength\": 2048}\n        },\n     \"required\": [\"key\", \"value\"],\n     \"additionalProperties\": false\n}\n\nBoth the *key* and *value* are required, though the *value* can be an empty string. Descriptions are optional."}

{"prompt": "How does the logConfig JSON schema work?", "completion": "The logConfig JSON schema is used to redirect stdout and stderr to named file(s) in supported runtimes. The schema is as follows: \n\n\"logConfig\": {\n    \"$comment\": \"Log file redirection and customization in supported runtimes\",\n    \"type\": \"object\",\n    \"required\": [ \"stdoutFilename\", \"stderrFilename\" ],\n    \"additionalProperties\": false,\n        \"properties\": {\n            \"stdoutFilename\": {\"type\": \"string\", \"minLength\": 1},\n            \"stderrFilename\": {\"type\": \"string\", \"minLength\": 1}\n        }\n}\n\nCurrently, only the Singularity (Apptainer) runtime is supported. When specified, both file name fields must be explicitly assigned, though they can be assigned to the same file. If a logConfig object is not specified, or in runtimes where it's not supported, then both stdout and stderr are directed to the default tapisjob.out file in the job's output directory. Output files, even when logConfig is used, are always relative to the ExecSystemOuputDir."}

{"prompt": "What are the standard environment variables passed into each application container run by Tapis?", "completion": "The following standard environment variables are passed into each application container run by Tapis as long as they have been assigned a value: \n\n_tapisAppId - Tapis app ID\n_tapisAppVersion - Tapis app version\n_tapisArchiveOnAppError - true means archive even if the app returns a non-zero exit code\n_tapisArchiveSystemDir - the archive system directory on which app output is archived\n_tapisArchiveSystemId - Tapis system used for archiving app output\n_tapisCoresPerNode - number of cores used per node by app\n_tapisDtnMountPoint - the mountpoint on the execution system for the source DTN directory\n_tapisDtnMountSourcePath - the directory exported by the DTN and mounted on the execution system\n_tapisDtnSystemId - the Data Transfer Node system ID\n_tapisDynamicExecSystem - true if dynamic system selection was used\n_tapisEffeciveUserId - the user ID under which the app runs\n_tapisExecSystemExecDir - the exec system directory where app artifacts are staged\n_tapisExecSystemHPCQueue - the actual batch queue name on an HPC host\n_tapisExecSystemId - the Tapis system where the app runs\n_tapisExecSystemInputDir - the exec system directory where input files are staged\n_tapisExecSystemLogicalQueue - the Tapis queue definition that specifies an HPC queue\n_tapisExecSystemOutputDir - the exec system directory where the app writes its output\n_tapisJobCreateDate - ISO 8601 date, example: 2021-04-26Z\n_tapisJobCreateTime - ISO 8601 time, example: 18:44:55.544145884Z\n_tapisJobCreateTimestamp - ISO 8601 timestamp, example: 2021-04-26T18:44:55.544145884Z\n_tapisJobName - the user-chosen name of the Tapis job\n_tapisJobOwner - the Tapis job's owner\n_tapisJobUUID - the UUID of the Tapis job\n_tapisJobWorkingDir - exec system directory that the app should use for temporary files\n_tapisMaxMinutes - the maximum number of minutes allowed for the job to run\n_tapisMemoryMB - the memory required per node by the app\n_tapisNodes - the number of nodes on which the app runs\n_tapisSysBatchScheduler - the HPC scheduler on the execution system\n_tapisSysBucketName - an object store bucket name\n_tapisSysHost - the IP address or DNS name of the exec system\n_tapisSysRootDir - the root directory on the exec system\n_tapisTenant - the tenant in which the job runs."}{"prompt": "What is the function of the HOST_EVAL macro in Tapis?", "completion": "The HOST_EVAL macro function in Tapis is used in directory assignments in systems, applications and job requests. It dynamically extracts the value of a named environment variable from an execution or archive host at the time the job request is made. The environment variable's value is retrieved by logging into the host as the Job owner and issuing \"echo $var\". An optional default value can be passed into the HOST_EVAL function for increased application portability. If the environment variable does not exist on the host, then the literal path parameter is returned by the function."}

{"prompt": "What are the possible states of a Tapis job?", "completion": "A Tapis job can be in one of the following states: PENDING, PROCESSING_INPUTS, STAGING_INPUTS, STAGING_JOB, SUBMITTING_JOB, QUEUED, RUNNING, ARCHIVING, BLOCKED, PAUSED, FINISHED, CANCELLED, and FAILED. The initial state is PENDING and the terminal states are FINISHED, CANCELLED and FAILED. The BLOCKED state indicates that the job is recovering from a resource constraint, network problem or other transient problem. When the problem clears, the job will restart from the state in which blocking occurred."}

{"prompt": "What information is included in the notification messages sent by the Jobs service in Tapis?", "completion": "The notification messages sent by the Jobs service in Tapis include a JSON object in the data field that always contains the following fields: jobName, jobOwner, jobUuid, and message. Depending on the Job event type, additional fields may be included. For example, JOB_NEW_STATUS includes newJobStatus and oldJobStatus, JOB_INPUT_TRANSACTION_ID includes transferStatus and transactionId, and JOB_ERROR_MESSAGE includes jobStatus. If the JOB_NEW_STATUS messages indicate a terminal newJobStatus, or JOB_ERROR_MESSAGE messages have eventDetail = \"FINAL_MESSAGE\", additional fields such as blockedCount, remoteJobId, remoteJobId2, remoteOutcome, remoteResultInfo, remoteQueue, remoteSubmitted, remoteStarted, and remoteEnded are included."}{"prompt": "How does the Tapis v3 Jobs service support Docker and Singularity containers?", "completion": "The Tapis v3 Jobs service supports Docker and Singularity containers by running them natively, without the use of a batch scheduler like Slurm. Jobs launches an application's container on a remote system, monitors the container's execution, and captures the application's exit code after it terminates. Jobs uses SSH to connect to the execution system to issue Docker, Singularity or native operating system commands. To launch a job, the Jobs service creates a bash script, tapisjob.sh, with the runtime-specific commands needed to execute the container."}

{"prompt": "How does the Tapis v3 Jobs service launch a Docker container?", "completion": "To launch a Docker container, the Jobs service will SSH to the target host and issue a command using this template: docker run [docker options] image[:tag|@digest] [application args]. The docker options, image, and application arguments are specified by the user. The container name is set to the job UUID, the container's user is set to the user ID used to establish the SSH session, the container ID file is specified as <JobUUID>.cid in the execSystemExecDir, and the container is removed after execution using the -rm option or by calling docker rm."}

{"prompt": "What are the two ways Tapis provides to launch a Singularity container?", "completion": "Tapis provides two distinct ways to launch a Singularity containers, using singluarity instance start or singularity run. For instance start, the Jobs service will SSH to the target host and issue a command using a specific template. For singularity run, the Jobs service will SSH to the target host and issue a command using a different template. In both cases, the singularity options, image id, and application arguments are specified by the user."}{"prompt": "What are the requirements for using Singularity start and Singularity run approaches?", "completion": "Singularity start requires the startscript to be defined in the image. Singularity run requires the runscript to be defined in the image. Both approaches allow SSH sessions between Jobs and execution hosts to end without interrupting container execution."}
{"prompt": "What is the recommended termination order for the initially launched program?", "completion": "The initially launched program should be the last part of the application to terminate. It can spawn any number of processes (and threads), but it should not exit before those processes complete."}
{"prompt": "How can I get the details of a specific job?", "completion": "You can get the details of a specific job using PySDK or CURL. The response will include details like job id, name, owner, tenant, status, creation and end time, and other specific details related to the job."}{"prompt": "How can I check the status of a job using PySDK or CURL?", "completion": "You can check the status of a job using PySDK or CURL by sending a request to the Job Status API. The response will contain a 'status' field which will indicate the current status of the job. For example, a 'FINISHED' status means that the job has completed successfully."}

{"prompt": "How can I retrieve the history of a job using PySDK or CURL?", "completion": "You can retrieve the history of a job using PySDK or CURL by sending a request to the Job History API. The response will contain a list of events that occurred during the job's lifecycle, including the time each event occurred, the new status of the job after each event, and a description of each event."}

{"prompt": "How can I get a list of output files from a job using PySDK or CURL?", "completion": "You can get a list of output files from a job using PySDK or CURL by sending a request to the Job Output Listing API. The response will contain a list of files that were output by the job, including information about each file such as its name, path, size, and last modified time. By default, the job must be in a terminal state (FINISHED or FAILED or CANCELLED) for the API to list the job's output files. However, you can set the 'allowIfRunning' query parameter to true to get the list of output files even if the job is not in a terminal state."}{"prompt": "How can I download the output files of a job using the Jobs API?", "completion": "The Jobs output download API retrieves the job's output files for a previously submitted job by its UUID. By default, the job must be in a terminal state (FINISHED or FAILED or CANCELLED) for the API to download the job's output files. There is a query parameter allowIfRunning set to false by default. If allowIfRunning=true, the API allows downloading the job output files even if the job is not in the terminal state. Note that if a file is being written at the time of the request, the file is still downloaded with the current content."}

{"prompt": "How can I search for jobs using the dedicated search endpoint in the jobs service?", "completion": "The jobs service provides dedicated search end-points to query jobs based on different conditions. The GET end-point allows to specify the query in the query parameters while the POST end-point allows complex queries in the request body using SQL-like syntax."}

{"prompt": "How can I cancel a job that has been previously submitted and is not in a terminal state?", "completion": "A previously submitted job not in terminal state can be cancelled by its UUID. The response will indicate that the request to cancel the job has been accepted. If the job is in a terminal state, the request will have no effect. If the job is transitioning between active and blocked states, another cancel request may need to be sent."}{"prompt": "How can I share a job with another user in the same tenant?", "completion": "A previously submitted job can be shared with a user in the same tenant. The job resources that can be shared are: JOB_HISTORY, JOB_RESUBMIT_REQUEST, JOB_OUTPUT. Currently only READ permission on the resources are allowed."}
{"prompt": "How can I retrieve information about a shared job?", "completion": "You can retrieve information about a shared job using PySDK or CURL. The response will include details such as the tenant, who created the job, the job UUID, the grantee, the job resource, the job permission, and when it was created."}
{"prompt": "How can I unshare a job that I previously shared?", "completion": "You can unshare a previously shared job using PySDK or CURL. The response will confirm that the job resource has been unshared."}{"prompt": "How can I share job output with another user in the same tenant?", "completion": "You can share job output with another user, for example 'testuser5', in the same tenant. This will allow 'testuser5' to list and download the job output using their own JWT, even if they are not the owner of the job."}

{"prompt": "What actions can a user perform on a shared job output?", "completion": "A user with whom a job output has been shared can perform actions such as listing the job output and downloading the job output. They can do this using their own JWT, even if they are not the owner of the job."}

{"prompt": "Can a user access the history of a job that has been shared with them?", "completion": "Yes, a user with whom a job has been shared can access the job's history, status, and detailed information. They can do this using their own JWT."}