{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code search\n",
    "\n",
    "We index our own [openai-python code repository](https://github.com/openai/openai-python), and show how it can be searched. We implement a simple version of file parsing and extracting of functions from python files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Authenticate with OpenAI API\n",
    "openai.api_key = os.environ.get('OPENAI')\n",
    "\n",
    "# Root directory where the Python repo is located\n",
    "ROOT_DIR = \"/Users/stephen/LLMResearch\"\n",
    "CODE_REPO = \"LearnMPM\"\n",
    "\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of py files: 0\n",
      "Double check that you have downloaded the repo and set the code_root variable correctly.\n",
      "Total number of functions extracted: 0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_function_name(code):\n",
    "    \"\"\"\n",
    "    Extract function name from a line beginning with \"def \"\n",
    "    \"\"\"\n",
    "    assert code.startswith(\"def \")\n",
    "    return code[len(\"def \"): code.index(\"(\")]\n",
    "\n",
    "def get_until_no_space(all_lines, i) -> str:\n",
    "    \"\"\"\n",
    "    Get all lines until a line outside the function definition is found.\n",
    "    \"\"\"\n",
    "    ret = [all_lines[i]]\n",
    "    for j in range(i + 1, i + 10000):\n",
    "        if j < len(all_lines):\n",
    "            if len(all_lines[j]) == 0 or all_lines[j][0] in [\" \", \"\\t\", \")\"]:\n",
    "                ret.append(all_lines[j])\n",
    "            else:\n",
    "                break\n",
    "    return \"\\n\".join(ret)\n",
    "\n",
    "def get_functions(filepath):\n",
    "    \"\"\"\n",
    "    Get all functions in a Python file.\n",
    "    \"\"\"\n",
    "    whole_code = open(filepath).read().replace(\"\\r\", \"\\n\")\n",
    "    all_lines = whole_code.split(\"\\n\")\n",
    "    for i, l in enumerate(all_lines):\n",
    "        if l.startswith(\"def \"):\n",
    "            code = get_until_no_space(all_lines, i)\n",
    "            function_name = get_function_name(code)\n",
    "            yield {\"code\": code, \"function_name\": function_name, \"filepath\": filepath}\n",
    "\n",
    "\n",
    "# get user root directory\n",
    "root_dir = os.path.expanduser(ROOT_DIR)\n",
    "\n",
    "# path to code repository directory\n",
    "code_root = root_dir + CODE_REPO\n",
    "\n",
    "code_files = [y for x in os.walk(code_root) for y in glob(os.path.join(x[0], '*.py'))]\n",
    "print(\"Total number of py files:\", len(code_files))\n",
    "\n",
    "if len(code_files) == 0:\n",
    "    print(\"Double check that you have downloaded the repo and set the code_root variable correctly.\")\n",
    "\n",
    "all_funcs = []\n",
    "for code_file in code_files:\n",
    "    funcs = list(get_functions(code_file))\n",
    "    for func in funcs:\n",
    "        all_funcs.append(func)\n",
    "\n",
    "print(\"Total number of functions extracted:\", len(all_funcs))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embedding and write to a CSV file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roberta Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-roberta-large-v1')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-roberta-large-v1')\n",
    "\n",
    "def roberta_embedding(text):\n",
    "    # Tokenize sentences\n",
    "    encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    # Compute token embeddings\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**encoded_input)\n",
    "\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "\n",
    "    return sentence_embeddings[0]    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bloom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BloomTokenizerFast, BloomModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloom-560m\")\n",
    "model = BloomModel.from_pretrained(\"bigscience/bloom-560m\")\n",
    "def bloom_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    embedding = model(**inputs)[0][0, -1, :]\n",
    "    return embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "\n",
    "def gpt2_embedding(text):\n",
    "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "    # Get the vector embedding\n",
    "    embedding = model(input_ids)[0][0, -1, :]\n",
    "    return embedding\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"count the number of tokens in a string\"\"\"\n",
    "    return len(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (580 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>function_name</th>\n",
       "      <th>filepath</th>\n",
       "      <th>tokens</th>\n",
       "      <th>code_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>def nodal_total_force(mesh):\\n    \"\"\"\\n    Com...</td>\n",
       "      <td>nodal_total_force</td>\n",
       "      <td>/update.py</td>\n",
       "      <td>129</td>\n",
       "      <td>[-0.008018724620342255, 0.025516413152217865, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>def nodal_acceleration_velocity(mesh, dt):\\n  ...</td>\n",
       "      <td>nodal_acceleration_velocity</td>\n",
       "      <td>/update.py</td>\n",
       "      <td>249</td>\n",
       "      <td>[0.008488386869430542, 0.024847134947776794, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def nodal_velocity(mesh):\\n    \"\"\"Compute noda...</td>\n",
       "      <td>nodal_velocity</td>\n",
       "      <td>/update.py</td>\n",
       "      <td>125</td>\n",
       "      <td>[-0.0038946340791881084, 0.013509807176887989,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>def fix_nodal_bc_momentum(mesh):\\n    \"\"\"Set m...</td>\n",
       "      <td>fix_nodal_bc_momentum</td>\n",
       "      <td>/update.py</td>\n",
       "      <td>133</td>\n",
       "      <td>[-0.0158998966217041, -0.005269299261271954, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>def fix_nodal_bc_force(mesh):\\n    \"\"\"Set noda...</td>\n",
       "      <td>fix_nodal_bc_force</td>\n",
       "      <td>/update.py</td>\n",
       "      <td>111</td>\n",
       "      <td>[-0.01658795401453972, 0.004961120896041393, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code  \\\n",
       "0  def nodal_total_force(mesh):\\n    \"\"\"\\n    Com...   \n",
       "1  def nodal_acceleration_velocity(mesh, dt):\\n  ...   \n",
       "2  def nodal_velocity(mesh):\\n    \"\"\"Compute noda...   \n",
       "3  def fix_nodal_bc_momentum(mesh):\\n    \"\"\"Set m...   \n",
       "4  def fix_nodal_bc_force(mesh):\\n    \"\"\"Set noda...   \n",
       "\n",
       "                 function_name    filepath  tokens  \\\n",
       "0            nodal_total_force  /update.py     129   \n",
       "1  nodal_acceleration_velocity  /update.py     249   \n",
       "2               nodal_velocity  /update.py     125   \n",
       "3        fix_nodal_bc_momentum  /update.py     133   \n",
       "4           fix_nodal_bc_force  /update.py     111   \n",
       "\n",
       "                                      code_embedding  \n",
       "0  [-0.008018724620342255, 0.025516413152217865, ...  \n",
       "1  [0.008488386869430542, 0.024847134947776794, -...  \n",
       "2  [-0.0038946340791881084, 0.013509807176887989,...  \n",
       "3  [-0.0158998966217041, -0.005269299261271954, -...  \n",
       "4  [-0.01658795401453972, 0.004961120896041393, -...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(all_funcs)\n",
    "df['tokens'] = df['code'].apply(lambda x: count_tokens(x))\n",
    "# Include only rows with < 1024 tokens\n",
    "df = df[df.tokens<1024]\n",
    "### OpenAI GPT3 \n",
    "df['code_embedding'] = df['code'].apply(lambda x: get_embedding(x, engine=EMBEDDING_MODEL))\n",
    "### GPT2 \n",
    "# df['code_embedding'] = df['code'].apply(lambda x: gpt2_embedding(x))\n",
    "### Bloom\n",
    "# df['code_embedding'] = df['code'].apply(lambda x: bloom_embedding(x))\n",
    "### Roberta\n",
    "# df['code_embedding'] = df['code'].apply(lambda x: roberta_embedding(x))\n",
    "\n",
    "df['filepath'] = df['filepath'].apply(lambda x: x.replace(code_root, \"\"))\n",
    "df.to_csv(\"code_search_openai-python.csv\", index=False)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read embeddings from a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>function_name</th>\n",
       "      <th>filepath</th>\n",
       "      <th>tokens</th>\n",
       "      <th>code_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>def nodal_total_force(mesh):\\n    \"\"\"\\n    Com...</td>\n",
       "      <td>nodal_total_force</td>\n",
       "      <td>/update.py</td>\n",
       "      <td>129</td>\n",
       "      <td>[-0.008052323944866657, 0.025560934096574783, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>def nodal_acceleration_velocity(mesh, dt):\\n  ...</td>\n",
       "      <td>nodal_acceleration_velocity</td>\n",
       "      <td>/update.py</td>\n",
       "      <td>249</td>\n",
       "      <td>[0.008488386869430542, 0.024847134947776794, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def nodal_velocity(mesh):\\n    \"\"\"Compute noda...</td>\n",
       "      <td>nodal_velocity</td>\n",
       "      <td>/update.py</td>\n",
       "      <td>125</td>\n",
       "      <td>[-0.0038946340791881084, 0.013509807176887989,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>def fix_nodal_bc_momentum(mesh):\\n    \"\"\"Set m...</td>\n",
       "      <td>fix_nodal_bc_momentum</td>\n",
       "      <td>/update.py</td>\n",
       "      <td>133</td>\n",
       "      <td>[-0.01586133986711502, -0.005389683414250612, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>def fix_nodal_bc_force(mesh):\\n    \"\"\"Set noda...</td>\n",
       "      <td>fix_nodal_bc_force</td>\n",
       "      <td>/update.py</td>\n",
       "      <td>111</td>\n",
       "      <td>[-0.01658795401453972, 0.004961120896041393, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code  \\\n",
       "0  def nodal_total_force(mesh):\\n    \"\"\"\\n    Com...   \n",
       "1  def nodal_acceleration_velocity(mesh, dt):\\n  ...   \n",
       "2  def nodal_velocity(mesh):\\n    \"\"\"Compute noda...   \n",
       "3  def fix_nodal_bc_momentum(mesh):\\n    \"\"\"Set m...   \n",
       "4  def fix_nodal_bc_force(mesh):\\n    \"\"\"Set noda...   \n",
       "\n",
       "                 function_name    filepath  tokens  \\\n",
       "0            nodal_total_force  /update.py     129   \n",
       "1  nodal_acceleration_velocity  /update.py     249   \n",
       "2               nodal_velocity  /update.py     125   \n",
       "3        fix_nodal_bc_momentum  /update.py     133   \n",
       "4           fix_nodal_bc_force  /update.py     111   \n",
       "\n",
       "                                      code_embedding  \n",
       "0  [-0.008052323944866657, 0.025560934096574783, ...  \n",
       "1  [0.008488386869430542, 0.024847134947776794, -...  \n",
       "2  [-0.0038946340791881084, 0.013509807176887989,...  \n",
       "3  [-0.01586133986711502, -0.005389683414250612, ...  \n",
       "4  [-0.01658795401453972, 0.004961120896041393, -...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Only works with OpenAI model\n",
    "df = pd.read_csv(\"code_search_openai-python.csv\")\n",
    "df['code_embedding'] = df['code_embedding'].apply(lambda x: [float(i) for i in x[1:-1].split(',')])\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(x, y):\n",
    "    return np.dot(np.array(x), np.array(y))\n",
    "\n",
    "def torch_cosine_similarity(x, y):\n",
    "    dot_product = torch.dot(x, y)\n",
    "    norm_x = torch.norm(x)\n",
    "    norm_y = torch.norm(y)\n",
    "    cosine_similarity = dot_product / (norm_x * norm_y)\n",
    "    return cosine_similarity\n",
    "\n",
    "def search_functions(df, code_query, n=3, pprint=True, n_lines=7):\n",
    "    # OpenAI\n",
    "    embedding = get_embedding(code_query, engine=EMBEDDING_MODEL)\n",
    "    df['similarities'] = df.code_embedding.apply(lambda x: compute_similarity(x, embedding))\n",
    "    # GPT2\n",
    "    # embedding = gpt2_embedding(code_query)\n",
    "    # Bloom\n",
    "    # embedding = bloom_embedding(code_query)\n",
    "    # Roberta\n",
    "    # embedding = roberta_embedding(code_query)\n",
    "    # df['similarities'] = df.code_embedding.apply(lambda x: torch_cosine_similarity(x, embedding))\n",
    "\n",
    "    res = df.sort_values('similarities', ascending=False).head(n)\n",
    "    \n",
    "    if pprint:\n",
    "        for r in res.iterrows():\n",
    "            # print(r[1].filepath+\":\"+r[1].function_name + \"  score=\" + str(round(r[1].similarities, 3)))\n",
    "            print(r[1].filepath+\":\"+r[1].function_name + \"  score=\" + str(r[1].similarities))\n",
    "            print(\"\\n\".join(r[1].code.split(\"\\n\")[:n_lines]))\n",
    "            print('-'*70)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/update.py:particle_position_velocity  score=0.755101060421284\n",
      "def particle_position_velocity(mesh, dt):\n",
      "    \"\"\"\n",
      "    Compute particle position and velocity based on nodal velocity. :math:`x_p += \\sum_i N_i(x_p) * v_i` and particle position :math:`x_p += v_p * dt`.\n",
      "\n",
      "    Arguments:\n",
      "        mesh: mesh\n",
      "            a mesh object\n",
      "----------------------------------------------------------------------\n",
      "/update.py:particle_velocity  score=0.7495547431425625\n",
      "def particle_velocity(mesh, dt):\n",
      "    \"\"\"\n",
      "    Compute particle velocity transfer nodal velocity to particle. :math:`v_p += \\sum_i N_i(x_p) * {f_{total}}_i/m_i * dt`.\n",
      "\n",
      "    Arguments:\n",
      "        mesh: mesh\n",
      "            a mesh object\n",
      "----------------------------------------------------------------------\n",
      "/update.py:nodal_velocity  score=0.748313075443582\n",
      "def nodal_velocity(mesh):\n",
      "    \"\"\"Compute nodal velocity as :math:`v = mv / m`.\n",
      "\n",
      "    Arguments:\n",
      "        mesh: mesh\n",
      "            a mesh object\n",
      "    \"\"\"\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "res = search_functions(df, 'How do I map velocity from nodes to the material points?', n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/solver.py:compute_stress  score=0.762776533735259\n",
      "def compute_stress(mesh, params):\n",
      "    \"\"\"compute stress update: (1) compute nodal velocity, (2) particle strain increment, \n",
      "    (3) update particle volume and density based on `dstrain`, and (4) compute particle stress.\n",
      "\n",
      "    Arguments:\n",
      "        mesh: Mesh object\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "/update.py:particle_stress  score=0.7509802901357261\n",
      "def particle_stress(mesh):\n",
      "    \"\"\"Update particle stress based on dstrain.\n",
      "\n",
      "    Arguments:\n",
      "        mesh: mesh\n",
      "            a mesh object\n",
      "    \"\"\"\n",
      "----------------------------------------------------------------------\n",
      "/interpolate.py:internal_force_to_nodes  score=0.7387064460346807\n",
      "def internal_force_to_nodes(mesh):\n",
      "\t\"\"\"Map internal force to nodes. The nodal stresses are mapped to the nodes as internal force. The nodal internal force :math:`f_{int} = -\\sum_i dN_i \\sigma_p m_p / P_p`.\n",
      "\n",
      "        Args:\n",
      "            mesh: mesh object with nodes, elements and particles\t\n",
      "\t\"\"\"\n",
      "\tfor el in mesh.elements:\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "res = search_functions(df, 'How do I compute stresses at the material points?', n=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a15fe14c9164b2c84764451972c480ab7caecb14ffdaafbc4f746bd44fda90e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
