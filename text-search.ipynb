{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT user support\n",
    "\n",
    "> Semantic search enabled via GPT and context-specific responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "COMPLETIONS_MODEL = \"text-davinci-003\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "# Authenticate with OpenAI API\n",
    "with open('apiKeys.txt', 'r') as temp:\n",
    "    apiKey = temp.read()\n",
    "client = OpenAI(api_key=apiKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt4(question, tokens=500):\n",
    "    messages=[{\"role\": \"user\", \"content\": question}]\n",
    "\n",
    "    response = client.chat.completions.create(model=\"gpt-4\",\n",
    "                                                max_tokens=tokens,\n",
    "                                                temperature=0,\n",
    "                                                messages=messages)\n",
    "\n",
    "    # Extract the content\n",
    "    content = response.choices[0].message.content\n",
    "\n",
    "    # Split the content into text and code\n",
    "    text_parts = []\n",
    "    code_parts = []\n",
    "    in_code_block = False\n",
    "\n",
    "    for line in content.split(\"\\n\"):\n",
    "        if line.startswith(\"```\"):\n",
    "            in_code_block = not in_code_block\n",
    "            continue\n",
    "        if in_code_block:\n",
    "            code_parts.append(line)\n",
    "        else:\n",
    "            text_parts.append(line)\n",
    "\n",
    "    # Print the text parts\n",
    "    for line in text_parts:\n",
    "        print(line)\n",
    "\n",
    "    # Print a separator\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "    # Print the code parts\n",
    "    for line in code_parts:\n",
    "        print(line)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Hallucination (lying)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"How to generate a token using Tapipy\"\n",
    "\n",
    "gpt4(prompt,300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ther is no website called \"Tapipy\" to create an account...all these are wrong!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forcing GPT to not lie!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Answer the question as truthfully as possible, and if you're unsure of the answer, say \"Sorry, I don't know\".\n",
    "\n",
    "Q: How to generate a token using Tapipy?\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "gpt4(prompt,300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well....that was very helpful!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Providing Context to GPT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What if we could provide GPT with some context so it can provide useful help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Answer the question as truthfully as possible, and if you're unsure of the answer, say \"Sorry, I don't know\".\n",
    "\n",
    "Context: \n",
    "Create an Tapis Client Object\n",
    "\n",
    "The first step in using the Tapis Python SDK, tapipy, is to create a Tapis Client object. First, import the Tapis class and create python object called t that points to the Tapis server using your TACC username and password. Do so by typing the following in a Python shell:\n",
    "\n",
    "# Import the Tapis object\n",
    "from tapipy.tapis import Tapis\n",
    "\n",
    "# Log into you the Tapis service by providing user/pass and url.\n",
    "t = Tapis(base_url='https://tacc.tapis.io',\n",
    "          username='your username',\n",
    "          password='your password')\n",
    "\n",
    "Generate a Token\n",
    "\n",
    "With the t object instantiated, we can exchange our credentials for an access token. In Tapis, you never send your username and password directly to the services; instead, you pass an access token which is cryptographically signed by the OAuth server and includes information about your identity. The Tapis services use this token to determine who you are and what you can do.\n",
    "\n",
    "    # Get tokens that will be used for authenticated function calls\n",
    "    t.get_tokens()\n",
    "    print(t.access_token.access_token)\n",
    "\n",
    "    Out[1]: eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9...\n",
    "\n",
    "Note that the tapipy t object will store and pass your access token for you, so you donâ€™t have to manually provide the token when using the tapipy operations. You are now ready to check your access to the Tapis APIs. It will expire though, after 4 hours, at which time you will need to generate a new token. If you are interested, you can create an OAuth client (a one-time setup step, like creating a TACC account) that can be used to generate access and refresh tokens. For simplicity, we are skipping that but if you are interested, check out the Tenancy and Authentication section.\n",
    "Q: How to generate a token using Tapipy?\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "gpt4(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Create a word embedding as vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import markdown2\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "context = 'actor'\n",
    "\n",
    "# Open the markdown file\n",
    "with open(os.path.join(context + '.md'), \"r\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Use markdown2 to convert the markdown file to html\n",
    "html = markdown2.markdown(content)\n",
    "\n",
    "# Use BeautifulSoup to parse the html\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Initialize variables to store heading, subheading, and corresponding paragraphs\n",
    "headings = []\n",
    "paragraphs = []\n",
    "\n",
    "data = []\n",
    "\n",
    "MAX_WORDS = 500\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"count the number of tokens in a string\"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "# Iterate through the tags in the soup\n",
    "for tag in soup.descendants:\n",
    "    # Check if the tag is a heading\n",
    "    if tag.name in [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]:\n",
    "        # When the next heading is encountered, print the heading, subheading, and corresponding paragraphs\n",
    "        if headings and paragraphs:\n",
    "            hdgs = \" \".join(headings)\n",
    "            para = \" \".join(paragraphs)\n",
    "            data.append([hdgs, para, count_tokens(para)])\n",
    "            headings = []\n",
    "            paragraphs = []\n",
    "        # Add to heading\n",
    "        headings.append(tag.text)\n",
    "    # Check if the tag is a paragraph\n",
    "    elif tag.name == \"p\":\n",
    "        paragraphs.append(tag.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dataset and filter out any sections with fewer than 40 tokens, as those are unlikely to contain enough context to ask a good question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=[\"heading\", \"content\", \"tokens\"])\n",
    "df = df[df.tokens>40]\n",
    "df = df.reset_index().drop('index',axis=1) # reset index\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, model: str=EMBEDDING_MODEL):\n",
    "    result = client.embeddings.create(model=model,\n",
    "                                        input=text).data[0].embedding\n",
    "    return result\n",
    "\n",
    "\n",
    "def compute_doc_embeddings(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Create an embedding for each row in the dataframe using the OpenAI Embeddings API.\n",
    "    \n",
    "    Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        idx: get_embedding(r.content) for idx, r in df.iterrows()\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding as vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_embedding = compute_doc_embeddings(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vector_embedding'] = pd.Series(vector_embedding)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stash the dataframe so that in the future we can just load it without having to revectorize it. This will save some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join('vectorizedDataFrames', context))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Find the most similar document embeddings to the question embedding\n",
    "\n",
    "We embed the query strip and use it to find the most similar document sections. Since this is a small example, we store and search the embeddings locally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def order_documents_query_similarity(data:pd.DataFrame, query_str:str, nres=3):\n",
    "    embedding = get_embedding(query_str, model=EMBEDDING_MODEL)\n",
    "    data['similarities'] = data.vector_embedding.apply(lambda x: 1-cosine(x, embedding))\n",
    "\n",
    "    res = data.sort_values('similarities', ascending=False).head(nres)\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the most relevant document sections for the token is listed at the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = order_documents_query_similarity(df, \"How to generate a token using Tapipy\")\n",
    "res.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Add the most relevant document sections to the query prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "question =  \"How to generate a token using Tapipy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_prompt(question: str, df: pd.DataFrame, ncontents = 3) -> str:\n",
    "    \"\"\"\n",
    "    Fetch relevant \n",
    "    \"\"\"\n",
    "    most_relevant_document_sections = order_documents_query_similarity(df, question)\n",
    "    \n",
    "    chosen_sections = []\n",
    "    chosen_section_len = 0\n",
    "\n",
    "    MAX_SECTION_LEN = 500\n",
    "    context = order_documents_query_similarity(df, question)\n",
    "    context.head()\n",
    "\n",
    "    for _, ctx in context.iterrows():\n",
    "        chosen_section_len += ctx.tokens\n",
    "        if chosen_section_len > MAX_SECTION_LEN:\n",
    "            break\n",
    "            \n",
    "        chosen_sections.append(\" \" + ctx.content.replace(\"\\n\", \" \"))\n",
    "    \n",
    "    header = \"\"\"Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\\n\\nContext:\\n\"\"\"\n",
    "    \n",
    "    return header + \"\".join(chosen_sections) + \"\\n\\n Q: \" + question + \"\\n A:\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_prompt(question=\"How to generate a token using Tapipy\", df=df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Answer the user's question based on the context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETIONS_API_PARAMS = {\n",
    "    # We use temperature of 0.0 because it gives the most predictable, factual answer.\n",
    "    \"temperature\": 0.0,\n",
    "    \"max_tokens\": 300,\n",
    "    \"model\": COMPLETIONS_MODEL,\n",
    "}\n",
    "\n",
    "def answer_query_with_context(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    show_prompt: bool = False) -> str:\n",
    "    \n",
    "    prompt = construct_prompt(\n",
    "        query,\n",
    "        df\n",
    "    )\n",
    "    \n",
    "    if show_prompt:\n",
    "        print(prompt)\n",
    "\n",
    "    response = client.completions.create(\n",
    "                prompt=prompt,\n",
    "                **COMPLETIONS_API_PARAMS\n",
    "            )\n",
    "\n",
    "    return response.choices[0].text.strip(\" \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original GPT without context - telling lies as it invents a new Tapipy website and App to generate a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"How to generate a token using Tapipy\"\n",
    "# [\"choices\"][0][\"text\"].strip(\" \\n\")\n",
    "client.completions.create(prompt=prompt, temperature=0, max_tokens=300, model=COMPLETIONS_MODEL).choices[0].text.strip(\" \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When you ask a question for which it can find a context! - It answers correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_query_with_context(\"How to generate a token using Tapipy\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When it doesn't know...at least it is honest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_query_with_context(\"How to access files using Tapipy\", df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a15fe14c9164b2c84764451972c480ab7caecb14ffdaafbc4f746bd44fda90e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
