{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT user support\n",
    "\n",
    "> Semantic search enabled via GPT and context-specific responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "COMPLETIONS_MODEL = \"text-davinci-003\"\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "# Authenticate with OpenAI API\n",
    "with open('apiKeys.txt', 'r') as temp:\n",
    "    apiKey = temp.read()\n",
    "client = OpenAI(api_key=apiKey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt4(question, tokens=500):\n",
    "    messages=[{\"role\": \"user\", \"content\": question}]\n",
    "\n",
    "    response = client.chat.completions.create(model=\"gpt-4\",\n",
    "                                                max_tokens=tokens,\n",
    "                                                temperature=0,\n",
    "                                                messages=messages)\n",
    "\n",
    "    # Extract the content\n",
    "    content = response.choices[0].message.content\n",
    "\n",
    "    # Split the content into text and code\n",
    "    text_parts = []\n",
    "    code_parts = []\n",
    "    in_code_block = False\n",
    "\n",
    "    for line in content.split(\"\\n\"):\n",
    "        if line.startswith(\"```\"):\n",
    "            in_code_block = not in_code_block\n",
    "            continue\n",
    "        if in_code_block:\n",
    "            code_parts.append(line)\n",
    "        else:\n",
    "            text_parts.append(line)\n",
    "\n",
    "    # Print the text parts\n",
    "    for line in text_parts:\n",
    "        print(line)\n",
    "\n",
    "    # Print a separator\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "    # Print the code parts\n",
    "    for line in code_parts:\n",
    "        print(line)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT Hallucination (lying)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tapipy (Tenable Application Programming Interface Python) is a Python client library for Tenable.io API. However, it does not directly generate tokens. \n",
      "\n",
      "The token generation is handled by Tenable.io API itself. You can generate API keys (access key and secret key) from your Tenable.io account. Here is how you can do it:\n",
      "\n",
      "1. Log in to your Tenable.io account.\n",
      "2. Click on \"My Account\" in the top right corner.\n",
      "3. Click on \"API Keys\" on the left side menu.\n",
      "4. Click on \"Generate\" button. It will generate a pair of keys: Access Key and Secret Key.\n",
      "\n",
      "Once you have these keys, you can use them in Tapipy to authenticate your requests. Here is an example:\n",
      "\n",
      "\n",
      "Remember to replace 'your_access_key' and 'your_secret_key' with your actual keys.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "from tapipy.tenable_io import TenableIOClient\n",
      "\n",
      "client = TenableIOClient(access_key='your_access_key', secret_key='your_secret_key')\n",
      "\n",
      "# Now you can use client to make requests\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'openai.Completion.create(\\n    prompt=prompt,\\n    temperature=0,\\n    max_tokens=300,\\n    model=COMPLETIONS_MODEL\\n)[\"choices\"][0][\"text\"].strip(\" \\n\")'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"How to generate a token using Tapipy\"\n",
    "\n",
    "gpt4(prompt,300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ther is no website called \"Tapipy\" to create an account...all these are wrong!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forcing GPT to not lie!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, I don't know.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'openai.Completion.create(\\n    prompt=prompt,\\n    temperature=0,\\n    max_tokens=300,\\n    model=COMPLETIONS_MODEL\\n)[\"choices\"][0][\"text\"].strip(\" \\n\")'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"Answer the question as truthfully as possible, and if you're unsure of the answer, say \"Sorry, I don't know\".\n",
    "\n",
    "Q: How to generate a token using Tapipy?\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "gpt4(prompt,300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well....that was very helpful!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Providing Context to GPT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What if we could provide GPT with some context so it can provide useful help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To generate a token using Tapipy, you first need to create a Tapis Client object. After importing the Tapis class and creating a python object that points to the Tapis server using your TACC username and password, you can exchange your credentials for an access token. Here are the steps:\n",
      "\n",
      "1. Import the Tapis object:\n",
      "\n",
      "\n",
      "2. Log into the Tapis service by providing your username, password, and url:\n",
      "\n",
      "\n",
      "3. Generate the tokens:\n",
      "\n",
      "\n",
      "The output will be your access token. The Tapipy object will store and pass your access token for you, so you don’t have to manually provide the token when using the Tapipy operations.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "from tapipy.tapis import Tapis\n",
      "t = Tapis(base_url='https://tacc.tapis.io',\n",
      "          username='your username',\n",
      "          password='your password')\n",
      "t.get_tokens()\n",
      "print(t.access_token.access_token)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Answer the question as truthfully as possible, and if you're unsure of the answer, say \"Sorry, I don't know\".\n",
    "\n",
    "Context: \n",
    "Create an Tapis Client Object\n",
    "\n",
    "The first step in using the Tapis Python SDK, tapipy, is to create a Tapis Client object. First, import the Tapis class and create python object called t that points to the Tapis server using your TACC username and password. Do so by typing the following in a Python shell:\n",
    "\n",
    "# Import the Tapis object\n",
    "from tapipy.tapis import Tapis\n",
    "\n",
    "# Log into you the Tapis service by providing user/pass and url.\n",
    "t = Tapis(base_url='https://tacc.tapis.io',\n",
    "          username='your username',\n",
    "          password='your password')\n",
    "\n",
    "Generate a Token\n",
    "\n",
    "With the t object instantiated, we can exchange our credentials for an access token. In Tapis, you never send your username and password directly to the services; instead, you pass an access token which is cryptographically signed by the OAuth server and includes information about your identity. The Tapis services use this token to determine who you are and what you can do.\n",
    "\n",
    "    # Get tokens that will be used for authenticated function calls\n",
    "    t.get_tokens()\n",
    "    print(t.access_token.access_token)\n",
    "\n",
    "    Out[1]: eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9...\n",
    "\n",
    "Note that the tapipy t object will store and pass your access token for you, so you don’t have to manually provide the token when using the tapipy operations. You are now ready to check your access to the Tapis APIs. It will expire though, after 4 hours, at which time you will need to generate a new token. If you are interested, you can create an OAuth client (a one-time setup step, like creating a TACC account) that can be used to generate access and refresh tokens. For simplicity, we are skipping that but if you are interested, check out the Tenancy and Authentication section.\n",
    "Q: How to generate a token using Tapipy?\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "gpt4(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Create a word embedding as vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Stephen\\.conda\\envs\\CEResearch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 7.44MB/s]\n",
      "c:\\Users\\Stephen\\.conda\\envs\\CEResearch\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Stephen\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 3.89MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 7.73MB/s]\n",
      "config.json: 100%|██████████| 665/665 [00:00<00:00, 187kB/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2262 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "import markdown2\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Open the markdown file\n",
    "with open(\"actor.md\", \"r\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Use markdown2 to convert the markdown file to html\n",
    "html = markdown2.markdown(content)\n",
    "\n",
    "# Use BeautifulSoup to parse the html\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# Initialize variables to store heading, subheading, and corresponding paragraphs\n",
    "headings = []\n",
    "paragraphs = []\n",
    "\n",
    "data = []\n",
    "\n",
    "MAX_WORDS = 500\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"count the number of tokens in a string\"\"\"\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "# Iterate through the tags in the soup\n",
    "for tag in soup.descendants:\n",
    "    # Check if the tag is a heading\n",
    "    if tag.name in [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"]:\n",
    "        # When the next heading is encountered, print the heading, subheading, and corresponding paragraphs\n",
    "        if headings and paragraphs:\n",
    "            hdgs = \" \".join(headings)\n",
    "            para = \" \".join(paragraphs)\n",
    "            data.append([hdgs, para, count_tokens(para)])\n",
    "            headings = []\n",
    "            paragraphs = []\n",
    "        # Add to heading\n",
    "        headings.append(tag.text)\n",
    "    # Check if the tag is a paragraph\n",
    "    elif tag.name == \"p\":\n",
    "        paragraphs.append(tag.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dataset and filter out any sections with fewer than 40 tokens, as those are unlikely to contain enough context to ask a good question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>heading</th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Actors Introduction to Abaco What is Abaco</td>\n",
       "      <td>Abaco is an NSF-funded web service and distrib...</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Using Abaco</td>\n",
       "      <td>Abaco is in production and has been adopted by...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Getting Started</td>\n",
       "      <td>This Getting Started guide will walk you throu...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Account Creation and Software Installation Cre...</td>\n",
       "      <td>The main instance of the Abaco platform is hos...</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Create a Docker account</td>\n",
       "      <td>Docker is an open-source container runtime\\npr...</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             heading  \\\n",
       "0         Actors Introduction to Abaco What is Abaco   \n",
       "1                                        Using Abaco   \n",
       "2                                    Getting Started   \n",
       "3  Account Creation and Software Installation Cre...   \n",
       "4                            Create a Docker account   \n",
       "\n",
       "                                             content  tokens  \n",
       "0  Abaco is an NSF-funded web service and distrib...     131  \n",
       "1  Abaco is in production and has been adopted by...      58  \n",
       "2  This Getting Started guide will walk you throu...      85  \n",
       "3  The main instance of the Abaco platform is hos...      77  \n",
       "4  Docker is an open-source container runtime\\npr...      55  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data, columns=[\"heading\", \"content\", \"tokens\"])\n",
    "df = df[df.tokens>40]\n",
    "df = df.reset_index().drop('index',axis=1) # reset index\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-8VOz2e5g76ZisRkbFSENcQnppdytk', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='\"The Baha Men\" is the name of the band that released the popular song \"Who Let the Dogs Out\" in 2000. The phrase is often used humorously to refer to situations where things are getting out of control.', role='assistant', function_call=None, tool_calls=None))], created=1702494896, model='gpt-4-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=47, prompt_tokens=12, total_tokens=59))\n"
     ]
    }
   ],
   "source": [
    "def testing(question, tokens=300):\n",
    "    messages=[{\"role\": \"user\", \"content\": question}]\n",
    "\n",
    "    response = client.chat.completions.create(model=\"gpt-4\",\n",
    "                                                max_tokens=tokens,\n",
    "                                                temperature=0,\n",
    "                                                messages=messages)\n",
    "    print(response)\n",
    "    # Extract the content\n",
    "    content = response.choices[0].message.content\n",
    "\n",
    "testing('who let the dogs out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text: str, model: str=EMBEDDING_MODEL):\n",
    "    result = openai.Embedding.create(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "    return result[\"data\"][0][\"embedding\"]\n",
    "\n",
    "\n",
    "def compute_doc_embeddings(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Create an embedding for each row in the dataframe using the OpenAI Embeddings API.\n",
    "    \n",
    "    Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        idx: get_embedding(r.content) for idx, r in df.iterrows()\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding as vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "APIRemovedInV1",
     "evalue": "\n\nYou tried to access openai.Embedding, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Stephen\\LLMResearch\\text-search.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Stephen/LLMResearch/text-search.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m vector_embedding \u001b[39m=\u001b[39m compute_doc_embeddings(df)\n",
      "\u001b[1;32mc:\\Users\\Stephen\\LLMResearch\\text-search.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Stephen/LLMResearch/text-search.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_doc_embeddings\u001b[39m(df: pd\u001b[39m.\u001b[39mDataFrame):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stephen/LLMResearch/text-search.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stephen/LLMResearch/text-search.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m    Create an embedding for each row in the dataframe using the OpenAI Embeddings API.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stephen/LLMResearch/text-search.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m    \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stephen/LLMResearch/text-search.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m    Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stephen/LLMResearch/text-search.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Stephen/LLMResearch/text-search.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stephen/LLMResearch/text-search.ipynb#X23sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         idx: get_embedding(r\u001b[39m.\u001b[39mcontent) \u001b[39mfor\u001b[39;00m idx, r \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39miterrows()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stephen/LLMResearch/text-search.ipynb#X23sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     }\n",
      "\u001b[1;32mc:\\Users\\Stephen\\LLMResearch\\text-search.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Stephen/LLMResearch/text-search.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_doc_embeddings\u001b[39m(df: pd\u001b[39m.\u001b[39mDataFrame):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stephen/LLMResearch/text-search.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stephen/LLMResearch/text-search.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m    Create an embedding for each row in the dataframe using the OpenAI Embeddings API.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stephen/LLMResearch/text-search.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m    \u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stephen/LLMResearch/text-search.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m    Return a dictionary that maps between each embedding vector and the index of the row that it corresponds to.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stephen/LLMResearch/text-search.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stephen/LLMResearch/text-search.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Stephen/LLMResearch/text-search.ipynb#X23sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m         idx: get_embedding(r\u001b[39m.\u001b[39;49mcontent) \u001b[39mfor\u001b[39;00m idx, r \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39miterrows()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Stephen/LLMResearch/text-search.ipynb#X23sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     }\n",
      "\u001b[1;32mc:\\Users\\Stephen\\LLMResearch\\text-search.ipynb Cell 19\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Stephen/LLMResearch/text-search.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_embedding\u001b[39m(text: \u001b[39mstr\u001b[39m, model: \u001b[39mstr\u001b[39m\u001b[39m=\u001b[39mEMBEDDING_MODEL):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Stephen/LLMResearch/text-search.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     result \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mEmbedding\u001b[39m.\u001b[39;49mcreate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Stephen/LLMResearch/text-search.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m       model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Stephen/LLMResearch/text-search.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m       \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49mtext\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Stephen/LLMResearch/text-search.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Stephen/LLMResearch/text-search.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m result[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Stephen\\.conda\\envs\\CEResearch\\lib\\site-packages\\openai\\lib\\_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[1;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m_args: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m---> 39\u001b[0m     \u001b[39mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_symbol)\n",
      "\u001b[1;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.Embedding, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
     ]
    }
   ],
   "source": [
    "vector_embedding = compute_doc_embeddings(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>heading</th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "      <th>vector_embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Actors Introduction to Abaco What is Abaco</td>\n",
       "      <td>Abaco is an NSF-funded web service and distrib...</td>\n",
       "      <td>131</td>\n",
       "      <td>[-0.008385769091546535, -0.01955496147274971, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Using Abaco</td>\n",
       "      <td>Abaco is in production and has been adopted by...</td>\n",
       "      <td>58</td>\n",
       "      <td>[-0.017907971516251564, -0.008049326948821545,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Getting Started</td>\n",
       "      <td>This Getting Started guide will walk you throu...</td>\n",
       "      <td>85</td>\n",
       "      <td>[-0.006924053188413382, -0.010912280529737473,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Account Creation and Software Installation Cre...</td>\n",
       "      <td>The main instance of the Abaco platform is hos...</td>\n",
       "      <td>77</td>\n",
       "      <td>[-0.0048550949431955814, -0.020554518327116966...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Create a Docker account</td>\n",
       "      <td>Docker is an open-source container runtime\\npr...</td>\n",
       "      <td>55</td>\n",
       "      <td>[-0.0035271041560918093, -0.03285187482833862,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             heading  \\\n",
       "0         Actors Introduction to Abaco What is Abaco   \n",
       "1                                        Using Abaco   \n",
       "2                                    Getting Started   \n",
       "3  Account Creation and Software Installation Cre...   \n",
       "4                            Create a Docker account   \n",
       "\n",
       "                                             content  tokens  \\\n",
       "0  Abaco is an NSF-funded web service and distrib...     131   \n",
       "1  Abaco is in production and has been adopted by...      58   \n",
       "2  This Getting Started guide will walk you throu...      85   \n",
       "3  The main instance of the Abaco platform is hos...      77   \n",
       "4  Docker is an open-source container runtime\\npr...      55   \n",
       "\n",
       "                                    vector_embedding  \n",
       "0  [-0.008385769091546535, -0.01955496147274971, ...  \n",
       "1  [-0.017907971516251564, -0.008049326948821545,...  \n",
       "2  [-0.006924053188413382, -0.010912280529737473,...  \n",
       "3  [-0.0048550949431955814, -0.020554518327116966...  \n",
       "4  [-0.0035271041560918093, -0.03285187482833862,...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['vector_embedding'] = pd.Series(vector_embedding)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Find the most similar document embeddings to the question embedding\n",
    "\n",
    "We embed the query strip and use it to find the most similar document sections. Since this is a small example, we store and search the embeddings locally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.embeddings_utils import cosine_similarity\n",
    "\n",
    "def order_documents_query_similarity(data, query_str, nres=3):\n",
    "    embedding = get_embedding(query_str, model=EMBEDDING_MODEL)\n",
    "    data['similarities'] = data.vector_embedding.apply(lambda x: cosine_similarity(x, embedding))\n",
    "\n",
    "    res = data.sort_values('similarities', ascending=False).head(nres)\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the most relevant document sections for the token is listed at the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>heading</th>\n",
       "      <th>content</th>\n",
       "      <th>tokens</th>\n",
       "      <th>vector_embedding</th>\n",
       "      <th>similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Get tokens that will be used for authenticated...</td>\n",
       "      <td>t.gettokens()\\n  print(t.accesstoken.access_to...</td>\n",
       "      <td>205</td>\n",
       "      <td>[-0.01216100063174963, -0.010938613675534725, ...</td>\n",
       "      <td>0.828162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Create an Tapis Client Object</td>\n",
       "      <td>The first step in using the Tapis Python SDK, ...</td>\n",
       "      <td>69</td>\n",
       "      <td>[-0.00659452797845006, -0.005156101193279028, ...</td>\n",
       "      <td>0.824878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Python with Tapipy</td>\n",
       "      <td>Setting up an Tapis object with token and API ...</td>\n",
       "      <td>502</td>\n",
       "      <td>[-0.015956224873661995, -0.0027807820588350296...</td>\n",
       "      <td>0.816858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              heading  \\\n",
       "10  Get tokens that will be used for authenticated...   \n",
       "7                       Create an Tapis Client Object   \n",
       "36                                 Python with Tapipy   \n",
       "\n",
       "                                              content  tokens  \\\n",
       "10  t.gettokens()\\n  print(t.accesstoken.access_to...     205   \n",
       "7   The first step in using the Tapis Python SDK, ...      69   \n",
       "36  Setting up an Tapis object with token and API ...     502   \n",
       "\n",
       "                                     vector_embedding  similarities  \n",
       "10  [-0.01216100063174963, -0.010938613675534725, ...      0.828162  \n",
       "7   [-0.00659452797845006, -0.005156101193279028, ...      0.824878  \n",
       "36  [-0.015956224873661995, -0.0027807820588350296...      0.816858  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = order_documents_query_similarity(df, \"How to generate a token using Tapipy\")\n",
    "res.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Add the most relevant document sections to the query prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "question =  \"How to generate a token using Tapipy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_prompt(question: str, df: pd.DataFrame, ncontents = 3) -> str:\n",
    "    \"\"\"\n",
    "    Fetch relevant \n",
    "    \"\"\"\n",
    "    most_relevant_document_sections = order_documents_query_similarity(df, question)\n",
    "    \n",
    "    chosen_sections = []\n",
    "    chosen_section_len = 0\n",
    "\n",
    "    MAX_SECTION_LEN = 500\n",
    "    context = order_documents_query_similarity(df, question)\n",
    "    context.head()\n",
    "\n",
    "    for _, ctx in context.iterrows():\n",
    "        chosen_section_len += ctx.tokens\n",
    "        if chosen_section_len > MAX_SECTION_LEN:\n",
    "            break\n",
    "            \n",
    "        chosen_sections.append(\" \" + ctx.content.replace(\"\\n\", \" \"))\n",
    "    \n",
    "    header = \"\"\"Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don't know.\"\\n\\nContext:\\n\"\"\"\n",
    "    \n",
    "    return header + \"\".join(chosen_sections) + \"\\n\\n Q: \" + question + \"\\n A:\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer the question as truthfully as possible using the provided context, and if the answer is not contained within the text below, say \"I don\\'t know.\"\\n\\nContext:\\n t.gettokens()   print(t.accesstoken.access_token) Out[1]: eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9...   ``` Note that the tapipy t object will store and pass your access token for you, so you don\\\\\\'t have to manually provide the token when using the tapipy operations. You are now ready to check your access to the Tapis APIs. It will expire though, after 4 hours, at which time you will need to generate a new token. If you are interested, you can create an OAuth client (a one-time setup step, like creating a TACC account) that can be used to generate access and refresh tokens. For simplicity, we are skipping that but if you are interested, check out the Tenancy and Authentication section. The first step in using the Tapis Python SDK, tapipy, is to create a Tapis Client object. First, import the Tapis class and create python object called t that points to the Tapis server using your TACC username and password. Do so by typing the following in a Python shell: ``` python\\n\\n Q: How to generate a token using Tapipy\\n A:'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_prompt(question=\"How to generate a token using Tapipy\", df=df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Answer the user's question based on the context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPLETIONS_API_PARAMS = {\n",
    "    # We use temperature of 0.0 because it gives the most predictable, factual answer.\n",
    "    \"temperature\": 0.0,\n",
    "    \"max_tokens\": 300,\n",
    "    \"model\": COMPLETIONS_MODEL,\n",
    "}\n",
    "\n",
    "def answer_query_with_context(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    show_prompt: bool = False) -> str:\n",
    "    \n",
    "    prompt = construct_prompt(\n",
    "        query,\n",
    "        df\n",
    "    )\n",
    "    \n",
    "    if show_prompt:\n",
    "        print(prompt)\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "                prompt=prompt,\n",
    "                **COMPLETIONS_API_PARAMS\n",
    "            )\n",
    "\n",
    "    return response[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original GPT without context - telling lies as it invents a new Tapipy website and App to generate a token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Create an account on Tapipy.\\n\\n2. Log in to your account and go to the “My Apps” page.\\n\\n3. Click on “Create New App” and enter the details of your app.\\n\\n4. Once your app is created, click on “Generate Token”.\\n\\n5. Enter the details of the token you want to generate and click “Generate”.\\n\\n6. Your token will be generated and displayed on the screen.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"How to generate a token using Tapipy\"\n",
    "\n",
    "openai.Completion.create(prompt=prompt, temperature=0, max_tokens=300, model=COMPLETIONS_MODEL)[\"choices\"][0][\"text\"].strip(\" \\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When you ask a question for which it can find a context! - It answers correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Use the t.gettokens() command to generate a token using Tapipy.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_query_with_context(\"How to generate a token using Tapipy\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When it doesn't know...at least it is honest!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_query_with_context(\"How to access files using Tapipy\", df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a15fe14c9164b2c84764451972c480ab7caecb14ffdaafbc4f746bd44fda90e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
